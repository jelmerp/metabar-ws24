[
  {
    "objectID": "06_div.html#introduction",
    "href": "06_div.html#introduction",
    "title": "Alpha & beta diversity statistics",
    "section": "1 Introduction",
    "text": "1 Introduction\nGoal: Learn some basic analysis and visualizations that are possible in phyloseq and vegan packages.\nAnswer some of the following questions:\n\nRelative abundance — What are some of the larger genera present in my dataset and are they more represented in some samples than others?\nAlpha diversity — How diverse is my community?\nBeta diversity — How different are the communities across my treatments?\nPermutational ANOVA (PERMANOVA) — Are my communities structured by a known environmental factor (sampling location, rotation treatment)?"
  },
  {
    "objectID": "06_div.html#setting-up",
    "href": "06_div.html#setting-up",
    "title": "Alpha & beta diversity statistics",
    "section": "2 Setting up",
    "text": "2 Setting up\n\nLoading packages\n\n# Set the R library to load packages from\n.libPaths(\"/fs/ess/PAS0471/jelmer/R/metabar\")\ndyn.load(\"/fs/ess/PAS0471/jelmer/software/GLPK/lib/libglpk.so.40\", local = FALSE)\n\n\n# Load the packages (package startup messages are not printed below)\nlibrary(tidyverse)\nlibrary(phyloseq)\nlibrary(vegan)\nlibrary(microViz)\n\nWe’re loading the following packages:\n\ntidyverse - A package for speeding up, organizing, streamlining and visualizing data science.\nphyloseq - A package for organizing, analyzing and visualizing microbial community analysis.\nvegan - Community ecology data analysis\nmicroViz - We’re only using this package for its distinct_palette() function to generate a large color palette.\n\n\n\n\nSettings\n\n# Set the ggplot plot theme\ntheme_set(theme_bw())\ntheme_update(panel.grid.minor = element_blank())\n\n# Set a random seed for ordination analyses\nset.seed(3131)\n\n\n\nDefine output directories\nFirst, let’s set up our directories:\n\n# Dir with input files:\nindir &lt;- \"results/ps_fulldata\"\n\n# Dir for output:\noutdir &lt;- \"data/postASV_analysis\"\n\n\n\n\nLoad the phyloseq object\nLets load our phyloseq object. This is a different object than the one we generated in the DADA2 workflow in the previous session. The object in part one was generated using a subset of data in order to speed up the analysis. The object here contains all of the data from the experiment.\n\nps &lt;- readRDS(file.path(indir, \"bac22rot_w_ASV.rds\"))\n\nOur phyloseq object is made up of the following components (“slots”):\n\notu_table - OTU/ASV abundance table - how many copies of each ASV are present in each of our samples\nsample_data - metadata table contains information about the samples including treatments such as location and rotation treatment\ntax_table - a table of taxonomic assignments for each ASV, typically containing taxonomic assignments at different levels (phylum, order, class, etc)\nphy_tree - a phylogenetic tree of the ASV (optional)\nrefseq - the nucletotide sequences for each ASV\n\nLet’s have a look at the object:\n\nps\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 8088 taxa and 30 samples ]\nsample_data() Sample Data:       [ 30 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 8088 taxa by 9 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 8088 tips and 8078 internal nodes ]\nrefseq()      DNAStringSet:      [ 8088 reference sequences ]\n\n# And the assigned taxon names:\ntaxa_names(ps)[1:3]\n\n[1] \"ASV_1\" \"ASV_2\" \"ASV_3\""
  },
  {
    "objectID": "06_div.html#filter-samples",
    "href": "06_div.html#filter-samples",
    "title": "Alpha & beta diversity statistics",
    "section": "3 Filter samples",
    "text": "3 Filter samples\nFiltering out any uninformative samples – those with low total taxon counts. First, how many counts do we have for each sample?\n\nsort(sample_sums(ps))\n\nT22NW403BC  T22NW404A T22NW103AB T22NW203BC T22NW404BC  T22NW203A  T22W205BC \n     42998      44212      46909      48149      48452      49440      53456 \n T22NW201C  T22NW403A   T22W101C   T22W103C  T22W101AB T22NW201AB   T22W205A \n     54378      56468      57650      58325      58696      59083      59975 \n T22NW305C T22NW305AB T22NW102AB  T22W303AB   T22W303C   T22W204A  T22NW103C \n     60127      60746      60750      61205      67292      69701      70054 \n T22NW102C  T22W103AB  T22W204BC  T22W404BC  T22W304AB  T22W403AB   T22W404A \n     70956      73787      80628      80900      81796      87470      94857 \n  T22W304C   T22W403C \n    100448     124441 \n\n\nTo remove uninformative samples, we will only keep those with over 1,000 counts — though note that in our case, this won’t remove any samples.\n\n# Remove samples with low counts:\nps &lt;- subset_samples(ps, sample_sums(ps) &gt; 1000)\n\nLet’s take a look at some of the components of our current phyloseq object:\n\n# The sample data (metadata)\nsample_data(ps)[1:10]\n\n           Year Location Rotation  Plot TRT Block Sampling.Date\nT22NW102AB 2022    NWARS       CS 102AB   2   100    June, 2022\nT22NW102C  2022    NWARS       CS  102C   7   100    June, 2022\nT22NW103AB 2022    NWARS      CSW 103AB   3   100    June, 2022\nT22NW103C  2022    NWARS      CSW  103C   8   100    June, 2022\nT22NW201AB 2022    NWARS      CSW 201AB   3   200    June, 2022\nT22NW201C  2022    NWARS      CSW  201C   8   200    June, 2022\nT22NW203A  2022    NWARS       CS  203A   7   200    June, 2022\nT22NW203BC 2022    NWARS       CS 203BC   2   200    June, 2022\nT22NW305AB 2022    NWARS       CS 305AB   2   300    June, 2022\nT22NW305C  2022    NWARS       CS  305C   7   300    June, 2022\n\n# The count table\nhead(t(otu_table(ps)[1:10]))\n\nOTU Table:          [10 taxa and 6 samples]\n                     taxa are columns\n           ASV_1 ASV_2 ASV_3 ASV_4 ASV_5 ASV_6 ASV_7 ASV_8 ASV_9 ASV_10\nT22NW102AB     0     0     0     0     0     0     0     0     0      0\nT22NW102C      0     0     0     0     0     0     0     0     0      0\nT22NW103AB     0     2     0     2     0     0     0     0     0      0\nT22NW103C      0     0     0     0     0     0     2     0     0      0\nT22NW201AB    13     0     0     0     0     0     0     0     0      0\nT22NW201C      0     0     0     0     0     0     0     0     0      0\n\n# The taxonomy table\nhead(tax_table(ps))\n\nTaxonomy Table:     [6 taxa by 9 taxonomic ranks]:\n      Kingdom    Phylum           Class                 Order Family Genus\nASV_1 \"Bacteria\" NA               NA                    NA    NA     NA   \nASV_2 \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" NA    NA     NA   \nASV_3 \"Bacteria\" \"Proteobacteria\" NA                    NA    NA     NA   \nASV_4 \"Bacteria\" \"Proteobacteria\" NA                    NA    NA     NA   \nASV_5 \"Bacteria\" \"Proteobacteria\" NA                    NA    NA     NA   \nASV_6 \"Bacteria\" NA               NA                    NA    NA     NA   \n      Species Species_exact confidence\nASV_1 NA      NA            \"0.70\"    \nASV_2 NA      NA            \"0.50\"    \nASV_3 NA      NA            \"0.56\"    \nASV_4 NA      NA            \"0.56\"    \nASV_5 NA      NA            \"0.57\"    \nASV_6 NA      NA            \"0.99\""
  },
  {
    "objectID": "06_div.html#plotting-relative-abundances",
    "href": "06_div.html#plotting-relative-abundances",
    "title": "Alpha & beta diversity statistics",
    "section": "4 Plotting relative abundances",
    "text": "4 Plotting relative abundances\nWhat are some of the larger genera present in my dataset and are they more represented in some samples than others?\nSubset to only keep bacteria:\n\nbacteria &lt;- subset_taxa(ps, Kingdom == \"Bacteria\")\nbacteria\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 8088 taxa and 30 samples ]\nsample_data() Sample Data:       [ 30 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 8088 taxa by 9 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 8088 tips and 8078 internal nodes ]\nrefseq()      DNAStringSet:      [ 8088 reference sequences ]\n\n\nFilter to only keep abundant taxa and sort alphabetically:\n\nbacteria_genera &lt;- bacteria %&gt;%\n  tax_glom(taxrank = \"Genus\") %&gt;%                      # agglomerate at Genus level\n  transform_sample_counts(function(x) {x / sum(x)} ) %&gt;% # Transform to rel. abundance\n  psmelt() %&gt;%                                         # Melt to long format\n  filter(Abundance &gt; 0.03) %&gt;%                         # Filter out low abundance taxa\n  arrange(Genus)                                       # Sort data frame alphabetically by Genus\n\nGenerate a relative abundance plot, by sample:\n\n# Relative abundance by sample and rotation treatment\nggplot(bacteria_genera, aes(x = Sample, y = Abundance, fill = Genus)) + \n  facet_wrap(vars(Rotation), nrow = 2, scales = \"free_y\") +\n  coord_flip() +\n  geom_col() +\n  scale_fill_manual(values = microViz::distinct_palette(pal = \"kelly\")) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  guides(fill = guide_legend(reverse = TRUE, keywidth = 1, keyheight = 1)) +\n  labs(x = NULL,\n       y = \"Relative Abundance (Genera &gt; 2%)\",\n       title = \"Relative Abundance of Bacteria\") +\n  theme(panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "06_div.html#alpha-diversity",
    "href": "06_div.html#alpha-diversity",
    "title": "Alpha & beta diversity statistics",
    "section": "5 Alpha Diversity",
    "text": "5 Alpha Diversity\nAlpha diversity analysis asks the question, how diverse is my community?\nUsually expressed in terms of species richness (number of species or number of ASVs). It defines the biological diversity within a ecosystem, community, etc.\nAlpha diversity has been given different definitions by several ecologists. And these various definitions might be influenced by different assumptions of species diversity. This is the reason why most researchers have been utilizing more than one index of diversity.\nMeasurements for species richness: Observed richness, Chao1\nCan also be expressed by diversity indices which take into account species richness and evenness. Evenness provides information about the equity in species abundance in each sample, to put another way: is there one dominant species or are there a lot of species that have similar abundances?\nDiversity indices: Simpson, Shannon, Inverse Simpson\nPhyloseq has a lot of options for plotting alpha diversity. We will take a looks at a few here.\nFirst, alpha diversity by sample, using observed richness (# of ASVs):\n\n# plot alpha diversity by sample\nplot_richness(ps, measures = \"Observed\")\n\nWarning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have\nany singletons. This is highly suspicious. Results of richness\nestimates (for example) are probably unreliable, or wrong, if you have already\ntrimmed low-abundance taxa from the data.\n\nWe recommended that you find the un-trimmed data and retry.\n\n\n\n\n\n\n\n\n\nWe can combine multiple diversity measures into one graph. Here we also combine samples by treatment, such as our rotational scheme. Adding color to take a look at if location stands out as a potential difference maker here.\n\n# plot multiple indices \nplot_richness(ps,\n              x = \"Rotation\",\n              color = \"Location\", \n              measures = c(\"Observed\", \"Shannon\", \"Simpson\", \"InvSimpson\")) +\n  theme(legend.position = \"top\")\n\nWarning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have\nany singletons. This is highly suspicious. Results of richness\nestimates (for example) are probably unreliable, or wrong, if you have already\ntrimmed low-abundance taxa from the data.\n\nWe recommended that you find the un-trimmed data and retry.\n\n\n\n\n\n\n\n\n\nLets compare plots combining rotational treatments across sites and the diversity at the individual sites.\n\nplot_richness(ps, x = \"Rotation\", measures = c(\"Observed\", \"Shannon\")) +\n  geom_boxplot()\n\nWarning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have\nany singletons. This is highly suspicious. Results of richness\nestimates (for example) are probably unreliable, or wrong, if you have already\ntrimmed low-abundance taxa from the data.\n\nWe recommended that you find the un-trimmed data and retry.\n\n\n\n\n\n\n\n\n\n\nplot_richness(ps, x = \"Location\", measures = c(\"Observed\", \"Shannon\")) +\n  geom_boxplot()\n\nWarning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have\nany singletons. This is highly suspicious. Results of richness\nestimates (for example) are probably unreliable, or wrong, if you have already\ntrimmed low-abundance taxa from the data.\n\nWe recommended that you find the un-trimmed data and retry.\n\n\n\n\n\n\n\n\n\nUsing ggplot to add some color to the figure:\n\n# Make alpha diversity figure\nplot_richness(ps,\n              x = \"Location\",\n              measures = c(\"Observed\", \"Shannon\"),\n              color = \"Location\",\n              nrow = 1) +\n  geom_boxplot() +\n  geom_jitter(width = 0.05) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Location\", y = \"Diversity\")\n\nWarning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have\nany singletons. This is highly suspicious. Results of richness\nestimates (for example) are probably unreliable, or wrong, if you have already\ntrimmed low-abundance taxa from the data.\n\nWe recommended that you find the un-trimmed data and retry.\n\n\n\n\n\n\n\n\n\nCalculate differences between alpha diversity metrics between samples\nHere we used a non-parametric pairwise Wilcoxon test to compare diversity between our locations:\n\n# Calculate diversity metrics using estimate_richness and store the data using a vector\nrich &lt;- estimate_richness(ps, measures = c(\"Observed\", \"Shannon\"))\n\nWarning in estimate_richness(ps, measures = c(\"Observed\", \"Shannon\")): The data you have provided does not have\nany singletons. This is highly suspicious. Results of richness\nestimates (for example) are probably unreliable, or wrong, if you have already\ntrimmed low-abundance taxa from the data.\n\nWe recommended that you find the un-trimmed data and retry.\n\n# Compare observed diversity between our sample location\nwilcox.observed &lt;- pairwise.wilcox.test(rich$Observed, \n                                        sample_data(ps)$Location, \n                                        p.adjust.method = \"BH\")\n\nWarning in wilcox.test.default(xi, xj, paired = paired, ...): cannot compute\nexact p-value with ties\n\nwilcox.observed\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  rich$Observed and sample_data(ps)$Location \n\n     NWARS \nWARS 0.0083\n\nP value adjustment method: BH \n\n\nWhat needs to change in order to test for differences in the Shannon index?\nHow about between our rotational treatments?\n\nwilcox.shannon &lt;- pairwise.wilcox.test(rich$Shannon, \n                                       sample_data(ps)$Rotation, \n                                       p.adjust.method = \"BH\")\nwilcox.shannon\n\n\n    Pairwise comparisons using Wilcoxon rank sum exact test \n\ndata:  rich$Shannon and sample_data(ps)$Rotation \n\n    CS  \nCSW 0.35\n\nP value adjustment method: BH"
  },
  {
    "objectID": "06_div.html#beta-diversity",
    "href": "06_div.html#beta-diversity",
    "title": "Alpha & beta diversity statistics",
    "section": "6 Beta Diversity",
    "text": "6 Beta Diversity\nHow different are my communities?\nBeta diversity basically tells us how similar or dissimilar samples are to one another. Phyloseq offers several ordination methods and distance metrics. Some examples of distance metrics are Bray-Curtis which is based on abundance and jaccard distance which is based on presence absences, unifrac takes into account the occurance table and phylogenetic diversity. Here we use non metric multidimensional scaling (NMDS) to visualize beta diversity coupled with Jensen–Shannon divergence a distance metric based on probability distributions that account for the occurance data.\nMultidimensional scaling (MDS) is a unique coordination technique in that a (small) number of ordination axes are explicitly chosen prior to the analysis and the data are thenfitted to those dimensions. Thus, if only 2 or 3 axes are chosen, there will be no nondisplayed axes of variation at the end of the analysis. Similar to PCoA, a matrix of object dissimilarities is first calculated using a chosen distance metric. In non metric multidimensional scaling (NMDS), ranks of these distances among all objects are calculated. The algorithm then finds a configuration of objects in the chosen N-dimensional ordination space that best matches differences in ranks.\n\n\n6.1 Ordination plotting\nMicrobiome data is compositional and sparse, NMDS is one option for ordination method in these cases. NMDS makes few assumptions about the nature of data and allows the use of any distance measure of the samples which are the exact opposite of other ordination methods.\n\n# Non-metric MultiDimensional Scaling (NMDS)\nord.nmds.jsd_slv &lt;- ordinate(ps, method = \"NMDS\", distance = \"jsd\")\n\nRun 0 stress 0.06127255 \nRun 1 stress 0.06106308 \n... New best solution\n... Procrustes: rmse 0.006378707  max resid 0.0243293\n# [...output truncated...]\n\n# Create a \"stressplot\"\nstressplot(ord.nmds.jsd_slv)\n\n\n\n\n\n\n\n\nWhat would we change to use Bray-curtis dissimilarity instead of Jensen-Shannon distance?\nHere we plot results from the Jensen-Shannon divergence that we calculated.\n\nplot_ordination(ps, ord.nmds.jsd_slv,\n                color = \"Location\", shape = \"Rotation\") +\n  geom_point(size = 4) +\n  coord_fixed() +\n  stat_ellipse(type = \"t\") \n\n\n\n\n\n\n\n\nWhat seems to be driving the clustering in this experiment?\nIt can be more convincing if more than one type of beta diversity analysis comes up with a similar solution, so lets try another type of analysis.\n\n\n\n6.2 Redundancy Analysis\nRedundancy analysis is a type of constrained ordination that assesses how much of the variation in one set of variables can be explained by the variation in another set of variables. It is the multivariate extension of simple linear regression that is applied to sets of variables.\n\n# Redundancy Analysis\nord.rda &lt;- ordinate(ps,\n                    formula = o ~ Rotation+Location,\n                    method = \"CAP\",\n                    distance = \"bray\")\n\nRsquareAdj(ord.rda) \n\n$r.squared\n[1] 0.4067635\n\n$adj.r.squared\n[1] 0.3628201\n\nanova.cca(ord.rda)\n\nPermutation test for capscale under reduced model\nPermutation: free\nNumber of permutations: 999\n\nModel: capscale(formula = OTU ~ Rotation + Location, data = data, distance = distance)\n         Df SumOfSqs      F Pr(&gt;F)    \nModel     2   1.5634 9.2565  0.001 ***\nResidual 27   2.2801                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nvif.cca(ord.rda)\n\n RotationCSW LocationWARS \n    1.005128     1.005128 \n\n\n\n# plot the RDA\nplot_ordination(ps,\n                vegan::scores(ord.rda, scaling = 1),\n                type = \"sites\",\n                color = \"Location\",\n                shape = \"Rotation\") \n\n\n\n\n\n\n\n\nIn both analysis we can see some clustering within groups and spread between groups, but this is not a test for statistical differences. Do microbial communities differ significantly by host taxa?"
  },
  {
    "objectID": "06_div.html#permutational-anova-permanova",
    "href": "06_div.html#permutational-anova-permanova",
    "title": "Alpha & beta diversity statistics",
    "section": "7 Permutational ANOVA: PERMANOVA",
    "text": "7 Permutational ANOVA: PERMANOVA\nDo communities cluster by a known environmental factor (rotation, location)?\nFinally, we’ll test whether there is a statistically significant ecological level treatment effect. PERMANOVA sounds fancy, but it is just an ANOVA performed using permutations. Permutations are used to determine how data may appear if there is no treatment effect and group differences are due to random chance. Observed data are then compared to the randomized data to calculate a p-value.\n\nsampledf &lt;- data.frame(sample_data(ps))\n\ndist.mat &lt;- phyloseq::distance(ps, method = \"jsd\")\n\nperm &lt;- how(nperm = 999)\nsetBlocks(perm) &lt;- with(sampledf, Block)\n\nadonis2(dist.mat ~ Location * Rotation,\n        data = sampledf, permutations = perm)\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nBlocks:  with(sampledf, Block) \nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = dist.mat ~ Location * Rotation, data = sampledf, permutations = perm)\n                  Df SumOfSqs      R2       F Pr(&gt;F)    \nLocation           1  0.52765 0.49807 27.8874  0.001 ***\nRotation           1  0.02225 0.02100  1.1759  0.236    \nLocation:Rotation  1  0.01755 0.01657  0.9278  0.373    \nResidual          26  0.49194 0.46436                   \nTotal             29  1.05940 1.00000                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \n\n\nAttribution\nInformation for this document was adapted by Timothy Frey from several excellent tutorials, most of which have more details than this one. These included:\n\nA tutorial from an earlier version of this workshop from Matthew Willman\nhttps://projectdigest.github.io/4_diversity.html#correlations_with_diversity\nhttps://www.yanh.org/2021/01/01/microbiome-r\nhttps://www.gdc-docs.ethz.ch/MDA/handouts/MDA20_PhyloseqFormation_Mahendra_Mariadassou.pdf\nA breakdown of multivariate statistics used in these analysis"
  },
  {
    "objectID": "homework/R.html#why-r",
    "href": "homework/R.html#why-r",
    "title": "Homework: Intro to R",
    "section": "1 Why R?",
    "text": "1 Why R?\nR is a versatile, open source programming/scripting language that’s particularly useful for statistics and data visualization.\nYes, there is a learning curve, and many of us just want to get on with our analysis –\nbut investing in learning R will pay off:\n\nR gives you greater flexibility to do anything you want.\nA greater reproducibility of scripting vs clicking.\nR is highly interdisciplinary – e.g. very useful for analyzing sequencing data but can also be used to create maps and perform GIS analyses, and so on!\nR is more than a platform to perform analysis. Combined with Markdown (a simple text markup language), you can use R to produce sophisticated reports, and create slide decks and websites such as this one!\n\nFurthermore, R:\n\nIs freely available on all platforms, and open source.\nHas a large and welcoming user community.\n\n\n\nLearning Objectives\n\nGet some basic familiarity with R and RStudio\nLearn why and how to use RStudio Projects\nUnderstand objects, functions, and how to use them\nUnderstand the concepts of vector and data.frame\nExplore the structure and the content of a data.frame\nLearn to subset/slice vectors and data frames\nLearn about how R handles missing data"
  },
  {
    "objectID": "homework/R.html#getting-set-up",
    "href": "homework/R.html#getting-set-up",
    "title": "Homework: Intro to R",
    "section": "2 Getting set up",
    "text": "2 Getting set up\n\n2.1 Start an RStudio session at OSC\n\nLog in to OSC at https://ondemand.osc.edu.\nClick on Interactive Apps (top bar) and then RStudio Server (all the way at the bottom).\nFill out the form as follows:\n\nCluster: Pitzer\nR version: 4.3.0\nProject: PAS2714\nNumber of hours: 4\nNode type: any\nNumber of cores: 1\n\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick the big blue Launch button at the bottom\nNow, you should be sent to a new page with a box at the top for your RStudio Server “job”, which should initially be “Queued” (waiting to start).\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nYour job should start running very soon, with the top bar of the box turning green and saying “Running”.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick Connect to RStudio Server at the bottom of the box, and an RStudio Server instance will open in a new browser tab. You’re ready to go!\n\n\n\n\n2.2 R vs. RStudio\nR simply provides a “console” to type your commands. However, because we want to save our commands in scripts, examine output such a graphics, and so on, we would like an environment that provides all of this side-by-side.\nWe will use RStudio, an excellent “Integrated Development Environment” (IDE) for R. In RStudio, we have a single interface to write code, navigate files on our computer, inspect the objects we create, and visualize plots.\nRStudio is divided into 4 “panes”1:\n\nTop-left: The editor for your scripts and documents.\nBottom-left: The R console.\nTop-right: Your environment/history.\nBottom-left: Your files/plots/packages/help/viewer.\n\n\n\n\nThe RStudio pane layout"
  },
  {
    "objectID": "homework/R.html#interacting-with-r",
    "href": "homework/R.html#interacting-with-r",
    "title": "Homework: Intro to R",
    "section": "3 Interacting with R",
    "text": "3 Interacting with R\n\n3.1 R as a calculator\nThe lower-left RStudio pane, i.e. the R console, is where you can interact with R directly. The &gt; sign is the R “prompt”. It indicates that R is ready for you to type something.\nLet’s start by performing a division:\n\n203 / 2.54\n\n[1] 79.92126\n\n\nR does the calculation and prints the result, and then you get the &gt; prompt again. (The [1] may look a bit weird when there is only one output element; this is how you can keep count of output elements.)\nWith the standard symbols you can use R as a general calculator:\n\n203 * 2.54   # Multiplication\n\n[1] 515.62\n\n203 + 2.54   # Addition\n\n[1] 205.54\n\n\n\n\n\n3.2 Trying some random things…\n\n   203     - 2.54\n\n[1] 200.46\n\n\nThis works: so R just ignores any extra spaces. How about:\n\n203 +\n\n\n\n Now the prompt is a +. What is going on? (Click for the answer)\n\nR is waiting for you to finish the command, since you typed an incomplete command: something has to come after the + to be added to what came before.\nWhile it was obvious here, you will often type incomplete commands without realizing you did so. Just remember that when you see the + prompt, you are missing something in your command: often, you’ll have forgotten a closing parenthesis ) or you accidentally opened up an unwanted opening parenthesis (.\n\nPress the Esc button to get your prompt back.\n\nAnd if we just type a number:\n\n203\n\n[1] 203\n\n\nR will print the number back to us! It turns out that the default, implicit action that R will perform on anything you type is to print it back to us (it is calling a function called print() under the hood).\n\nRather than a number, what if we want R to print back to us some text, which in programming lingo is called a “character string”?\n\nFantastic\n\nError in eval(expr, envir, enclos): object 'Fantastic' not found\n\n\n\n\n What seems to be going wrong here? (Click for the answer)\n\nWhenever you type a character string, R expects to find an “object” with that name. (Or, if you would use parentheses after the string, like string(), it will expect a function.)\nBecause no object called Fantastic exists, R throws an error. To refer to a literal string instead, we need to use quotes (see below).\n\n\nWe can get R to print character strings back to us, and work with them in other ways, as long as we quote the strings:\n\n\"Fantastic\"\n\n[1] \"Fantastic\"\n\n\n\n\"I'm really liking R so far.\"\n\n[1] \"I'm really liking R so far.\"\n\n\nSo, R treats numbers and character strings differently: unlike numbers, character strings need to be quoted. This avoids confusion with objects (we’ll learn about those in a minute) because unquoted character strings are assumed to be objects, and also allows for “special characters” like spaces.\n\n\n\n\n\n\nQuote types\n\n\n\nDouble quotes (\"Fantastic\") and single quotes ('Fantastic') can be used interchangeably in R. Double quotes are preferred by most “style guides”."
  },
  {
    "objectID": "homework/R.html#getting-organized",
    "href": "homework/R.html#getting-organized",
    "title": "Homework: Intro to R",
    "section": "4 Getting Organized",
    "text": "4 Getting Organized\n\n4.1 Need for Scripts and RStudio Projects\nWe can go along like this, typing commands directly into the R console. But to better keep track of what you’re doing, it’s a good idea to write your code in files, i.e. “scripts”. And when we start creating scripts, we need to worry about how we organize the scripts and data for a project.\nIt is good practice to keep a set of related data, analyses, and text self-contained in a single folder, and use that folder as the working directory — in the Unix shell and in R alike. RStudio provides a helpful way to keep your working directory constant through its “Projects”. When you use a Project, your working directory will be the top-level directory of that project.\n\n\n\n4.2 Create a new RStudio Project\nTo create a new RStudio Project inside your personal dir in /fs/ess/PAS2714/users (e.g., for me, the dir /fs/ess/PAS2714/users/jelmer)\n\n\n\n\n\n\nDon’t have a personal dir there? (Click to expand)\n\n\n\n\n\nIf you followed the shell homework, you should have created your own dir within /fs/ess/PAS2714/users.\nIf you don’t have one, you can quickly create it as follows:\n\ndir.create(paste0(\"/fs/ess/PAS2714/users/\", Sys.getenv(\"USER\")))\n\n\n\n\n\nClick File (top bar, below your browser’s address bar) &gt; New Project\nIn the popup window, click Existing Directory.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick Browse... to select your personal dir.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nIn the next window, you should be in your Home directory (abbreviated as ~), from which you can’t click your way to /fs/ess! Instead, you’ll first have to click on the (very small!) ... highlighted in the screenshot below:\n\n\n\n\n\n\n\nType at least part of the path to your personal dir in /fs/ess/PAS2714/users, e.g. like shown below, and click OK:\n\n\n\n\n\n\n\nNow you should be able to browse/click the rest of the way to your personal directory, something like /fs/ess/PAS2714/users/jelmer.\nClick Choose to pick your selected directory.\nClick Create Project.\n\nRStudio should reload and you should now have your new Project “open”. Your working directory should be the Project’s directory. We can check this using getwd():\n\ngetwd()\n\n/fs/ess/PAS2714/users/jelmer\nFrom now on, we will not change our working directory, and refer to all files relative to our project’s top-level directory.\n\n\n\n4.3 Create an R script\n\nCreate a new R script (File &gt; New File &gt; R Script)\nClick File &gt; Save As to save the script in the scripts dir that you should see within your personal dir.2 Give it a descriptive name like intro-to-R.R.\n\nFrom now on, type your commands into this script and execute the commands from there.\nTo send code from your script to the console, press Ctrl/Cmd + Enter. This will copy the line of code that your cursor is at to the R console and execute it, and then the cursor will move to the next line.\n\n\n\n4.4 Commenting\nYou can use # signs to comment your code:\n\n# Divide by 2.54 to get the wingspan in inches:\n203 / 2.54    # Original measurement was in cm\n\n\nAnything to the right of a # is ignored by R, meaning it won’t be executed\nYou can use # both at the start of a line (entire line is a comment) or anywhere in a line following code (rest of the line is a comment)\nIn your R script, comments are formatted differently so you can clearly distinguish them from code\n\nWe recommend that you use lots of comments in your R scripts! They are useful not only for others that you may share your code with, but also for yourself when you look back at your code a day, a month, or a year later."
  },
  {
    "objectID": "homework/R.html#r-objects",
    "href": "homework/R.html#r-objects",
    "title": "Homework: Intro to R",
    "section": "5 R objects",
    "text": "5 R objects\n\n5.1 Assigning stuff to objects\nWe can assign pretty much anything to an object with the assignment operator, &lt;-3. (This is a smaller-than sign &lt; followed by a dash -.)\nA few examples:\n\nwingspan_cm &lt;- 203\nconversion &lt;- 2.54\n\nType that into your script, and use Ctrl/Cmd + Enter twice to send it to the console.\n\n\n\n\n\n\nThe Environment tab\n\n\n\nThe objects you create get added to your “environment”, which RStudio shows in the Environment tab in the top-right panel — check that wingspan_cm and conversion are indeed there.\n\n\nAfter you’ve assigned a number to an object, you can use it in calculations like so:\n\nwingspan_cm / conversion\n\n[1] 79.92126\n\n\nOr, similarly:\n\nwingspan_inch &lt;- wingspan_cm / conversion\nwingspan_inch\n\n[1] 79.92126\n\n\nThis illustrates that when you execute code with objects, R substitutes the object name that you provide by its contents under the hood. In other words, the object is just a reference to the underlying value(s).\n\n\n5.2 Object names\nObjects can be given almost any name such as x, current_temperature, or subject_id. Some pointers on naming:\n\nBecause R is case sensitive, wingspan_inch is different from Wingspan_inch!\nAn object name cannot contain spaces — so for readability, you should separate words using:\n\nUnderscores: wingspan_inch (this is called “snake case”)\nPeriods: wingspan.inch\nCapitalization: wingspanInch or WingspanInch (“camel case”)\n\nYou will make things easier for yourself by naming objects in a consistent way, e.g. by sticking to a case type.\nObject names can contain but cannot start with a number: x2 is valid but 2x is not4.\nMake object names descriptive yet not too long — this is not always easy!\n\n\n Exercises: objects and strings\nA) Which of the following do you think would work and which would return an error:\n\nsession_topic &lt;- \"introduction\"\n\n\nsession_topic &lt;- introduction\n\nTry both to see which works and what error you get for the other one. Also, try to describe in words what the correct line of code is doing.\n\n\nSolution (click here)\n\nThe first of the two options was correct R code, while the second returns an error.\nIn general, keep in mind that unquoted character strings represent objects whereas quoted character strings are “literals”. Here, we wanted to assign the literal string \"introduction\" to the object session_topic — so the former should be quoted and the latter not.\nAn error is produced when you run the second option, because the object introduction does not exist (unless, of course, you had created an object of that name!):\n\nsession_topic &lt;- introduction\n\nError in eval(expr, envir, enclos): object 'introduction' not found\n\n\n\n\nB) Having run the code above, which of the following would make R print \"introduction\"?\n\n\"session_topic\"\n\n\nsession_topic\n\n\n\nSolution (click here)\n\nThe second option is the correct one: here, we want to have R print the value of the object session_topic (which we had just created in exercise A), so we shouldn’t use quotes.\n\nsession_topic\n\n[1] \"introduction\"\n\n\n\n\nC) Do you think the following code would successfully add 5 and 7? If not, what might happen instead?\n\n\"5\" + \"7\"\n\n\n\nSolution (click here)\n\nIn the code above, the “numbers” are saved not as numbers (in R lingo: as a numeric) but as character strings (character).\nR can’t add character strings, so it will return an error:\n\n\"5\" + \"7\"\n\nError in \"5\" + \"7\": non-numeric argument to binary operator\n\n\n(Perhaps you expected it to combine/“concatenate” the two strings in some way — this is in fact what Python would do. Or to automatically convert the characters to numbers, since you’re clearly wanting them to be numbers — but it doesn’t do that either.)\n\n\nD) (Bonus) What is the value of y after running the following lines of code?\n\nx &lt;- 50\ny &lt;- x * 2\nx &lt;- 80\n\n\n\n Solution (click here)\n\nObjects don’t get linked to each other, so if you change one, it won’t affect the values of any others. Therefore, y will keep the value 100."
  },
  {
    "objectID": "homework/R.html#functions",
    "href": "homework/R.html#functions",
    "title": "Homework: Intro to R",
    "section": "6 Functions",
    "text": "6 Functions\nEarlier, we divided 203 by 2.54, but what if we wanted to round the resulting number? Like for many things you may want to do in R, there is a function for that.\nFunctions are used by typing their name followed by parentheses:\n\nround(203 / 2.54)\n\n[1] 80\n\n\nHere, round() is a function that rounds a number. The value in the parentheses is called a function “argument”, which is used in the execution of the function.\n\n\n6.1 Using named arguments\nFunctions can have more than one argument, and some of them may have default values.\nThere are some functions that take many arguments and it can get confusing trying to keep them in order. In that case, it is better to explicitly name the arguments.\nWhen you type a function name and pause for a moment, the arguments, their names, and their default values (i.e., the value if the argument is left unspecified) will be shown.\n\n\n\n\n\n\n\n What is the second argument for round() and what is its default value? (Click here)\n\nround has a second argument digits whose default is 0, such that numbers will be rounded to whole integers.\n\nBelow is an example using named arguments with round(). When the arguments are named, the order doesn’t matter! You might also enter the first few important arguments positionally, and later ones by naming them.\n\nround(x = 1.538462, digits = 2)\n\n[1] 1.54\n\nround(digits = 2, x = 1.538462)\n\n[1] 1.54\n\nround(1.538462, digits = 2)\n\n[1] 1.54\n\n\nAlso here, we can directly plug in objects:\nwingspan_in &lt;- 203 / 2.54\nround(wingspan_in)\nOr “nest” functions — here we are adding the log() function to compute the natural log:\nlog(round(203 / 2.54 ))\n\n\n What is the order of execution in the last command? (Click for the solution)\n\nround() is executed first, and the output of round() is used as the input of log()."
  },
  {
    "objectID": "homework/R.html#getting-help",
    "href": "homework/R.html#getting-help",
    "title": "Homework: Intro to R",
    "section": "7 Getting help",
    "text": "7 Getting help\nAs we saw, when we typed round and paused for a moment, we got a pop-up with information about the function. Alternatively, you could type:\n\n?round\n\n… and the documentation for the function will show up in the lower-right pane.\nThis documentation is often a bit too detailed, and can be terse, so it takes some practice to read. Usage, Arguments, and at the bottom, Examples, are most useful.\nGoogling, even if you don’t know whether a function exists, will work too (e.g. “rounding a number in r”)."
  },
  {
    "objectID": "homework/R.html#vectors",
    "href": "homework/R.html#vectors",
    "title": "Homework: Intro to R",
    "section": "8 Vectors",
    "text": "8 Vectors\nA vector is the most common and basic data structure in R, and is composed of a series of values of the same type5. We can assign a series of values to a vector using the c() function (for “combine”). For example:\n\nwingspans_cm &lt;- c(11.8, 203, 18.2, 27.9)\n\nA vector can also contain characters – but again, quoting is important, or R will think the strings are objects:\n\nbirds &lt;- c(\"hummingbird\", \"bald_eagle\", \"chickadee\", \"cardinal\")\n\nAs mentioned, all of a vector’s elements have to be of the same type of data. The function class() indicates what kind of data you are working with:\n\nclass(wingspans_cm)\n\n[1] \"numeric\"\n\nclass(birds)\n\n[1] \"character\"\n\n\n\n\n8.1 Data types in R\nThe classes we saw above are different types of atomic vectors, R’s simplest data type. The 4 most common atomic vector types are:\n\n\"numeric\" (or \"double\") – floating point numbers (numbers with decimals)\n\"integer\" – integer numbers (no decimals)\n\"character\" – character strings\n\"logical\" – TRUE and FALSE (also known as boolean)\n\nAlso worth mentioning in this context is:\n\nfactor – Character strings with a discrete set of possible values, used mostly for statistical tests and plotting6.\n\n\n\n\n\n\n\nSide note: Vector coercion – when not all elements are of the same type. (Click to expand)\n\n\n\n\n\nWhat happens if we try to mix vector types (e.g., “character and numeric”) in a single vector? R converts them to all be the same type, and it does so without telling us about it. For example:\n\nnum_char &lt;- c(1, 2, 3, \"a\")\nclass(num_char)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n8.2 Vectorization!\nLet’s say we wanted to convert our vector of wingspans to inches: dividing each length in centimeters by 2.54. It turns out that this is as easy as dividing the vector by 2.54:\n\n# wingspans_cm &lt;- c(11.8, 203, 18.2, 27.9)  # Still working with the same wingspans_cm vector\n\nwingspans_in &lt;- wingspans_cm / 2.54\nwingspans_in\n\n[1]  4.645669 79.921260  7.165354 10.984252\n\n\nThis works because R “vectorizes” operations whenever it can. This means that in this case, each element in the vector weights_cm will be divided by 2.54 – this number is recycled to match the number of weights. Very useful!\nSimilarly, we can use two vectors of equal length to quickly operate on each element of the vector:\n\nsizes_cm &lt;- c(7.62, 90, 13.1, 21.8)\n\nratio &lt;- wingspans_cm / sizes_cm\nratio\n\n[1] 1.548556 2.255556 1.389313 1.279817\n\n\n\n\n Exercise: Temperature conversion\nRecall that you can convert a temperature of, for example, 26 Fahrenheit to Celcius as follows:\n\n# (26°F - 32) / 1.8 = -3.3°C\n(26 - 32) / 1.8\n\n[1] -3.333333\n\n\nCreate and store a vector with the values 26, 21, 24, 32, 33, 41, 51, representing temperatures in Fahrenheit.\nNow convert all the values in this vector from Fahrenheit to Celcius.\n\n\nClick here for the solution\n\nThanks to R’s vectorization, you don’t have to convert these values 1-by-1:\n\ntemps_f &lt;- c(26, 21, 24, 32, 33, 41, 51)\n\n(temps_f - 32) / 1.8\n\n[1] -3.3333333 -6.1111111 -4.4444444  0.0000000  0.5555556  5.0000000 10.5555556\n\n\n\n\n\n\n\n8.3 Other data structures in R\nWhile vectors can be composed of one of several data types, they, in turn, are one of several data structures that R uses. Other important ones are7:\n\ndata.frame – A rectangular data structure where each column can be a different data type.\nmatrix – A rectangular data structure of a single data type.\nlist – A very flexible data structure that we will not further discuss here."
  },
  {
    "objectID": "homework/R.html#data-frames",
    "href": "homework/R.html#data-frames",
    "title": "Homework: Intro to R",
    "section": "9 Data Frames",
    "text": "9 Data Frames\nA data frame (formal object type: data.frame) is a rectangular data structure in which:\n\nRows are observations and columns are variables.\nEach column can be of a different type (numeric, character, etc.)\nSince each column is a vector, all the values (cells) within a column are of the same type.\nAll columns have the same length.\n\n\n\n\n\n\n\n9.1 Create, write, and read a data frame\nWe can easily create a data frame by hand using the data.frame() function and “column_name = column_vector” notation for each column:\n\nbirds_df &lt;- data.frame(species = birds,\n                       wingspan = wingspans_cm,\n                       size = sizes_cm,\n                       n_eggs = c(2, 2, 7, 4)) \n\n\nbirds_df\n\n      species wingspan  size n_eggs\n1 hummingbird     11.8  7.62      2\n2  bald_eagle    203.0 90.00      2\n3   chickadee     18.2 13.10      7\n4    cardinal     27.9 21.80      4\n\n\nMost often, however, you’ll be reading your data frames from files. And you’ll also want to save your modified data frames.\nSo let’s practice writing and reading a data frame to and from a “CSV” file — a plain-text, tabular file in which columns are delimited by commas (“Comma-Separated Values” file).\n\n# Write a data frame to CSV format:\nwrite.csv(x = birds_df, file = \"sandbox/bird-data.csv\", row.names = FALSE)\n\n\n\n\n\n\n\nCurious what the file itself looks like? (Click to expand)\n\n\n\n\n\nIf you want to do the following yourself: next to the R Console, there is tab called Terminal, which will open a Unix shell (!). In there, type the following:\ncat sandbox/bird-data.csv\n\"species\",\"wingspan\",\"size\",\"n_eggs\"\n\"hummingbird\",11.8,7.62,2\n\"bald_eagle\",203,90,2\n\"chickadee\",18.2,13.1,7\n\"cardinal\",27.9,21.8,4\n\n\n\nNow we read our data frame back in from the file:\n\nbirds_df_2 &lt;- read.csv(\"sandbox/bird-data.csv\")\n\nbirds_df_2\n\n      species wingspan  size n_eggs\n1 hummingbird     11.8  7.62      2\n2  bald_eagle    203.0 90.00      2\n3   chickadee     18.2 13.10      7\n4    cardinal     27.9 21.80      4\n\n\n\n\n\n9.2 Inspecting a Data Frame\nUse str() to look at the “structure” of the data — it tells us the number rows and columns, and for each column, gives information about the data type and shows the first few values:\n\nstr(birds_df)\n\n'data.frame':   4 obs. of  4 variables:\n $ species : chr  \"hummingbird\" \"bald_eagle\" \"chickadee\" \"cardinal\"\n $ wingspan: num  11.8 203 18.2 27.9\n $ size    : num  7.62 90 13.1 21.8\n $ n_eggs  : num  2 2 7 4\n\n\nFor larger data frames, the head() function, which will print the first 6 rows, is also useful (but is of no real use for a tiny data frame like birds_df):\n\nhead(birds_df)\n\n      species wingspan  size n_eggs\n1 hummingbird     11.8  7.62      2\n2  bald_eagle    203.0 90.00      2\n3   chickadee     18.2 13.10      7\n4    cardinal     27.9 21.80      4\n\n\nFinally, in RStudio, you can open a data frame in a spreadsheet-like manner by clicking on an object in the “Environment” pane, or equivalently, using View():\n\nView(birds_df)\n\n\n\n\n9.3 Functions to get an overview of data frames\n\nSize:\n\nnrow() – Number of rows\nncol() – Number of columns\ndim() – Dimensions: c(number of rows, number of columns)\nlength() – For a dataframe: number of columns. For a vector: number of elements.\n\nContent:\n\nhead() – shows the first 6 rows\ntail() – shows the last 6 rows\n\nNames:\n\nnames() or colnames() – column names\nrownames() – row names\n\nSummary:\n\nstr() – structure of the object and information about the class, length and content of each column\nsummary() – summary statistics for each column\n\n\n\n\n Exercise: Cars\nmtcars is an example data frame that is always available in R.\n\nUse head() to print the first 6 rows of the mtcars dataframe.\n\n\n\nClick here for the solution\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\nHow many rows and columns does mtcars contain?\n\n\n\nClick here for the solution\n\nIt contains 32 rows and 11 columns:\n\ndim(mtcars)\n\n[1] 32 11\n\n\nOr:\n\nnrow(mtcars)\n\n[1] 32\n\nncol(mtcars)\n\n[1] 11\n\n\n\n\nGet a vector with the column names of mtcars.\n\n\n\nClick here for the solution\n\n\ncolnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\""
  },
  {
    "objectID": "homework/R.html#subsetting",
    "href": "homework/R.html#subsetting",
    "title": "Homework: Intro to R",
    "section": "10 Subsetting",
    "text": "10 Subsetting\n\n10.1 Basic subsetting of data frames and vectors\nWe can pull out parts of vectors and data frames using square brackets.\n\nVectors\nFor example, for vectors:\n\n# Remind ourselves what this vector contains\nwingspans_cm\n\n[1]  11.8 203.0  18.2  27.9\n\n\n\n# Get the first element\nwingspan_cm[1]\n\n[1] 203\n\n\n\n# Get the third element\nwingspan_cm[3]\n\n[1] NA\n\n\nYou can pull out larger “slices” from the vector by providing vectors of indices:\n\n# Get the first and the third element\nwingspan_cm[c(1, 3)]\n\n[1] 203  NA\n\n\nThe : operator gives you a sequence of consecutive values, which you can also using for slicing:\n\n# Get the second through the fourth element\nwingspan_cm[2:4]\n\n[1] NA NA NA\n\n\n\n\n\nData frames\nTo subset data frames, we need to provide two values: row and column, with a comma between them.\nFor example, to get the element in the 1st row, 1st column:\n\nbirds_df[1, 1]\n\n[1] \"hummingbird\"\n\n\nTo get the element in the 2nd row, 3rd column:\n\nbirds_df[2, 3]\n\n[1] 90\n\n\nTo get the entire 2nd row, leave the column part blank:\n\nbirds_df[2, ]\n\n     species wingspan size n_eggs\n2 bald_eagle      203   90      2\n\n\nAnd to extract the entire 3rd column, leave the row part blank:\n\nbirds_df[, 3]\n\n[1]  7.62 90.00 13.10 21.80\n\n\nTo extract a column, you can also refer to it by name, in multiple ways:\n\nbirds_df$size\n\n[1]  7.62 90.00 13.10 21.80\n\n\n\nbirds_df[, \"size\"]\n\n[1]  7.62 90.00 13.10 21.80\n\n\n\n\n\n Exercise: Subsetting\n\nExtract the 1st-3rd rows and the 4th colum from birds_df.\n\n\n\nClick here for the solution\n\n\nbirds_df[1:3, 4]\n\n[1] 2 2 7\n\n\nOr:\n\nbirds_df[c(1, 2, 3), 4]\n\n[1] 2 2 7\n\n\n\n\nUse the $ notation to extract the n_eggs column from birds_df.\n\n\n\nClick here for the solution\n\n\nbirds_df$n_eggs\n\n[1] 2 2 7 4"
  },
  {
    "objectID": "homework/R.html#miscellaneous",
    "href": "homework/R.html#miscellaneous",
    "title": "Homework: Intro to R",
    "section": "11 Miscellaneous",
    "text": "11 Miscellaneous\n\n11.1 Missing data\nAs R was designed to analyze datasets, it includes the concept of missing data (which is uncommon in other programming languages). Missing data are represented as NA (for “Not Available”).\n\nheights &lt;- c(2, 4, 4, NA, 6)\n\nWhen doing operations on numbers, most functions will return NA if the data you are working with include missing values. It is a safer behavior as otherwise you may overlook that you are dealing with missing data. You can add the argument na.rm=TRUE to calculate the result while ignoring the missing values.\n\nmean(heights)\n\n[1] NA\n\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\n\n\n\n\n11.2 Packages\nThe functions that we have been using so far (and many, many more) are available in any R session as soon as you start R (we refer to this functionality as “base R”). However, when doing specialized analyses such as in microbiomics, rather than coding up everything using the basic building blocks in R, we can load add-on code that will allow us to use “high-level” functions specifically geared towards the effective analyses of such data.\nThis type of add-on code is distributed in R packages. The default repository for R packages is CRAN, and you install CRAN packages with the install.packages() function:\n\n# Don't run this\ninstall.packages(\"tidyverse\")\n\nIf you’re doing bioinformatic analyses in R, as we will be doing, you will encounter packages that are not on CRAN but are on “Bioconductor”. To install a package from Bioconductor, use the BiocManager package – for example:\n\n# Don't run this\ninstall.packages(\"BiocManager\")  # Install the BiocManager package\nBiocManager::install(\"dada2\")    # Install the dada2 package from Bioconductor\n\n\n\n\n\n\n\nInstallation issues at OSC\n\n\n\nThe installation of some packages is tricky at OSC nowadays, and if you were to try the commands above, they would likely fail. With a bit of extra effort, we can install these packages, but during the workshop, we will be using a custom collection of pre-installed packages.\n\n\n\n\n\n11.3 Saving your data\nSome very brief notes on saving your data in R:\n\nWe already saw the use of write.csv() to save data frames, and you can also use one of readr’s writing functions.\nTo save R objects “as is”, which can be useful when you’re working with complex S4 objects that may have taken a long time to generate, like a phyloseq object, you can use:\n\n# Don't run this\n\n# Save an object:\nsaveRDS(my_phyloseq_object, \"my_phyloseq_object.RDS\")\n\n# Load it again in a new R session:\nmy_phyloseq_object &lt;- readRDS(\"my_phyloseq_object.RDS\")\n\nA general recommendation is to not rely on your R session to keep things around, especially “overnight”. Devise your workflow such that you are always saving important objects and results outside of R, and can always use your R script to restart from where you left off.\n\n\n\n\n\n\n\nOptional: Change a setting to not let R save your Workspace\n\n\n\nAlong the line of the above, the default behavior of saving and restoring your “Workspace”, which are all the items (objects) that you create during an R session, is bad practice. Instead, you should always recreate your environment from a script and/or saved files with individual pieces of data.\nChange the following setting to prevent R from saving your Workspace whenever you close R:\n\nClick Tools (top bar, below your browser’s address bar) &gt; Global Options\nIn the pop-up window (stay on the General tab), change the settings under the “Workspace” heading to:\n\n\n\n\n\n\n\n\n\n\n\n11.4 S4 Objects\nWhile the object types we have discussed so far are so-called “S3” objects, we will also be seeing “S4” objects in this workshop. S4 object are commonly used by bioinformatics packages, for instance phyloseq.\nIn a nutshell, S4 objects allow for complicated, multifaceted datasets (e.g. multiple dataframes with and metadata) to be represented in a single object in a standardized way.\nUnlike S3 objects, S4 objects are usually not manipulated by simple assignment with &lt;-, but with specialized functions that are sure to adhere to the strict object definitions."
  },
  {
    "objectID": "homework/R.html#where-to-go-from-here",
    "href": "homework/R.html#where-to-go-from-here",
    "title": "Homework: Intro to R",
    "section": "12 Where to go from here",
    "text": "12 Where to go from here\nThis document only scratched the surface of R, but it has hopefully provided a good starting point for working with R.\nHere are some potential next steps:\n\nLearn about plotting with ggplot2. Start with these two OSU Code Club sessions:\n\nggplot part I\nggplot part II\n\nLearn about data wrangling with tidyverse packages, especially dplyr and tidyr. Start with these two OSU Code Club sessions:\n\nIntroduction to the tidyverse\nTidyverse 2: More dplyr Data Wrangling\n\n\nBoth of those topics and some other material are also covered in this excellent Carpentries workshop R for Reproducible Scientific Analysis.\nIf you want to start with a book, I would recommend Wickham & Grolemund’s “R for Data Science”, which is freely available on the web in a really nice format here.\n\n\n\n\n\n\nWant to try the tidyverse (includes ggplot2) at OSC?\n\n\n\nIf you want to try using the tidyverse in RStudio at OSC now, then load it as follows:\n\n.libPaths(\"/fs/ess/PAS0471/jelmer/R/metabar\")\ndyn.load(\"/fs/ess/PAS0471/jelmer/software/GLPK/lib/libglpk.so.40\", local = FALSE)\nlibrary(tidyverse)"
  },
  {
    "objectID": "homework/R.html#bonus-conditional-subsetting",
    "href": "homework/R.html#bonus-conditional-subsetting",
    "title": "Homework: Intro to R",
    "section": "13 Bonus: conditional subsetting",
    "text": "13 Bonus: conditional subsetting\nAnother common way of subsetting is by using a logical vector of the same length as the original vector: any TRUE will select the element with the same index, while FALSE will not:\n\nwingspans_cm\n\n[1]  11.8 203.0  18.2  27.9\n\n# This will extract the 1st and the 4th element\nwingspans_cm[c(TRUE, FALSE, FALSE, TRUE)]\n\n[1] 11.8 27.9\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 20:\n\n# This will return a logical vector with TRUE for indices that meet the condition\nwingspans_cm &gt; 20\n\n[1] FALSE  TRUE FALSE  TRUE\n\n\n\n# We can use such a vector to select only the values above 20\nwingspans_cm[wingspans_cm &gt; 20]\n\n[1] 203.0  27.9\n\n\n\n\n== and %in%\nYou can test for equality with ==:\n\n\"chickadee\" == \"chickadee\"\n\n[1] TRUE\n\n# Which element(s) of the birds vector equal \"chickadee\"\nbirds == \"chickadee\"\n\n[1] FALSE FALSE  TRUE FALSE\n\n# Extract the element(s) of the birds vector that equal \"chickadee\"\nbirds[birds == \"chickadee\"]\n\n[1] \"chickadee\"\n\n\nIf you want to use a search vector with multiple items, use %in% instead:\n\n# Which element(s) of the birds vector match any of the 3 birds on the right-hand side\nbirds %in% c(\"chickadee\", \"hummingbird\", \"shoebill\")\n\n[1]  TRUE FALSE  TRUE FALSE\n\n# Extract those elements\nbirds[birds %in% c(\"chickadee\", \"hummingbird\", \"shoebill\")] \n\n[1] \"hummingbird\" \"chickadee\"  \n\n\n\n\n\n Exercise: Conditional subsetting\nGiven the following inhabitants of a house, and a list of species to keep:\n\ninhabitants &lt;- c(\"rat\", \"rat\", \"dog\", \"mouse\", \"cat\", \"cat\")\nkeep &lt;- c(\"dog\", \"cat\")\n\n\nWhat do you think the following would return? (Test it and see if you were right.)\n\ninhabitants %in% keep\n\n\nkeep %in% inhabitants\n\n\n\n\nClick here for the solution\n\n\ninhabitants %in% keep\n\n[1] FALSE FALSE  TRUE FALSE  TRUE  TRUE\n\n\n\nkeep %in% inhabitants\n\n[1] TRUE TRUE\n\n\n\n\nExtract the dogs and cats from the inhabitants vector with logical subsetting.\n\n\n\nClick here for the solution\n\n\ninhabitants[inhabitants %in% keep]\n\n[1] \"dog\" \"cat\" \"cat\"\n\n\n\n\n\n\nAttribution\nThis material was modified after material from The Carpentries, especially from this Data Carpentry workshop and this “R for Ecology” workshop."
  },
  {
    "objectID": "homework/R.html#footnotes",
    "href": "homework/R.html#footnotes",
    "title": "Homework: Intro to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe placement of these panes and their content can be customized.↩︎\nYou created this in the shell introduction. If you don’t have it, create it now.↩︎\nIn RStudio, typing Alt + - will write &lt;- in a single keystroke. You can also use = as assignment, but that symbol can have other meanings, and so I recommend sticking with the &lt;- combination.↩︎\nThere are also some other restrictions that are beyond the scope of this introduction. For example, there are some names that cannot be used because they are the names of fundamental keywords in R (e.g., if, else, for, see here for a complete list). In general, it’s also best not to use the names of existing functions, even though this is possible.↩︎\nTechnically, a vector can have a length of 1, so our earlier single numbers were vectors too.↩︎\nThis is technically a type of data structure.↩︎\nNote, that technically, factors are a data structure, but they are more intuitively thought of as a data type↩︎"
  },
  {
    "objectID": "homework/instructions.html",
    "href": "homework/instructions.html",
    "title": "Homework Instructions",
    "section": "",
    "text": "Before the workshop starts, please work your way through the following two pages on this website:\nEach of these may take around 2 hours to complete with no prior experience1."
  },
  {
    "objectID": "homework/instructions.html#footnotes",
    "href": "homework/instructions.html#footnotes",
    "title": "Homework Instructions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n That’s a bit of a guess, we welcome feedback about how long it took you to do this.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amplicon Metabarcoding Workshop",
    "section": "",
    "text": "Day\nTime\nTopic\nBy\n\n\n\n\nWed\n1 - 2 pm\nLecture: Intro to metabarcoding\nTim\n\n\n\n2:15 - 3 pm\nLab: Intro to OSC\nJelmer\n\n\n\n3 - 4:30 pm\nLab: QC & trimming\nJelmer\n\n\nThu\n1 - 2 pm\nLecture: ASV/OTU calling and pipeline considerations\nSoledad\n\n\n\n2:15 - 3:15 pm\nLab: Calling ASVs with the DADA2 pipeline\nTim\n\n\n\n3:30 - 4:30 pm\nLab: Alpha & beta diversity\nTim\n\n\nFri\n1 - 1:30 pm\nLab: Differential abundance with DESeq2\nMelanie\n\n\n\n1:30 - 3 pm\nLab: Network analysis\nMelanie\n\n\n\n3 - 4:40 pm\nLab: Core microbiome analysis\nFiama\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "02_osc.html#a-computational-infrastructure-for-genomics",
    "href": "02_osc.html#a-computational-infrastructure-for-genomics",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 A computational infrastructure for genomics",
    "text": "1 A computational infrastructure for genomics\nA laptop or desktop computer is often not sufficient to work with large-scale genomics data. Additionally, many of the specialized programs that help you analyze your data can only be run through a “command-line interface”.\nThose are some of the reasons that a typical computational infrastructure to do what we may call “command-line genomics” involves the following components:\n\nA supercomputer — in our case, the Ohio Supercomputer Center (OSC) [this session]\nA text editor — I recommend and will demonstrate VS Code [this session]\nThe Unix shell (terminal) [homework and next session]\nR1 for interactive statistical analysis and visualization [homework and Thu + Fri]\n\nWe will be using all of these components during this workshop. This session will provide an introduction to supercomputers in general and to the Ohio Supercomputer Center (OSC) specifically. In all of the lab sessions at this workshop, we’ll continue to work at OSC, so you will get a fair bit of experience with it."
  },
  {
    "objectID": "02_osc.html#high-performance-computing",
    "href": "02_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working with genomics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\nAccess to OSC’s computing power and storage space goes through OSC “Projects”. For this workshop, we have an educational project called PAS2714. If you want to use OSC for your own research after this workshop, you should ask your PI to create an OSC project for this purpose if you don’t have one already.\n\n\n\n\n\n\n\nOSC websites (Click to expand)\n\n\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage."
  },
  {
    "objectID": "02_osc.html#the-structure-of-a-supercomputer-center",
    "href": "02_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nCore / Processor / CPU / Thread\nComponents of a computer (node) that can each (semi-)indendepently be asked to perform a computing task like running a bioinformatics program. For our purposes, we can treat these terms as synonyms.\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: “Pitzer” and “Owens”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\n\n\n\n\nFile systems\nWhile OSC has several distinct file systems, we will only be working in a so-called project directory (located in /fs/ess) for our workshop’s OSC Project, PAS2714.\n\n\n\n\n\n\nOverview of OSC file systems (Click to expand)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nOSC Project\n\n\n\n\n\n\n\n\n\n\n\n\nDirectory is just another word for folder, often written as “dir” for short\n\n\n\n\n\n\n\n\n\nLogin nodes vs. compute noes\nLogin nodes an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and cannot be “reserved”. As such, login nodes are meant only to do things like organizing your files and creating scripts, and are not meant for any serious computing.\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). One way of doing so is through the OnDemand website, as we’ll do in a minute to start a VS Code session on a compute node.\n\n\n\n\n\n\nSide note: What works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We’ll learn much more about these later on in the course, but here is an overview:\n\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses."
  },
  {
    "objectID": "02_osc.html#osc-ondemand",
    "href": "02_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2714, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2714 project’s “scratch” directory (/fs/scratch/PAS2714)\nThe PAS2714 project’s “project” directory (/fs/ess/PAS2714)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n In the Files dropdown menu, click on our focal directory /fs/ess/PAS2714.\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files2 — see the buttons across the top.\n\n\n\n\n\n\nDidn’t create your own dir in the Unix shell homework? Click here and follow these instructions to do it now.\n\n\n\n\n\n\nClick on the users dir in /fs/ess/PAS714.\nCreate your own dir by clicking the New Directory button towards the top.\nPlease give it the exact same name as your OSC username (also match the capitalization!).\n\n(If you’re not sure what your username is — look at the right side of the blue top bar, “Logged in as”:)\n\n\n\n\n\n\n\n\n\n\n\n4.2 Clusters: Unix shell access\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI’m selecting a shell on the Pitzer supercomputer (“Pitzer Shell Access”), which will open a new browser tab:\n\n\n\n\n\nHowever, from now on, we’ll be accessing a Unix shell inside the VS Code text editor, which also gives us some additional functionality in a user-friendly way.\n\n\n\n4.3 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\nSelect VS Code using the “Code Server” button:\n\n\n\n\n\n\n\nInteractive Apps like VS Code and RStudio run on compute nodes (not login nodes). Because compute nodes always need to be “reserved”, we have to fill out a form and specify the following details:\n\nThe OSC “Project” that we want to bill for the compute node usage: PAS2714.\nThe “Number of hours” we want to make a reservation for: 3\nThe “Working Directory” for the program: your personal folder in /fs/ess/PAS2714/users.\nThe “Codeserver Version”: 4.8 (most recent)\n[^9]: Note that we’ll be kicked off as soon as that amount of time has passed! [^10]: This will be your starting location in the file system; we’ll talk more about working dirs in a little bit.\n\n\n\n\n\n\n\n\nClick on Launch at the bottom, which will send your request to the “compute job” scheduler.\nFirst, your job will be “Queued” — that is, waiting for the job scheduler to allocate compute node resources to it:\n\n\n\n\n\n\n\nYour job is typically granted resources within a few seconds (the card will then say “Starting”), and should be ready for usage (“Running”) in another couple of seconds:\n\n\n\n\n\n\n\nOnce it appears, click on the blue Connect to VS Code button to open VS Code in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:"
  },
  {
    "objectID": "02_osc.html#vs-code",
    "href": "02_osc.html#vs-code",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "5 VS Code",
    "text": "5 VS Code\n\n5.1 What is VS Code?\nVS Code (Visual Studio Code, AKA “Code Server”) is basically a fancy text editor. To emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. For our purposes:\n\nVS code will be our IDE for Unix shell code\nRStudio will be our IDE for R\n\n\n\n\n5.2 The VS Code User Interface\n\n\n\n\n\n\nSide bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle (wide) Side Bar options — but we’ll only use the default selection, the Explorer (file browser)\n\n\n\nEditor pane\nThe main part of VS Code is the editor pane, where we can open files like scripts and other text files, and images.\n\n\nTerminal (with a Unix shell)\n Open a terminal by clicking      =&gt; Terminal =&gt; New Terminal.\n\n\n\n Exercise: Try a few color themes\n\nAccess the “Color Themes” option by clicking  =&gt; Color Theme.\nTry out a few themes and see pick one you like!\n\n\n\n\n\n5.3 A folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\n(If you need to switch folders, click      =&gt;   File   =&gt;   Open Folder.)\n\n\n\n\n\n\n\nSome VS Code tips and tricks (Click to expand)\n\n\n\n\n\n\nResizing panes\nYou can resize panes (the terminal, editor, and side bar) by hovering your cursor over the borders and then dragging.\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P).\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.) A couple of useful keyboard shortcuts are highlighted below."
  },
  {
    "objectID": "02_osc.html#further-reading",
    "href": "02_osc.html#further-reading",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Further reading",
    "text": "Further reading\n\nOSC’s learning resources\n\nAn extended version of this introduction\nOSC’s online asynchronous courses\nOSC’s new User Resource Guide3"
  },
  {
    "objectID": "02_osc.html#footnotes",
    "href": "02_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr Python↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available but are beyone the scope of this introduction.↩︎\n Attribution: This page uses material from an OSC Introduction written by Mike Sovic and from OSC’s Kate Cahill Software Carpentry introduction to OSC.↩︎"
  },
  {
    "objectID": "ref/ref_software.html#overview",
    "href": "ref/ref_software.html#overview",
    "title": "Software management",
    "section": "Overview",
    "text": "Overview\nAt supercomputers like OSC, there are often system-wide installations of a number of bioinformatics programs. We do need to “load” such programs before we can use them. However, OSC’s collection of bioinformatics programs is unfortunately not comprehensive, and some of the available programs only come in relatively old versions.\nWe therefore also need another way to make bioinformatics programs available to ourselves. Two common methods are the Conda software management program and containers. We will talk about loading MCIC’s Conda environments, while the at-home reading covers installing software yourself with Conda, and using containers downloaded from the internet."
  },
  {
    "objectID": "ref/ref_software.html#loading-software-at-osc-with-lmod-modules",
    "href": "ref/ref_software.html#loading-software-at-osc-with-lmod-modules",
    "title": "Software management",
    "section": "1 Loading software at OSC with Lmod modules",
    "text": "1 Loading software at OSC with Lmod modules\nOSC administrators manage software with the “Lmod” system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it. That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nWe can load, unload, and search for available software modules using the module command and its various subcommands.\n\n1.1 Checking whether a program is available\nThe OSC website has a list of installed software. You can also search for available software in the shell using two subtly different module subcommands1:\n\nmodule spider lists all modules that are installed.\nmodule avail lists modules that can be directly loaded given the current environment (i.e., taking into account which other software has been loaded).\n\nSimply running module spider or module avail would spit out the full lists of installed/available programs — it is more useful to add a search term as an argument to these commands — below, we’ll search for the Conda distribution “miniconda”, with each of these two subcommands:\nmodule spider miniconda\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  miniconda3:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        miniconda3/4.10.3-py37\n        miniconda3/4.12.0-py38\n        miniconda3/4.12.0-py39\n        miniconda3/23.3.1-py310\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"miniconda3\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider miniconda3/4.12.0-py39\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nmodule avail miniconda\n------------------------------------------------------------------------------------------------------ /apps/lmodfiles/Core -------------------------------------------------------------------------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py38    miniconda3/4.12.0-py39    miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\nAs stated at the bottom of the output below, the (D) in the module avail output above marks the default version of the program: this is the version of the program that will be loaded if we don’t specify a version ourselves (see examples below). The module spider command does not provide this information.\n\n\n\n1.2 Loading software\nAll other Lmod software functionality is also accessed using module subcommands. For instance, to make a program available to us we use the load subcommand:\n# Load a module:\nmodule load miniconda3               # Load the default version\nmodule load miniconda3/23.3.1-py310  # Load a specific version\n\n\n\n\n\n\nModules do not remain loaded across separate shell sessions\n\n\n\nModule loading does not persist across shell sessions. Whenever you get a fresh shell session (including but not limited to after logging into OSC again), you will have to (re)load any modules you want to use!\n\n\nTo check which modules have been loaded, use module list. Its output will also include automatically loaded modules, so for example, if you loaded miniconda3/23.3.1-py310, you should see the following list where the miniconda3 module is listed as the 6th entry:\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/23.3.1-py310\nOccasionally, when you run into conflicting (mutually incompatible) modules, it can be useful to unload modules, which you can do as follows:\nmodule unload miniconda3    # Unload a specific module\nmodule purge                # Unload all modules\n\n\n\n1.3 A practical example: FastQC again\nHere, we’ll load the module for FastQC again. First, let’s confirm that we indeed cannot currently use FastQC by running the fastqc command with the --help option:\nfastqc --help\nbash: fastqc: command not found\n\n\n\n\n\n\nHelp!\n\n\n\nMany command-line programs can be run with with a --help (and/or -h) flag, and this is often a good thing to try first, since it will tell use whether we can use the program — and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether FastQC is available at OSC, and if so, in which versions:\nmodule avail fastqc\nfastqc/0.11.8\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be a reason to specify the version when we load the FastQC module?\n\n\n\n\n\nWhen we use the module load command inside a script, specifying a version would:\n\nEnsure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nMake it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\nmodule load fastqc/0.11.8\nNow, we can retry our --help attempt:\nfastqc --help\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN  \n# [...truncated...]\n\nOn your own: load miniconda3\nThe miniconda3 module will allow us to use Conda software environments, which we’ll talk about more below.\n\nLet’s start with a clean sheet by running module purge.\nLoad the default version of miniconda3, and then check which version was loaded.\n\n\n\n\n\n\n\nSolution (Click here)\n\n\n\n\n\nmodule load miniconda3\n\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/4.10.3-py37\nThe version 4.10.3-py37 was loaded.\n\n\n\n\nNow load the latest version of miniconda3 without unloading the earlier version first. What output do you get?\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\nLmod detected that you tried to load a different version of a software that was already loaded, so it changes the version and tells you about it:\nmodule load miniconda3/23.3.1-py310\nThe following have been reloaded with a version change:\n  1) miniconda3/4.10.3-py37 =&gt; miniconda3/23.3.1-py310"
  },
  {
    "objectID": "ref/ref_software.html#when-software-isnt-installed-at-osc",
    "href": "ref/ref_software.html#when-software-isnt-installed-at-osc",
    "title": "Software management",
    "section": "2 When software isn’t installed at OSC",
    "text": "2 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than what is available. In that case, the following two are generally your best options:\n\nConda, which creates software environments that you can activate much like the Lmod modules.\nContainers, which are self-contained software environments that include operating systems, akin to mini virtual machines. While Docker containers are most well-known, OSC uses Apptainer (formerly known as Singularity) containers.\n\n\n\n\n\n\n\nOther options to install software / get it installed\n\n\n\n\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through Lmod / module commands).\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”2 and will often have difficulties with “dependencies”3.\n\n\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nNext, we’ll talk about Conda and using the MCIC’s Conda environments. The at-home reading includes installing software yourself with Conda, and using containers downloaded from the internet."
  },
  {
    "objectID": "ref/ref_software.html#intro-to-conda-using-mcics-conda-environments",
    "href": "ref/ref_software.html#intro-to-conda-using-mcics-conda-environments",
    "title": "Software management",
    "section": "3 Intro to Conda & using MCIC’s Conda environments",
    "text": "3 Intro to Conda & using MCIC’s Conda environments\nThe Conda software can create so-called environments in which one can install one or more software packages.\nAs you can see in the at-home reading below, as long as a program is available in one of the online Conda repositories (which is nearly always for bioinformatics programs), then installing it is quite straightforward, doesn’t require admin privileges, and is done with a procedure that is nearly identical regardless of the program you are installing.\nHowever, at OSC, you will probably not even have to install anything yourself, at least not if you are following “standard” workflows with common data like RNAseq. To this end, I maintain an “MCIC collection” of Conda environments that anyone can use.\nA Conda environment is just a directory, and since all the environments in this collection are in the same place at OSC, you can list the MCIC Conda environments as follows:\nls /fs/ess/PAS0471/jelmer/conda\nabricate-1.0.1  bedops-2.4.39  checkm-1.2.0   entrez-direct    htseq-2.0.2          longstitch-1.0.3  nanopolish-0.13.2    prokka            repeatmasker-4.1.2.p1         samtools                star\nagat-0.9.1      bedtools       clinker        evigene          inspector-1.2.0      mafft             ncbi-datasets        pseudofinder      repeatmodeler-2.0.3           scoary                  subread-2.0.1\nalv             bioawk         clonalframeml  fastp            interproscan-5.55    maskrc-svg        nextdenovo-env       purge_dups-1.2.6  resfinder                     seqkit                  tgsgapcloser\namrfinderplus   biopython      codan-1.2      fastqc           iqtree               medaka-1.7.2      nextflow             pycoqc-2.5.2      resistomeanalyzer-2018.09.06  seqtk                   tracy-0.7.1\nantismash       bit            cogclassifier  fastq-dl         justorthologs-0.0.2  metaxa-2.2.3      orna-2.0             qiime2-2022.8     rgi-5.2.1                     signalp-6.0             transabyss-2.0.1\nariba-2.14.6    blast          cutadapt       fasttree-2.1.11  kallisto-0.48.0      minibusco         orthofinder          qualimap-env      r-metabar                     sistr-1.1.1             transdecoder-5.5.0\nastral-5.7.8    bowtie2-2.5.0  deeploc        filtlong-env     kat-2.4.2            minimap2-2.24     orthofisher          quast-5.0.2       rnaquast-2.2.1                smartdenovo-env         treetime\naswcli          bracken-2.6.1  deeptmhmm      flye-2.9.1       knsp-3.1             mlst              panaroo              quickmerge-env    roary-3.13                    snippy-4.6.0            trimgalore\nbactopia        braker2-env    deeptmhmm2     fmlrc2-0.1.7     kofamscan            mlst_check        phylofisher          racon-1.5.0       r-rnaseq                      snp-sites-2.5.1         trimmomatic-0.39\nbactopia-dev    busco          diamond        gcta             kraken2-2.1.2        mobsuite          pilon-1.24           ragtag-2.1.0      rsem-1.3.3                    soapdenovo-trans-1.0.4  trinity-2.13.2\nbakta           bwa-0.7.17     dwgsim         gffread-0.12.7   krakentools-1.2      multiqc           pkgs                 rascaf            rseqc-env                     sortmerna-env           unicycler\nbase            bwa-mem-2.2.1  eggnogmapper   gubbins          krona                mummer4           plasmidfinder-2.1.6  rcorrector-1.0.5  r_tree                        sourmash                virulencefinder\nbbmap           cactus         emboss         hisat2           liftoff-1.6.3        nanolyse-1.2.1    plink2               r-deseq           sabre-1.0                     spades-3.15.5           wtdbg-2.5\nbcftools        cgmlst         entap-0.10.8   hmmer            links-2.0.1          nanoplot          porechop             recognizer-1.8.3  salmon                        sra-tools\nThis is organized similarly to the Lmod modules in that there’s generally one separate environment for one program (and all its dependencies), and the environment is named after that program.\nThe naming of the environments is unfortunately not entirely consistent: many environments include the version number of the program, but many others do not. (Generally speaking, for environments without version numbers, you should expect the version of the program to be very recent, as I try to keep these up-to-date4).\nThis collection includes Conda environments for several programs we need during RNAseq analysis that are not installed at OSC, such as MultiQC, TrimGalore, and SortMeRNA.\n\n\n3.1 Activating Conda environments\nConda itself is already installed at OSC through Miniconda, but we always need to load its module before we can use it:\nmodule load miniconda3\nAs mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system. But whereas we use the term “load” for Lmod modules, we use “activate” for Conda environments — it means the same thing.\nAlso like Lmod, there is a main command (conda) and several subcommands (deactivate, create, install, update) for different functionality. However, for historical reasons, the most foolproof way to activate a Conda environment is to use source activate rather than the expected conda activate — for instance:\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n(multiqc) [jelmer@p0085 rnaseq-intro]$\n\n\n\n\n\n\nConda environment indicator\n\n\n\nWhen we have an active Conda environment, its name is displayed in front of our prompt, as depicted above with (multiqc).\n\n\nAfter we have activated the MultiQC environment, we should be able to actually use the program. To test this, we’ll simply run the multiqc command with the --help option like we did for FastQC:\n\nmultiqc --help\n\n /// MultiQC 🔍 | v1.15                                                                                                                                                                                                            \n                                                                                                                                                                                                                                   \n Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]                                                                                                                                                                                     \n                                                                                                                                                                                                                                   \n MultiQC aggregates results from bioinformatics analyses across many samples into a single report.                                                                                                                                 \n It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.                                                       \n To run, supply with one or more directory to scan for analysis results. For example, to run in the current working directory, use 'multiqc .'                                                                                     \n                                                                                                                                                                                                                                   \n╭─ Main options ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ --force            -f  Overwrite any existing reports                                                                                                                                                                           │\n│ --config           -c  Specific config file to load, after those in MultiQC dir / home dir / working dir. (PATH)                                                                                                                │\n│ --cl-config            Specify MultiQC config YAML on the command line (TEXT)                                                                                                                                                   │\n│ --filename         -n  Report filename. Use 'stdout' to print to standard out. (TEXT)                                                                                                                                           │\n│ --outdir           -o  Create report in the specified output directory. (TEXT)                                                                                                                                                  │\n│ --ignore           -x  Ignore analysis files (GLOB EXPRESSION)                                                                                                                                                                  │\n│ --ignore-samples       Ignore sample names (GLOB EXPRESSION)                                                                                                                                                                    │\n│ --ignore-symlinks      Ignore symlinked directories and files                                                                                                                                                                   │\n│ --file-list        -l  Supply a file containing a list of file paths to be searched, one per row                                                                                                                                │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n[...truncated...]\n\nUnlike Lmod / module load, Conda will by default only keep a single environment active. Therefore, when you have one environment activate and then activate another, you will switch environments:\n# After running this command, the multiqc env will be active\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# After running his command, the trimgalore env will be active...\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# ...but the multiqc env will no longer be:\nmultiqc --help\nbash: multiqc: command not found...\nHowever, the conda activate --stack option enables you to have multiple Conda environments active at once:\n# Assuming you had trimgalore activated, now add the multiqc env:\nconda activate --stack /fs/ess/PAS0471/jelmer/conda/multiqc\n\nmultiqc --help\n# (Output not shown, but this should print help info)\n\ntrim_galore --help\n# (Output not shown, but this should print help info)\nNote that the command is conda activate --stack and not source activate --stack!\n\n\n\n3.2 Lines to add to your shell script\nAs mentioned above for Lmod modules, you need to load them in every shell session you want to use them — and the same is true for Conda environments. While Conda enviroments that are loaded in your interactive shell environment will “carry over” to the environment in which your script runs (even when you submit them to the Slurm queue with sbatch; topic of the next session), it is good practice to always include the necessary code to load/activate programs in your shell scripts.\nWhen the program you will run in a script is in an Lmod module, this only involves a module load call — e.g., for FastQC:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load fastqc\nWhen the program you will run in a script is in a Conda environment, this entails a module load command to load Conda itself, followed by a source activate command to load the relevant Conda environment — e.g. for MultiQC:\n#!/bin/bash\n\n# Load software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# Strict/safe Bash settings \nset -euo pipefail\n\n\n\n\n\n\nPerils of Conda environments inside scripts\n\n\n\n\nIn the example above, the set -euo pipefail line was moved below the source activate command, because the Conda activation procedure can otherwise result in “unbound variable” errors.\nAnother unfortunate aspect of Conda environments at OSC is the following. Problems can occur when you have a Conda environment active in your interactive shell while you submit a script as a batch job that activates a different environment.\nTherefore, it is generally a good idea to not have any Conda environments active in your interactive shell when submitting batch jobs5. To deactivate the currently active Conda environment, simply type conda deactivate without any arguments:\nconda deactivate"
  },
  {
    "objectID": "ref/ref_software.html#bonus-i-creating-your-own-conda-environments",
    "href": "ref/ref_software.html#bonus-i-creating-your-own-conda-environments",
    "title": "Software management",
    "section": "Bonus I: Creating your own Conda environments",
    "text": "Bonus I: Creating your own Conda environments\nWhen you want to create your own Conda environments and install programs, make sure to load the most recent miniconda3 module, which is currently not the default one. This is because installation has become much quicker and less likely to fail than in earlier versions. (Note that when we are just loading environments, like above, the version doesn’t matter).\nAs of August 2023, the most recent miniconda version is 23.3.1-py310 (recall that you can list available versions with module spider):\nmodule load miniconda3/23.3.1-py310\n\nOne-time Conda configuration\nBefore we can create our own environments, we first have to do some one-time configuration6. This will set the Conda “channels” (basically, software repositories) that we want to use when we install programs, including the relative priorities among channels (since one program may be available from multiple channels).\nWe can do this configuration with the config subcommand — run the following commands in your shell:\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\nLet’s check whether the configuration was successfully saved:\nconda config --get channels\n--add channels 'defaults'   # lowest priority\n--add channels 'bioconda'\n--add channels 'conda-forge'   # highest priority\n\n\n\n3.3 Example: Creating an environment for Trim Galore!\nTo practice using Conda, we will now create a Conda environment with the program Trim Galore! installed. Trim Galore! is a commonly used tool for quality trimming and adapter trimming of FASTQ files — we’ll learn more about it in a later session, since we will use it on our RNAseq data. It does not have a system-wide installation at OSC, unfortunately.\nHere is the command to all at once create a new Conda environment and install Trim Galore! into that environment:\n\n# (Don't run this)\nconda create -y -n trim-galore -c bioconda trim-galore\n\nLet’s break that command down:\n\ncreate is the Conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation once Conda has determined what needs to be installed.\nFollowing the -n option, we can specify the name of the environment, so -n trim-galore means that we want our environment to be called trim-galore. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a “channel” (repository) from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe trim-galore at the end of the line simply tells Conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\nSpecifying a version\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name, and may also want to include that version number in the Conda environment’s name — try running the command below:\nconda create -y -n trim-galore-0.6.10 -c bioconda trim-galore=0.6.10\nCollecting package metadata (current_repodata.json): done  \nSolving environment: done\n# [...truncated...]\n\n\n\n\n\n\nSee the full output when I ran this command (Click to expand)\n\n\n\n\n\n\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 23.3.1\n  latest version: 23.7.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.7.2\n\n\n\n## Package Plan ##\n\n  environment location: /fs/project/PAS0471/jelmer/conda/trimgalore-0.6.10\n\n  added / updated specs:\n    - trim-galore=0.6.10\n\n\nThe following packages will be downloaded:\n\n    | package            | build                                            |\n    | ------------------ | ------------------------------------------------ |\n    | bz2file-0.98       | py_0           9 KB  conda-forge                 |\n    | cutadapt-1.18      | py37h14c3975_1         206 KB  bioconda          |\n    | fastqc-0.12.1      | hdfd78af_0        11.1 MB  bioconda              |\n    | pigz-2.6           | h27826a3_0          87 KB  conda-forge           |\n    | python-3.7.12      | hf930737_100_cpython        57.3 MB  conda-forge |\n    | trim-galore-0.6.10 | hdfd78af_0          45 KB  bioconda              |\n    | xopen-0.7.3        | py_0          11 KB  bioconda                    |\n    ------------------------------------------------------------\n                                           Total:        68.8 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.9-hd590300_0 \n  bz2file            conda-forge/noarch::bz2file-0.98-py_0 \n  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 \n  ca-certificates    conda-forge/linux-64::ca-certificates-2023.7.22-hbcca054_0 \n  cairo              conda-forge/linux-64::cairo-1.16.0-hbbf8b49_1016 \n  cutadapt           bioconda/linux-64::cutadapt-1.18-py37h14c3975_1 \n  expat              conda-forge/linux-64::expat-2.5.0-hcb278e6_1 \n  fastqc             bioconda/noarch::fastqc-0.12.1-hdfd78af_0 \n  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-hab24e00_0 \n  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n  freetype           conda-forge/linux-64::freetype-2.12.1-hca18f0e_1 \n  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n  giflib             conda-forge/linux-64::giflib-5.2.1-h0b41bf4_3 \n  graphite2          conda-forge/linux-64::graphite2-1.3.13-h58526e2_1001 \n  harfbuzz           conda-forge/linux-64::harfbuzz-7.3.0-hdb3a94d_0 \n  icu                conda-forge/linux-64::icu-72.1-hcb278e6_0 \n  keyutils           conda-forge/linux-64::keyutils-1.6.1-h166bdaf_0 \n  krb5               conda-forge/linux-64::krb5-1.21.2-h659d440_0 \n  lcms2              conda-forge/linux-64::lcms2-2.15-haa2dc70_1 \n  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.40-h41732ed_0 \n  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n  libcups            conda-forge/linux-64::libcups-2.3.3-h4637d8d_4 \n  libdeflate         conda-forge/linux-64::libdeflate-1.18-h0b41bf4_0 \n  libedit            conda-forge/linux-64::libedit-3.1.20191231-he28a2e2_2 \n  libexpat           conda-forge/linux-64::libexpat-2.5.0-hcb278e6_1 \n  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n  libgcc-ng          conda-forge/linux-64::libgcc-ng-13.1.0-he5830b7_0 \n  libglib            conda-forge/linux-64::libglib-2.76.4-hebfc3b9_0 \n  libgomp            conda-forge/linux-64::libgomp-13.1.0-he5830b7_0 \n  libiconv           conda-forge/linux-64::libiconv-1.17-h166bdaf_0 \n  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-2.1.5.1-h0b41bf4_0 \n  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0 \n  libpng             conda-forge/linux-64::libpng-1.6.39-h753d276_0 \n  libsqlite          conda-forge/linux-64::libsqlite-3.42.0-h2797004_0 \n  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.1.0-hfd8a6a1_0 \n  libtiff            conda-forge/linux-64::libtiff-4.5.1-h8b53f26_0 \n  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n  libwebp-base       conda-forge/linux-64::libwebp-base-1.3.1-hd590300_0 \n  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n  libzlib            conda-forge/linux-64::libzlib-1.2.13-hd590300_5 \n  ncurses            conda-forge/linux-64::ncurses-6.4-hcb278e6_0 \n  openjdk            conda-forge/linux-64::openjdk-20.0.0-h8e330f5_0 \n  openssl            conda-forge/linux-64::openssl-3.1.2-hd590300_0 \n  pcre2              conda-forge/linux-64::pcre2-10.40-hc3806b6_0 \n  perl               conda-forge/linux-64::perl-5.32.1-4_hd590300_perl5 \n  pigz               conda-forge/linux-64::pigz-2.6-h27826a3_0 \n  pip                conda-forge/noarch::pip-23.2.1-pyhd8ed1ab_0 \n  pixman             conda-forge/linux-64::pixman-0.40.0-h36c2ea0_0 \n  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n  python             conda-forge/linux-64::python-3.7.12-hf930737_100_cpython \n  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n  setuptools         conda-forge/noarch::setuptools-68.0.0-pyhd8ed1ab_0 \n  sqlite             conda-forge/linux-64::sqlite-3.42.0-h2c6b66d_0 \n  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 \n  trim-galore        bioconda/noarch::trim-galore-0.6.10-hdfd78af_0 \n  wheel              conda-forge/noarch::wheel-0.41.1-pyhd8ed1ab_0 \n  xopen              bioconda/noarch::xopen-0.7.3-py_0 \n  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002 \n  xorg-inputproto    conda-forge/linux-64::xorg-inputproto-2.3.2-h7f98852_1002 \n  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.6-h8ee46fc_0 \n  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n  xorg-libxi         conda-forge/linux-64::xorg-libxi-1.7.10-h7f98852_0 \n  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.3.0-hd590300_1 \n  xorg-libxtst       conda-forge/linux-64::xorg-libxtst-1.2.3-h7f98852_1002 \n  xorg-recordproto   conda-forge/linux-64::xorg-recordproto-1.14.2-h7f98852_1002 \n  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5 \n  zstd               conda-forge/linux-64::zstd-1.5.2-hfc55251_7 \n\n\n\nDownloading and Extracting Packages\n                                                                                                                                                                                                                                   \nPreparing transaction: done                                                                                                                                                                                                        \nVerifying transaction: done                                                                                                                                                                                                        \nExecuting transaction: done                                                                                                                                                                                                        \n#                                                                                                                                                                                                                                  \n# To activate this environment, use                                                                                                                                                                                                \n#                                                                                                                                                                                                                                  \n#     $ conda activate trimgalore-0.6.10\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\n\n\n\nNow, you should be able to activate the enviroment (using just it’s name – see the box below):\n# Activate the environment:\nsource activate trim-galore\n\n# Test if TrimGalore can be run - note, the command is 'trim_galore': \ntrim_galore --help\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nSpecifying the full path to the environment dir\n\n\n\nYou may have noticed above that we merely gave the enviroment a name (trim-galore or trim-galore-0.6.10), and did not tell it where to put this environment. Similarly, we were able to activate the environment with just its name. Conda assigns a personal default directory for its environments, somewhere in your Home directory.\nYou can install environments in a different location with the -p (instead of -n) option, for example:\nmkdir -p /fs/scratch/PAS0471/$USER/conda\nconda create -y -p /fs/scratch/PAS0471/$USER/conda/trim-galore -c bioconda trim-galore\nAnd when you want to load someone else’s Conda environments, you’ll always have to specify the full path to environment’s dir, like you did when loading an MCIC Conda environment above.\n\n\n\n\n\n\n3.4 Finding the Conda installation info online\nMinor variations on the conda create command above can be used to install almost any program for which a Conda package is available, which is the vast majority of open-source bioinformatics programs!\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its Conda package’s name is\nWhich Conda channel we should use\nWhich versions are available\n\nMy strategy to finding this out is to simply Google the program name together with “conda”, e.g. “cutadapt conda” if I wanted to install the CutAdapt program. Let’s see that in action:\n\n\n\nClick on that first link (it should always be the first Google hit):\n\n\n\n\nBuild the installation command\nI always take the top of the two example installation commands as a template, which is here: conda install -c bioconda cutadapt.\nYou may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated Conda environment. Since our strategy here –and my general strategy– is to create a new environment each time you’re installing a program, just installing a program into whatever environment is currently active is not a great idea. To use the install command with a new environment, the strategy would be to first create an “empty” environment, and then run the install command.\nHowever, we saw above that we can do all of this in a single command. To build this create-plus-install command, all we need to do is replace install in the example command on the Conda website by create -y -n &lt;env-name&gt;. Then, our full command (without version specification) will be:\nconda create -y -n cutadapt -c bioconda cutadapt\nTo see which version will be installed by default, and to see which older versions are available:\n\n\n\nFor almost any other program, you can use the exact same procedure to find the Conda package and install it!\n\n\n\n\n\n\nA few more Conda commands to manage your environments\n\n\n\n\nExport a plain-text “YAML” file that contains the instructions to recreate your currently-active environment (useful for reproducibility!)\nconda env export &gt; my_env.yml\nAnd you can use the following to create a Conda environment from such a YAML file:\nconda env create -n my_env --force --file my_env.yml\nRemove an environment entirely:\nconda env remove -n cutadapt\nList all your conda environments:\nconda env list\nList all packages (programs) installed in an environment — due to dependencies, this can be a long list, even if you only actively installed one program:\nconda list -p /fs/ess/PAS0471/jelmer/conda/multiqc\n\n\n\n\n\n\n\n\n\nUse one environment per program (as here) or one per research project\n\n\n\nBelow are two reasonable ways to organize your Conda environments, and their respective advantages:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nEven though it might seem easier, a third alternative, to simply install all programs across all projects in one single environment, is not recommended. This doesn’t benefit reproducibility, and your environment is likely to stop functioning properly sooner or later.\n(A side note: even when you want to install a single program, multiple programs are in fact nearly always installed: the programs that your target program depends on, i.e. “dependencies”.)"
  },
  {
    "objectID": "ref/ref_software.html#bonus-ii-using-apptainer-containers",
    "href": "ref/ref_software.html#bonus-ii-using-apptainer-containers",
    "title": "Software management",
    "section": "Bonus II: Using Apptainer containers",
    "text": "Bonus II: Using Apptainer containers\nBesides Conda, containers are another way to use bioinformatics programs at OSC that don’t have system-wide installations.\nContainers are similar to Virtual Machines and different from Conda environments in that they come with an entire operating system. This makes creating your own container “image” (see box below on terminology) much more involved than creating a Conda environment, and we will not cover that here.\nHowever, there are pre-existing container images available for most bioinformatics programs, and they can be easily found, downloaded, and used.\n\n\n\n\n\n\nContainer terminology\n\n\n\n\nContainer image: File (Apptainer) or files (Docker) that contain the container application.\nContainer (sensu stricto): A running container image.\nDefinition file (Apptainer) / Dockerfile (Docker): A plain text file that contains the recipe to build a container image.\n\n\n\nAmong container platforms, Apptainer (formerly known as Singularity) and especially Docker are the most widely used ones. At supercomputers like OSC, however, only Apptainer containers can be used. Luckily, the Apptainer program can work with Docker container images: it will convert them on the fly.\n\nFinding container images online\nThere are several online repositories with publicly available container images, but I would recommend BioContainers https://biocontainers.pro/registry or Quay.io https://quay.io/biocontainers.\nFor example, let’s look on the BioContainers website for a TrimGalore container image:\n\n\n\n\nThe search result on the BioContainers website after entering “trim galore” in the search box.\n\n\n\nClick on the only entry that is shown, trim-galore, which will get you to a page like this:\n\n\n\n\n\nAs you can see, this website also includes Conda installation instructions — to see the container results, scroll down and you should see this:\n\n\n\n\nAfter scrolling down on the results page, you should see a recent available container image. Note that the command shown is singularity run, but we will use the more up-to-date apptainer run later.\n\n\n\nThe version tag that is shown (0.6.9--hdfd78af_0 above) pertains to the version of TrimGalore, but the result that is shown here is not will always the container image(s) with the most recent version. To see a list of all available images, click on the Packages and Containers tab towards the top, and then sort by Last Update:\n\n\n\n\nThe logo with the large S depicts Singularity/Apptainer containers.\n\n\n\nWhenever both a Singularity/Apptainer and a Docker image for the desired version of the program is available, use the Singularity/Apptainer image. This is because those don’t have to be converted, while Docker images do. But when the version you want is only available as a Docker image, that will work too: as mentioned above, it will be automatically converted to the proper format.\n\n\n\nRunning a container image\nWhen you’ve found a container image that you want to use, copy its URL from the BioContainers website. For example, for the most recent TrimGalore version as of September 2023: https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0.\nYou could also copy the full command — however, we will modify that in two ways:\n\nWe will use the more up-to-date apptainer command7\nWe’ll use the exec subcommand instead of run, which allows us to enter a custom command to run in the container (the run subcommand would only run some preset default action, which is rarely useful for our purposes).\n\nAs such, our base command to run TrimGalore in the container will be as follows:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0\n# (Don't run this, we'll need to add a TrimGalore command)\n\n\n\n\n\n\nYou can’t use the Docker URL as-is\n\n\n\nIf you want to use a Docker container, the listed quasi-URL on BioContainers will start with “quay.io”. In your apptainer exec command, you need to preface this URL with docker://. For instance:\napptainer exec docker://quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_0\n\n\nAfter the code above, we would finish our command by simply entering a TrimGalore command in the exact same way as we would when running TrimGalore outside of the context of a container. For example, to just print the help info like we’ve been doing before, the TrimGalore command is:\ntrim_galore --help\nAnd to run that inside the container, our full command will be:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0 \\\n    trim_galore --help\nINFO:    Downloading network image\n321.4MiB / 321.4MiB [===================================================================================================================================] 100 % 3.0 MiB/s 0s\nWARNING: Environment variable LD_PRELOAD already has value [], will not forward new value [/apps/xalt/xalt/lib64/libxalt_init.so] from parent process environment\n\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nNote\n\n\n\n\nThe Apptainer/Singularity software does not need to be loaded at OSC, it is always automatically loaded.\nThe \\ in the code above allows us to continue a command on another line.\n\n\n\nSo, all that is different from running a program inside a container versus a locally installed program, is that you prefix apptainer exec &lt;URL&gt; when using a container.\nThe first time you run this command, the container will be downloaded, which can take a few minutes (by default it will be downloaded to ~/.apptainer/cache, but you can change this by setting the $APPTAINER_CACHEDIR environment variable). After that, the downloaded image will be used and the command should be executed about as instantaneously as when running TrimGalore outside of a container.\nYou will keep seeing the warning WARNING: Environment variable LD_PRELOAD [...] whenever you run a container, but this is nothing to worry about.\nFinally, the --help option above can also simply be replaced by a host of other TrimGalore options and arguments so as to actually trim a pair of FASTQ files, i.e. with input and output files. You can just specify the paths to those files in the same way as without a container, this will work out of the box!\n\n\n\n\n\n\nWhen to use a Container versus Conda\n\n\n\n\nCurrently, my default is to first try installation with Conda. But I will try a container when installing a program through Conda fails, or my Conda environment misbehaves (e.g., memory errors with dumped cores).\nWhen you need multiple programs in quick succession or in a single command (e.g., you’re piping the output of one program into a second program), it can be more convenient to have those programs installed in a single environmnent or container. Pre-built multi-program containers are not as easy to find. And since building your own Conda environment is easier than building your own container, this is a situation where you might prefer Conda."
  },
  {
    "objectID": "ref/ref_software.html#footnotes",
    "href": "ref/ref_software.html#footnotes",
    "title": "Software management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, we call module the command and e.g. spider the subcommand. But sometimes the subcommands are also simply called commands.↩︎\nWhen your personal computer asks you to “authenticate” while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC, you don’t have such privileges.↩︎\nOther software upon which the software that you are trying to install depends.↩︎\nIt isn’t feasible to keep separate environments around for many different versions of a program, mostly because Conda environments contain a very large number of files, and OSC has file number quotas. This is why I have in many cases chosen the strategy of just updating the version within the same environment.↩︎\nUnless you first deactivate any active environments in your script.↩︎\nThat is, these settings will be saved somewhere in your OSC home directory, and you never have to set them again unless you need to make changes.↩︎\nThough note that as of September 2023, the singularity command does still work, and it will probably continue to work for a while.↩︎"
  },
  {
    "objectID": "03_qc-trim.html#introduction",
    "href": "03_qc-trim.html#introduction",
    "title": "Read QC and Trimming",
    "section": "Introduction",
    "text": "Introduction\nThe first series of steps our analysis workflow concerns the quality control (QC) & “pre-processing” of the sequence reads, which are stored in FASTQ files.\nThe QC part will leave the data untouched, whereas the pre-processing involves the removal of unwanted bits of sequence: in our case, amplicon primers. After the pre-processing step, we will still have FASTQ files, just with somewhat less content.\nSpecifically, we will go through the following steps:\n\nQC with FastQC\nSummarizing FastQC results with MultiQC\nRemoving primers with Cutadapt\n\n\n\n\n\n\n\n\nYou should have an active VS Code session. If not, follow these steps (Click to expand)\n\n\n\n\n\nStart a new VS Code session with an open terminal:\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2714\nThe starting directory: /fs/ess/PAS2714/&lt;user&gt; (replace &lt;user&gt; with your username)\nNumber of hours: 3\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal."
  },
  {
    "objectID": "03_qc-trim.html#running-fastqc-for-1-sample",
    "href": "03_qc-trim.html#running-fastqc-for-1-sample",
    "title": "Read QC and Trimming",
    "section": "1 Running FastQC for 1 sample",
    "text": "1 Running FastQC for 1 sample\n\n1.1 Intro to FastQC\nFastQC is a ubiquitous tools for quality control of FASTQ files. Running FastQC or a similar program is the first step in nearly any high-throughput sequencing project. FastQC is also a good introductory example of a tool with a command-line interface.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser with about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\nA FastQC per-base quality score graph for files with reasonably good quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding) and the x-axis shows the position along the read.\n\n\n\n\n\n1.2 Our FASTQ files\nLet’s take a look at our list of FASTQ files:\nls -lh data/fastq\ntotal 150M\n-rw-r-----+ 1 jelmer PAS0471 2.0M Mar  1 17:09 NW102AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 17:09 NW102AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 17:09 NW102C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.0M Mar  1 17:09 NW102C_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 1.9M Mar  1 17:09 NW103AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 17:09 NW103AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 17:09 NW103C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.1M Mar  1 17:09 NW103C_R2.fastq.gz\n# [...output truncated...]\nNote in the file listing above that:\n\nThere are two files per sample: _R1 (forward reads) and _R2 (reverse reads). This indicates that we have data from paired-end reads, as is customary with amplicon metabarcoding.\nThe files all have a .gz extension, indicating they have been compressed with the gzip utility.\n\n\n\n\n\n\n\nDon’t have the input files? Click here to copy them.\n\n\n\n\n\nRun the following two cp commands to copy the necessary data:\ncp -rv /fs/ess/PAS2714/share/data /fs/ess/PAS2714/users/$USER\ncp -rv /fs/ess/PAS2714/share/results /fs/ess/PAS2714/users/$USER\n\n\n\n\n\n\n1.3 Building our FastQC command\nTo run FastQC, we can use the command fastqc.\nIf you want to analyze one of your FASTQ files with default FastQC settings, a complete FastQC command to do so would simply be fastqc followed by the name of the file:\n# (Don't run this)\nfastqc data/fastq/NW102AB_R1.fastq.gz\nHowever, an annoying FastQC default behavior is that it writes its output files in the dir where the input files are — in general, it’s not great practice to directly mix your primary data and results like that!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen. Let’s try that:\nfastqc -h\nbash: fastqc: command not found...\nHowever, there is a wrinkle: while FastQC is installed at OSC1, we have to first “load it”. The way we will do this here is with a a so-called “Conda environment” that has FastQC installed along with the other programs we will need today.\nHere’s how we can load that Conda software environment — we first load OSC’s (mini)conda installation, and then we can load (“activate”) the Conda environment that I created for you:\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n\n\n\n\n\nConda and software management\n\n\n\nWe won’t have time to get into this now, but you want to learn more about Conda / software usage at supercomputers, see this reference page elsewhere on the website.\n\n\n\n\n Exercise: FastQC help and output dir\nPrint FastQC’s help info, and figure out which option you can use to specify a custom output directory.\n\n\nClick for the solution\n\nfastqc -h and fastqc --help will both work to show the help info.\nYou’ll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\n\n\nWith the added --outdir (or -o) option, let’s try to run the following FastQC command:\n# We'll have to first create the outdir ourselves, in this case\nmkdir -p results/fastqc\n\n# Now we run FastQC\nfastqc --outdir results/fastqc data/fastq/NW102AB_R1.fastq.gz\napplication/gzip\nStarted analysis of NW102AB_R1.fastq.gz\nApprox 5% complete for NW102AB_R1.fastq.gz\nApprox 10% complete for NW102AB_R1.fastq.gz\nApprox 15% complete for NW102AB_R1.fastq.gz\n[...truncated...]\nAnalysis complete for NW102AB_R1.fastq.gz\nSuccess!! 🎉\n\n\n\n1.4 FastQC output files\nLet’s take a look at the files in the output dir we specified:\nls -lh results/fastqc\ntotal 1.2M\n-rw-r--r-- 1 jelmer PAS0471 713K Feb  4 14:02 NW102AB_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 431K Feb  4 14:02 NW102AB_R1_fastqc.zip\n\nThere is a .zip file, which contains tables with FastQC’s data summaries\nThere is an .html (HTML) file, which contains plots — this is what we’ll look at next\n\n\n\n Exercise: Another FastQC run\nRun FastQC for the corresponding R2 FASTQ file. Would you use the same output dir?\n\n\nClick for the solution\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don’t need to worry about overwriting files, and it will be more convenient to have all results in a single dir.\nTo run FastQC for the R2 (reverse-read) file:\nfastqc --outdir results/fastqc data/fastq/NW102AB_R2.fastq.gz\nStarted analysis of NW102AB_R2.fastq.gz\nApprox 5% complete for NW102AB_R2.fastq.gz\nApprox 10% complete for NW102AB_R2.fastq.gz\nApprox 15% complete for NW102AB_R2.fastq.gz\n[...truncated...]\nAnalysis complete for NW102AB_R2.fastq.gz\nls -lh results/fastqc\n-rw-r--r-- 1 jelmer PAS0471 241K Mar 13 14:50 NW102AB_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 256K Mar 13 14:50 NW102AB_R1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 234K Mar 13 14:53 NW102AB_R2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 244K Mar 13 14:53 NW102AB_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs."
  },
  {
    "objectID": "03_qc-trim.html#interpreting-fastqc-output",
    "href": "03_qc-trim.html#interpreting-fastqc-output",
    "title": "Read QC and Trimming",
    "section": "2 Interpreting FastQC output",
    "text": "2 Interpreting FastQC output\n\n2.1 FastQC HTML modules\nWe’ll now go through a couple of the FastQC plots/modules with example plots2 with good/bad results for reference.\nFastQC has “pass” (checkmark in green), “warning” (exclamation mark in orange), and “fail” (cross in red) assessments for each module, as you can see below.\nThese assessments are handy, but a “warning”/“fail” is not necessarily the bad news it may appear to be:\n\nSome of these modules are perhaps overly strict.\nSome warnings and fails are easily remedied or simply not a very big deal.\nFastQC effectively assumes that your data is derived from whole-genome shotgun sequencing — some other types of data with different properties will therefore always trigger a couple of warnings and fails, but these are not meaningful. This is very much the case for metabarcoding data.\n\n\n\n\nAn example module results overview from a FastQC HTML file.The green checkmarks indicate Pass, the orange exclamation marks indicate Warning, and the red crosses indicate Fail.\n\n\n\n\nBasic statistics\nThis contains, among other things, the number of sequences (reads) and the read length range:\n\n\n\n\n\n\n\n\nPer base quality sequence quality\nThis figure visualize the mean per-base quality score (y-axis) along the length of the reads (x-axis). Note that:\n\nA decrease in sequence quality along the reads (from left to right) is normal.\nR2 (reverse) reads are usually of worse quality than R1 (forward) reads.\n\n\n\n\n\n\nGood / acceptable\n\n\n\n\n\n\nBad\n\n\n\n\nTo interpret the y-axis quality scores, note the color scaling in the graphs above, and see this table for details:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent\n\n\n\n\n\n\nPer sequence quality scores\nThis shows the same quality scores we saw above, but now simply as a density plot of per-read averages, with the quality score now along the x-axis, and the number of reads with that quality score along the y-axis:\n\n\n\n\n\nGood\n\n\n\n\n\n\nBad\n\n\n\n\n\n\nPer tile sequence quality\nThis graph shows whether specific tiles (areas on the flow cell, see box below) have lower quality bases in some parts of or across all of the read.\nPoor qualities in some but not other tiles may indicate some (transient) problems with the flow cell. For example, if a tile has poor qualities for a stretch of bases, this could indicate that there was a bubble. If such tile-based problems are severe, you can contact the sequencing facility as they may be able to re-run your data at no cost.\n\n\n\n\n\n\nWhat is a tile? (Click to expand)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood\n\n\n\n\n\n\nSome problems\n\n\n\n\n\n\nSequence length distribution\nWith Illumina sequencing, the vast majority of reads typically have nearly the same read length — in our case, 300 bp. This graph can help you check if that is indeed the case. The module will throw a warning as soon as not all sequences are of the same length (like below), but having reads with a slightly shorter read length is normal and does not matter.\n\n\n\n\n\nGoodNearly all reads have the same read length. FastQC still threw a warning.\n\n\n\n\n\n\nSomewhat badThere are unusually many reads that are shorter than the aimed-for, 150 bp read length. Though the large majority of reads are still 150 bp.\n\n\n\n\n\n\nOther FastQC modules\nAnother module to check the overall quality of your reads is Per base N content. This line graph should typically not visibly rise above 0%.\nThe remaining modules are not that useful for metabarcoding data:\n\nAdapter content3\nSequence duplication levels4\nOverrepresented sequences5\nPer sequence GC content6\n\n\n\n\n\n2.2 Checking your FastQC results\nFirst, you’ll unfortunately have to download FastQC’s output HTML files to your computer:\n\nFind the FastQC HTML files in the file explorer in the VS Code side bar.\nRight-click on one of them, click Download... and follow the prompt to download the file somewhere to your computer (doesn’t matter where).\nRepeat this for the second file.\nThen, open your computer’s file browser, find the downloaded files, and double-click on one. It should be opened in your default web browser.\n\n\n Exercise: Interpreting your FastQC results\n\nOpen the HTML file for the R1 FASTQ file and go through the modules we discussed above. Can you make sense of it? Does the data look good to you, overall?\nNow open the HTML file for the R2 FASTQ file and take a look just at the quality scores. Does it look any worse than the R1?"
  },
  {
    "objectID": "03_qc-trim.html#running-fastqc-for-all-samples",
    "href": "03_qc-trim.html#running-fastqc-for-all-samples",
    "title": "Read QC and Trimming",
    "section": "3 Running FastQC for all samples",
    "text": "3 Running FastQC for all samples\nIf we want to run FastQC for all samples, it will be much better to write a shell script and submit that as a so-called Slurm batch job, rather than running FastQC “interactively” like we did for the first sample.\nThis is especially true for a complete data set, which would have much larger FASTQ files and possibly more samples.\n\n3.1 Building our script\nFirst, we will put our earlier FastQC code inside a script.\n\nOpen a new file in VS Code: click , then File, then New File.\nSave the file (e.g. press Ctrl/⌘+S) in your scripts directory as fastqc.sh.\nPaste the following code (same as we used above) in the script:\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Create the output dir\nmkdir -p results/fastqc\n\n# Run FastQC\nfastqc --outdir results/fastqc data/fastq/NW102AB_R1.fastq.gz\n\nHowever, we will need to modify our call to fastqc — we will loop over all FASTQ files as follows:\n# Run FastQC (replacement for fastqc line above)\nfor fastq_file in data/fastq/*fastq.gz; do\n    fastqc --outdir results/fastqc \"$fastq_file\"\ndone\n\nWe are looping over all FASTQ files with the globbing pattern data/fastq/*fastq.gz. The loop will run as many times as we have FASTQ files.\nIn every iteration of the loop, the \"$fastq_file\" variable will contain 1 FASTQ file name, and we will run fastqc for that file7.\n\n\nWe will be submitting this script as a batch job to the Slurm compute job scheduler. To do so, we should also add some lines at the top of the script:\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-fastqc.out\n\nThe first line #!/bin/bash merely indicates that this is a shell script8 rather than, say, an R or Python script.\nThe lines starting with #SBATCH tell Slurm some details about our compute job request (much like we did when we filled out the form to start a VS Code session):\n\nWe always need to specify an “account”, i.e. OSC project, that should be billed.\nThe only other option (of many possible!) we will use here is to specify the output file: this is where any output will go that would otherwise be printed to screen, such as the FastQC progress output we saw above.\n\n\n\nWe will also add the following line to change some shell script settings, which will cause the script to stop running if any errors occur:\n# Strict bash settings\nset -euo pipefail\n\nAll in all, our script should read:\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-fastqc.out\n\n# Strict bash settings\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Create the output dir\nmkdir -p results/fastqc\n\n# Run FastQC for all FASTQ files\nfor fastq_file in data/fastq/*fastq.gz; do\n    fastqc --outdir results/fastqc \"$fastq_file\"\ndone\n\n# Report\necho \"Done with script fastqc.sh\"\ndate\n\n\n\n3.2 Submitting the script\nSubmit the script to Slurm (“submit it to the queue”) with the sbatch command:\nsbatch scripts/fastqc.sh\nSubmitted batch job 27047185\nAfter some seconds (sometimes up to a minute or so9), the Slurm job should start and create the output file that we specified at the top of the script: slurm-fastqc.out.\n\n\n\n3.3 Checking the output\nEarlier, FastQC logging output (“10% complete”, etc.) was printed to screen, but because this job now runs remotely on another compute node, such output will end up in the “Slurm log file” whose name we specified in the script. Let’s take a look:\nless slurm-fastqc.out\napplication/gzip\nStarted analysis of NW102AB_R1.fastq.gz\nApprox 5% complete for NW102AB_R1.fastq.gz\nApprox 15% complete for NW102AB_R1.fastq.gz\nApprox 20% complete for NW102AB_R1.fastq.gz\nApprox 30% complete for NW102AB_R1.fastq.gz\nApprox 35% complete for NW102AB_R1.fastq.gz\nApprox 45% complete for NW102AB_R1.fastq.gz\nApprox 50% complete for NW102AB_R1.fastq.gz\nApprox 60% complete for NW102AB_R1.fastq.gz\nApprox 70% complete for NW102AB_R1.fastq.gz\nApprox 75% complete for NW102AB_R1.fastq.gz\nApprox 85% complete for NW102AB_R1.fastq.gz\nApprox 90% complete for NW102AB_R1.fastq.gz\nAnalysis complete for NW102AB_R1.fastq.gz\napplication/gzip\nStarted analysis of NW102AB_R2.fastq.gz\nApprox 5% complete for NW102AB_R2.fastq.gz\nApprox 15% complete for NW102AB_R2.fastq.gz\nApprox 20% complete for NW102AB_R2.fastq.gz\n#[...output truncated...]\nThat looks good, in the output I printed above we can see that FastQC ran to completion for one FASTQ file and then started a second — and this will go on and on, for all of our 64 FASTQ files.\nYou will know that the job has successfully finished when the last few lines of the Slurm log file read “Done with script fastqc.sh” and print the date and time (as per the last lines of our script!):\ntail slurm-fastqc.out\nApprox 75% complete for W404BC_R2.fastq.gz\nApprox 85% complete for W404BC_R2.fastq.gz\nApprox 90% complete for W404BC_R2.fastq.gz\nApprox 95% complete for W404BC_R2.fastq.gz\nAnalysis complete for W404BC_R2.fastq.gz\nDone with script fastqc.sh\nWed Mar  6 13:34:10 EST 2024\n\nOf course, we should also check the main output files — the HTMLs and zip files:\nls results/fastqc\nNW102AB_R1_fastqc.html  NW103C_R1_fastqc.zip    NW203A_R2_fastqc.html   NW304BC_R2_fastqc.zip   NW403BC_R1_fastqc.html  W101AB_R1_fastqc.zip   W103C_R2_fastqc.html   W205A_R2_fastqc.zip    W304AB_R1_fastqc.html  W403C_R1_fastqc.zip\nNW102AB_R1_fastqc.zip   NW103C_R2_fastqc.html   NW203A_R2_fastqc.zip    NW305AB_R1_fastqc.html  NW403BC_R1_fastqc.zip   W101AB_R2_fastqc.html  W103C_R2_fastqc.zip    W205BC_R1_fastqc.html  W304AB_R1_fastqc.zip   W403C_R2_fastqc.html\nNW102AB_R2_fastqc.html  NW103C_R2_fastqc.zip    NW203BC_R1_fastqc.html  NW305AB_R1_fastqc.zip   NW403BC_R2_fastqc.html  W101AB_R2_fastqc.zip   W204A_R1_fastqc.html   W205BC_R1_fastqc.zip   W304AB_R2_fastqc.html  W403C_R2_fastqc.zip\nNW102AB_R2_fastqc.zip   NW201AB_R1_fastqc.html  NW203BC_R1_fastqc.zip   NW305AB_R2_fastqc.html  NW403BC_R2_fastqc.zip   W101C_R1_fastqc.html   W204A_R1_fastqc.zip    W205BC_R2_fastqc.html  W304AB_R2_fastqc.zip   W404A_R1_fastqc.html\nNW102C_R1_fastqc.html   NW201AB_R1_fastqc.zip   NW203BC_R2_fastqc.html  NW305AB_R2_fastqc.zip   NW404A_R1_fastqc.html   W101C_R1_fastqc.zip    W204A_R2_fastqc.html   W205BC_R2_fastqc.zip   W304C_R1_fastqc.html   W404A_R1_fastqc.zip\nNW102C_R1_fastqc.zip    NW201AB_R2_fastqc.html  NW203BC_R2_fastqc.zip   NW305C_R1_fastqc.html   NW404A_R1_fastqc.zip    W101C_R2_fastqc.html   W204A_R2_fastqc.zip    W303AB_R1_fastqc.html  W304C_R1_fastqc.zip    W404A_R2_fastqc.html\nNW102C_R2_fastqc.html   NW201AB_R2_fastqc.zip   NW304A_R1_fastqc.html   NW305C_R1_fastqc.zip    NW404A_R2_fastqc.html   W101C_R2_fastqc.zip    W204BC_R1_fastqc.html  W303AB_R1_fastqc.zip   W304C_R2_fastqc.html   W404A_R2_fastqc.zip\nNW102C_R2_fastqc.zip    NW201C_R1_fastqc.html   NW304A_R1_fastqc.zip    NW305C_R2_fastqc.html   NW404A_R2_fastqc.zip    W103AB_R1_fastqc.html  W204BC_R1_fastqc.zip   W303AB_R2_fastqc.html  W304C_R2_fastqc.zip    W404BC_R1_fastqc.html\nNW103AB_R1_fastqc.html  NW201C_R1_fastqc.zip    NW304A_R2_fastqc.html   NW305C_R2_fastqc.zip    NW404BC_R1_fastqc.html  W103AB_R1_fastqc.zip   W204BC_R2_fastqc.html  W303AB_R2_fastqc.zip   W403AB_R1_fastqc.html  W404BC_R1_fastqc.zip\nNW103AB_R1_fastqc.zip   NW201C_R2_fastqc.html   NW304A_R2_fastqc.zip    NW403A_R1_fastqc.html   NW404BC_R1_fastqc.zip   W103AB_R2_fastqc.html  W204BC_R2_fastqc.zip   W303C_R1_fastqc.html   W403AB_R1_fastqc.zip   W404BC_R2_fastqc.html\nNW103AB_R2_fastqc.html  NW201C_R2_fastqc.zip    NW304BC_R1_fastqc.html  NW403A_R1_fastqc.zip    NW404BC_R2_fastqc.html  W103AB_R2_fastqc.zip   W205A_R1_fastqc.html   W303C_R1_fastqc.zip    W403AB_R2_fastqc.html  W404BC_R2_fastqc.zip\nNW103AB_R2_fastqc.zip   NW203A_R1_fastqc.html   NW304BC_R1_fastqc.zip   NW403A_R2_fastqc.html   NW404BC_R2_fastqc.zip   W103C_R1_fastqc.html   W205A_R1_fastqc.zip    W303C_R2_fastqc.html   W403AB_R2_fastqc.zip\nNW103C_R1_fastqc.html   NW203A_R1_fastqc.zip    NW304BC_R2_fastqc.html  NW403A_R2_fastqc.zip    W101AB_R1_fastqc.html   W103C_R1_fastqc.zip    W205A_R2_fastqc.html   W303C_R2_fastqc.zip    W403C_R1_fastqc.html\nThat’s a lot of files! Do we need to check all of them? Luckily not, thanks to MultiQC."
  },
  {
    "objectID": "03_qc-trim.html#summarizing-qc-results-with-multiqc",
    "href": "03_qc-trim.html#summarizing-qc-results-with-multiqc",
    "title": "Read QC and Trimming",
    "section": "4 Summarizing QC results with MultiQC",
    "text": "4 Summarizing QC results with MultiQC\nHere are some challenges you may run into after running FastQC:\n\nWhen you have many FASTQ files, you’ll generate a lot of FastQC HTML files to sort through (as we did above).\nEven if you do diligently go through each file, it’s not that easy to compare the results across samples in detail, since they are not drawn in the same graphs.\n\nMultiQC addresses these problems by aggregating FastQC results from many files, and summarizing them into a single HTML file with (still) one graph per FastQC module.\n\n\n\n\n\n\nNot just for FastQC results! MultiQC can recognize and process the output of dozens of bioinformatics tools.\n\n\n\n\n\n\nMultiQC’s graphs are also interactive, but here is a static example of a graph showing the mean base quality scores along the read for many FASTQ files:\n\n\n\n\n\n\n\n\n Above, what could the two “groups of lines”, which diverge towards the right-hand side, represent?\n\nThese are the files with forward (top lines, better quality) and reverse (bottom lines, worse quality) reads.\n\n\nMultiQC will also create a graph comparing the number of reads across files, which can be quite useful:\n\n\n\n\n\n\n\n4.1 Running MultiQC\nWe will only need to run MultiQC once (because it will aggregate all FastQC results at once), and that will only take a few seconds — therefore, we can run the command interactively without using a script.\nLet’s start by running MultiQC (command multiqc) with the --help option:\nmultiqc --help\n# (Only the top part of the output is shown in the screenshot below)\n\n\n\n\n\nAs the first couple of help lines in the paler gray color explain, MultiQC will search the [ANALYSIS DIRECTORY], a dir that we pass to it as an argument at the end of the command line. That is, if we tell MultiQC about the results/fastqc directory like so, it should find and then aggregate all the FastQC results in there:\n# (Don't run this - we'll complete the command in a second)\nmultiqc results/fastqc\nThe default output directory of MultiQC is the current working directory, so just like with FastQC, we do want to use the option for the output dir — this is our final command and you can go ahead and execute it:\n# Run MultiQC to summarize the FastQC results\nmultiqc --outdir results/multiqc results/fastqc\n\n\n\n\n\n\n\n\n4.2 MultiQC output\nOnce its done, you should have the following files in the output dir:\nls -lh results/multiqc\ntotal 1.7M\ndrwxr-xr-x 2 jelmer PAS2250 4.0K Mar  13 14:57 multiqc_data\n-rw-r--r-- 1 jelmer PAS2250 1.7M Mar  13 14:57 multiqc_report.html\nGo ahead and find the HTML file in VS Code’s file browser, right-click on it and then download it to your computer, and click on the file in your own computer to open it in your browser (i.e., just like we did with the FastQC output).\n\n Exercise: Explore the MultiQC results\nCheck for example whether patterns are consistent across samples, or if there are any outliers."
  },
  {
    "objectID": "03_qc-trim.html#cutadapt",
    "href": "03_qc-trim.html#cutadapt",
    "title": "Read QC and Trimming",
    "section": "5 Cutadapt",
    "text": "5 Cutadapt\nWhen you prepare samples for amplicon metabarcoding, you amplify a specific region with primers, and these primers will be included in the sequences that you receive. Before we go any further, we need to remove these primer sequences, which we can do with the program Cutadapt.\nWe will write a script with a loop to run Cutadapt for all samples and submit it as a batch job like we did with FastQC.\n Open a new text file and save it as scripts/cutadapt.sh.\n\n5.1 Primer sequences\nWhen we run Cutadapt, we need to tell it about our primer sequences as well as their reverse complements. We’ll start by storing the primer sequences in variables:\n# Primer sequences\nprimer_f=GTGTGYCAGCMGCCGCGGTAA\nprimer_r=GGACTACNVGGGTWTCTAAT\n\n\n\n\n\n\nAmbiguity codes\n\n\n\nNote that the Y, M, N, V and W in the primer sequences are so-called “ambiguity codes” that represent multiple possible bases. For example, a Y represents a C or a T, and an N represents any of the 4 bases.\n\n\nThere are many ways of getting the reverse complement of a sequence, including manually building it up, but here we’ll use a trick with the tr command to change each base into its complement, followed by the rev command to get the reverse complement — for example, for the forward primer:\necho \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev\nTTACCGCGGCKGCTGRCACAC\nBelow, we’ll get the reverse complement for both primers, and will store those in a variable as well using the construct variable=$(command)10:\n# Get the reverse-complements of the primers\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\n\n# Check the sequences\necho \"$primer_f_rc\"\necho \"$primer_r_rc\"\nTTACCGCGGCKGCTGRCACAC\nATTAGAWACCCBNGTAGTCC\n\n\n\n\n\n\nMore about the above tr | rev command (Click to expand)\n\n\n\n\n\n\ntr A T changes every A to a T\ntr ATC TAG changes every A to a T (first character in each of the two sequences), every T to an a (second character in each of the two sequences), and every C to a G (third character in each of the two sequences).\nTherefore, in the full tr command above, we list all possible bases and ambiguity codes, and then change them to their complement.\nThe rev command simply reverses a sequence, so that we end up with the reverse complement.\n\necho ACCT | rev\nTCCA\n\n\n\n\n\n\n\n5.2 Building the Cutadapt command\nFirst, here is how we can tell Cutadapt about the primer sequences:\n\nWith the -a and -A option we specify the primer sequences in the forward and reverse reads, respectively.\nThe forward reads should contain the forward primer at the beginning of the read. Because reads are sometimes longer than the amplicon length, the reverse primer may be present at the end of the read, but as its reverse complement. We specify this using -a \"$primer_f\"...\"$primer_r_rc\".\nSimilarly, the reverse reads should contain the reverse primer at the beginning of the read, and may contain the reverse complement of the forward primer at the end of the read, which we specify using -A \"$primer_r\"...\"$primer_f_rc\".\n\nAll in all, our primer specification looks like this:\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\"\n\n\n\n\n\n\nSpreading commands across multiple lines with \\\n\n\n\nAbove, I spread the command across multiple lines, which makes it a little easier to read. You can run the command exactly like that: the backslashes (\\) at the end of all except the last line tell the shell that our command will continue on the next line.\n\n\nWe will also:\n\nTell Cutadapt to only keep sequences that contain the primer11, with the --trimmed-only option.\nInstruct Cutadapt to use 8 “cores” with --cores 8, which can speed up the run by up to 8-fold. For our small FASTQ files, this isn’t really necessary, but for a larger dataset, that can save quite some time.\n\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\" \\\n    --trimmed-only \\\n    --cores 8\nFinally, let’s also add the output files (--output for R1 and --paired-output for R2) and the input files (as positional arguments at the end of the command) for a single example sample. With that, we have a final example command of running Cutadapt for a single sample:\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\" \\\n    --trimmed-only \\\n    --cores 8 \\\n    --output results/cutadapt/NW102AB_R1.fastq.gz \\\n    --paired-output results/cutadapt/NW102AB_R1.fastq.gz \\\n    data/fastq/NW102AB_R1.fastq.gz \\\n    data/fastq/NW102AB_R2.fastq.gz\n\n\n\n5.3 Our Cutadapt loop\nIn our script, we will run Cutadapt inside a loop, similar to how we ran FastQC. However, this case is a bit more complicated, because we need to run Cutadapt for one sample and therefore two FASTQ files at a time, rather than for one FASTQ file at a time.\nWe will do that by looping over the R1 (forward read) files only, and inside the loop, inferring the name of the R2 file:\n# Loop over the R1 files\nfor R1_in in data/fastq/*R1.fastq.gz; do\n    # Get the R2 file name with \"parameter expansion\"\n    # This does a search-and-replace: replace \"_R1\" with \"_R2\"\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    \n    # Define the output files\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Run Cutadapt\n    cutadapt \\\n            -a \"$primer_f\"...\"$primer_r_rc\" \\\n            -A \"$primer_r\"...\"$primer_f_rc\" \\\n            --trimmed-only \\\n            --cores 8 \\\n            --output \"$R1_out\" \\\n            --paired-output \"$R2_out\" \\\n            \"$R1_in\" \"$R2_in\"\ndone\n\n\n\n5.4 The final script\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-cutadapt.out\n#SBATCH --cpus-per-task=8\n\n# Strict bash settings\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Primer sequences\nprimer_f=GTGTGYCAGCMGCCGCGGTAA\nprimer_r=GGACTACNVGGGTWTCTAAT\n\n# Get the reverse-complements of the primers\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\n\n# Create the output dir\noutdir=results/cutadapt\nmkdir -p \"$outdir\"\n\n# Loop over the R1 files\nfor R1_in in data/fastq/*R1.fastq.gz; do\n    # Get the R2 file name\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    \n    # Define the output files\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Run Cutadapt\n    cutadapt \\\n            -a \"$primer_f\"...\"$primer_r_rc\" \\\n            -A \"$primer_r\"...\"$primer_f_rc\" \\\n            --trimmed-only \\\n            --cores 8 \\\n            --output \"$R1_out\" \\\n            --paired-output \"$R2_out\" \\\n            \"$R1_in\" \"$R2_in\"\ndone\n\n# Report\necho \"Done with script cutadapt.sh\"\ndate\n\n\n\n5.5 Check the output\nOnce we see the “Done with script” line when we use tail on the Slurm log file, we know the job has finished:\ntail slurm-cutadapt.sh\n22      4       0.0     2       0 3 1\n24      1       0.0     2       0 1\n25      1       0.0     2       0 0 1\n34      1       0.0     2       0 0 1\n38      2       0.0     2       0 2\n42      1       0.0     2       0 0 1\n44      1       0.0     2       0 1\n47      1       0.0     2       0 1\nDone with script cutadapt.sh\nWed Mar  6 14:46:56 EST 2024\nLet’s use less to take a closer look at the logging output:\nless slurm-cutadapt.sh\nInput files: data/fastq/NW102AB_R1.fastq.gz data/fastq/NW102AB_R2.fastq.gz\nThis is cutadapt 4.6 with Python 3.10.13\nCommand line parameters: -a GTGTGYCAGCMGCCGCGGTAA...ATTAGAWACCCBNGTAGTCC -A GGACTACNVGGGTWTCTAAT...TTACCGCGGCKGCTGRCACAC --trimmed-only --cores 8 --output results/cutadapt/NW102AB_R1.fastq.gz --paired-output results/cutadapt/NW102AB_R2.fast\nq.gz data/fastq/NW102AB_R1.fastq.gz data/fastq/NW102AB_R2.fastq.gz\nProcessing paired-end reads on 8 cores ...\nFinished in 0.875 s (68.148 µs/read; 0.88 M reads/minute).\n\n=== Summary ===\n\nTotal read pairs processed:             12,844\n  Read 1 with adapter:                  12,798 (99.6%)\n  Read 2 with adapter:                  12,541 (97.6%)\n\n== Read fate breakdown ==\nPairs discarded as untrimmed:              337 (2.6%)\nPairs written (passing filters):        12,507 (97.4%)\n# [...output truncated...]\nWe can see that Cutadapt reports the numbers and percentages of reads that contained what it calls the “adapter”: in our case, that’s the primer. In the example above, and that is typical, the percentages are in the upper 90s.\n\n\n\n\n\n\nIf you see much lower percentages here, then something is wrong, e.g. with the primer sequences you provided or the Cutadapt syntax you used.\n\n\n\n\n\n\nWe will use the grep command to print all lines that contain the information on numbers and percentages of reads with the primer sequences:\ngrep \"with adapter:\" slurm-cutadapt.sh\n  Read 1 with adapter:                  12,798 (99.6%)\n  Read 2 with adapter:                  12,541 (97.6%)\n  Read 1 with adapter:                  14,499 (99.7%)\n  Read 2 with adapter:                  14,211 (97.7%)\n  Read 1 with adapter:                  12,174 (99.7%)\n  Read 2 with adapter:                  11,835 (97.0%)\n  Read 1 with adapter:                  15,054 (99.7%)\n  Read 2 with adapter:                  14,737 (97.6%)\n# [...output truncated...]\nYou should always take a careful look at this output, to check if there are no samples with much lower percentages: it looks like there are no such samples in this case, fortunately.\n\n\n\n\n\n\nWant to quickly see the lowest % of reads with adapter? (Click to expand)\n\n\n\n\n\nUse this command to extract and sort the percentages:\ngrep \"with adapter:\" slurm-cutadapt.out | cut -d\"(\" -f2 | sort -n | head\n97.0%)\n97.3%)\n97.5%)\n97.5%)\n97.5%)\n97.5%)\n97.6%)\n\n\n\n\n\n\n\n\n\nNeed to identify the samples with a specific percentage? (Click to expand)\n\n\n\n\n\nUse this command to print the 7 lines preceding each match, so you can see the file name. (Of course, you could also scroll through the entire file.)\ngrep -B7 \"with adapter:\" results/cutadapt/logs/slurm-cutadapt.out\nCommand line parameters: -a GTGTGYCAGCMGCCGCGGTAA...ATTAGAWACCCBNGTAGTCC -A GGACTACNVGGGTWTCTAAT...TTACCGCGGCKGCTGRCACAC --trimmed-only --cores 8 --output results/cutadapt/NW102AB_R1.fastq.gz --paired-output results/cutadapt/NW102AB_R2.fastq.gz data/fastq/NW102AB_R1.fastq.gz data/fastq/NW102AB_R2.fastq.gz\nProcessing paired-end reads on 8 cores ...\nFinished in 0.875 s (68.148 µs/read; 0.88 M reads/minute).\n\n=== Summary ===\n\nTotal read pairs processed:             12,844\n  Read 1 with adapter:                  12,798 (99.6%)\n  Read 2 with adapter:                  12,541 (97.6%)\n--\nCommand line parameters: -a GTGTGYCAGCMGCCGCGGTAA...ATTAGAWACCCBNGTAGTCC -A GGACTACNVGGGTWTCTAAT...TTACCGCGGCKGCTGRCACAC --trimmed-only --cores 8 --output results/cutadapt/NW102C_R1.fastq.gz --paired-output results/cutadapt/NW102C_R2.fastq.gz data/fastq/NW102C_R1.fastq.gz data/fastq/NW102C_R2.fastq.gz\nProcessing paired-end reads on 8 cores ...\nFinished in 0.801 s (55.030 µs/read; 1.09 M reads/minute).\n\n=== Summary ===\n\nTotal read pairs processed:             14,549\n  Read 1 with adapter:                  14,499 (99.7%)\n  Read 2 with adapter:                  14,211 (97.7%)\n--\n# [...output truncated...]\n\n\n\n\nFinally, let’s check the output dir, which contains the trimmed FASTQ files we’ll use in the next step of the workflow:\nls results/cutadapt\nNW102AB_R1.fastq.gz  NW103C_R1.fastq.gz   NW203A_R1.fastq.gz   NW304BC_R1.fastq.gz  NW403A_R1.fastq.gz   NW404BC_R1.fastq.gz  W103AB_R1.fastq.gz  W204BC_R1.fastq.gz  W303AB_R1.fastq.gz  W304C_R1.fastq.gz   W404A_R1.fastq.gz\nNW102AB_R2.fastq.gz  NW103C_R2.fastq.gz   NW203A_R2.fastq.gz   NW304BC_R2.fastq.gz  NW403A_R2.fastq.gz   NW404BC_R2.fastq.gz  W103AB_R2.fastq.gz  W204BC_R2.fastq.gz  W303AB_R2.fastq.gz  W304C_R2.fastq.gz   W404A_R2.fastq.gz\nNW102C_R1.fastq.gz   NW201AB_R1.fastq.gz  NW203BC_R1.fastq.gz  NW305AB_R1.fastq.gz  NW403BC_R1.fastq.gz  W101AB_R1.fastq.gz   W103C_R1.fastq.gz   W205A_R1.fastq.gz   W303C_R1.fastq.gz   W403AB_R1.fastq.gz  W404BC_R1.fastq.gz\nNW102C_R2.fastq.gz   NW201AB_R2.fastq.gz  NW203BC_R2.fastq.gz  NW305AB_R2.fastq.gz  NW403BC_R2.fastq.gz  W101AB_R2.fastq.gz   W103C_R2.fastq.gz   W205A_R2.fastq.gz   W303C_R2.fastq.gz   W403AB_R2.fastq.gz  W404BC_R2.fastq.gz\nNW103AB_R1.fastq.gz  NW201C_R1.fastq.gz   NW304A_R1.fastq.gz   NW305C_R1.fastq.gz   NW404A_R1.fastq.gz   W101C_R1.fastq.gz    W204A_R1.fastq.gz   W205BC_R1.fastq.gz  W304AB_R1.fastq.gz  W403C_R1.fastq.gz\nNW103AB_R2.fastq.gz  NW201C_R2.fastq.gz   NW304A_R2.fastq.gz   NW305C_R2.fastq.gz   NW404A_R2.fastq.gz   W101C_R2.fastq.gz    W204A_R2.fastq.gz   W205BC_R2.fastq.gz  W304AB_R2.fastq.gz  W403C_R2.fastq.gz\nls -lh results/cutadapt\n-rw-rw----+ 1 jelmer PAS0471 1.9M Mar  6 14:46 NW102AB_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.4M Mar  6 14:46 NW102AB_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.2M Mar  6 14:46 NW102C_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.8M Mar  6 14:46 NW102C_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 1.8M Mar  6 14:46 NW103AB_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.4M Mar  6 14:46 NW103AB_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.2M Mar  6 14:46 NW103C_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.9M Mar  6 14:46 NW103C_R2.fastq.gz\n# [...output truncated...]"
  },
  {
    "objectID": "03_qc-trim.html#bonus-content",
    "href": "03_qc-trim.html#bonus-content",
    "title": "Read QC and Trimming",
    "section": "6 Bonus content",
    "text": "6 Bonus content\n\n6.1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines. Like most genomic data files, these are plain text files. Each sequence that is read by the sequencer (i.e., each “read”) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\nThe “Q” in FASTQ stands for “quality”, to contrast this format with FASTA, a more basic and generic sequence data format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nFASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call. Specifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretations for Illumina data:\n\n\n\n\n\n\n\n\n\nPhred quality score\nError probability\nRough interpretation\nASCII character\n\n\n\n\n10\n1 in 10\nterrible\n+\n\n\n20\n1 in 100\nbad\n5\n\n\n30\n1 in 1,000\ngood\n?\n\n\n40\n1 in 10,000\nexcellent\n?\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character” (last column in the table). This allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read. (For your reference, here is a complete lookup table — look at the top table, “BASE=33”).\n\n\n\n\n\n\n6.2 Viewing FASTQ files\nNext, we’ll take a peak inside one of these FASTQ files.\nThe head command prints the first lines of a file. Let’s use it try to and print 8 lines, which should show us two reads:\nhead -n 8 data/fastq/NW102AB_R1.fastq.gz\n�\nԽے�8�E��_1f�\"�QD�J��D�fs{����Yk����d��*��\n|��x���l޴�j�N������?������ٔ�bUs�Ng�Ǭ���i;_��������������|&lt;�v����3��������|���ۧ��3ĐHyƕ�bIΟD�%����Sr#~��7��ν��1y�Ai,4\nw\\]\"b�#Q����8��+[e�3d�4H���̒�l�9LVMX��U*�M����_?���\\[\"��7�s\\&lt;_���:�$���N��v�}^����sw�|�n;&lt;�&lt;�oP����\ni��k��q�ְ(G�ϫ��L�^��=��&lt;���K��j�_/�[ۭV�ns:��U��G�z�ݎ�j����&��~�F��٤ZN�'��r2z}�f\\#��:�9$�����H�݂�\"�@M����H�C�\n�0�pp���1�O��I�H�P됄�.Ȣe��Q�&gt;���\n�'�;@D8���#��St�7k�g��|�A䉻���_���d�_c������a\\�|�_�mn�]�9N������l�٢ZN�c�9u�����n��n�`��\n\"gͺ�\n    ���H�?2@�FC�S$n���Ԓh�       nԙj��望��f      �?N@�CzUlT�&�h�Pt!�r|��9~)���e�A�77�h{��~��     ��\n# [...output truncated...]\n\n\nOuch! 😳 What went wrong here? (Click for the solution)\n\nWhat happened here is that we are directly seeing the contents of the compressed file, which is simply not human-readable.\n\n\n\n\n\n\n\n\nNo need to decompress\n\n\n\nTo get around the problem we just encountered with head, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones. Fortunately, we don’t need to decompress them:\n\nAlmost any bioinformatics tool will accept compressed FASTQ files.\nWe can still view these files in compressed form, as shown below.\n\n\n\nInstead, we’ll use the less command, which will automatically display gzip-compressed files in human-readable form:\nless -S data/fastq/NW102AB_R1.fastq.gz\n@M02815:77:000000000-KPK85:1:2101:3678:10660 1:N:0:CCTAAGAC+TTCTAGCT\nCGAGCAATCCACTCGAGTGCCAGCAGCCGCAGTAATACGGAGGGTGCGAGCGTTGTCCGGAATCACTGGGCGTAAAGGGCGCGTAGGCGGCGCGGATAGTCGGCGGTGAAAGCCCGGAGCTCAACTCCGGGTCGGCCGTCGATACTTCCGGGCTTGAGCACTGTAGAGGCAGATGGAATTCCGGGTGTAGCGGTGGAATGCGTAGAGATCCGGAAGAACACCGGTGGCGAAGGCGGTCTGCTGGGCAGTTGCTGACGCTGATGCGCGACAGCGTGGGGAGCAAACAGGATTAGATACC\n+\nCCCCCGGGGGGGGGGGGGGFGGGGGGGGGG+CFGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGEGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGFGGFGFFFFEBFFGFFFDGFGFGBFGFGFGFFFF6?FFFGBF?FBFFF\n@M02815:77:000000000-KPK85:1:2108:2535:14400 1:N:0:CCTAAGAC+TTCTAGCT\nCGAGCAATCCACTCGAGTGTCAGCCGCCGCGGTAATACAGAGGTCCCGAGCGTTGTTCGGATTCATTGGGCGTAAAGGGTGCGTAGGCGGCGGGGAAAGTCTGATGTGAAATCCTGGGGCTCAACCCTGGAACTGCATTGGATACTTCCTTGCTAGAGTACTGGAGAGGAAACTGGAATTTACGGTGTAGCAGTGAAATGCGTAGAGATCGTAAGGAAGACCAGTGGCGAAGGCGAGTTTCTGGACAGTTACTGACGCTGAGGCACGAAGGCCAGGGGAGCAAACGGGATTAGATACC\n+\nCCCCCCGFGFGGGC-FFFGFGFFGGDFFGGGGGECGEGGAEGGGGGGGFGGDGG7CFFGGDCCFGGFCF8FGGGGGGCEGDGGGGGCGGGGGGDEGGGGBFGGDFGGGDG&lt;DFGGGGCEGGGD:FFGGGGFFGFGGFFFFGGGFGGCFGGFGGGGG9CGCGGGG7FGGC:FFGGGGGFGG&lt;?FCGGGGGGGGGGG9CG&lt;ACC?EG5CFGGGGF8CCCC:C@FGCFGGGGGC58=EEG8??77:9@:&lt;3A&gt;7AGFGGGGC?DFC?5&lt;5&gt;&gt;BGGGFGGGGG&gt;4?C42::3:DG=&gt;&lt;&lt;*)*\n\n\n\n\n\n\nless -S suppresses line-wrapping: lines in the file will not be “wrapped” across multiple lines\n\n\n\n\n\n\n\n\n\n\nExercise: Explore the file with less\nless doesn’t print stuff to screen but instead opens it in a “pager”. After running the command above, you should be viewing the file inside the less pager.\nYou can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d down half a page).\nNotice you won’t get your shell prompt back until you press q to quit less."
  },
  {
    "objectID": "03_qc-trim.html#footnotes",
    "href": "03_qc-trim.html#footnotes",
    "title": "Read QC and Trimming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list↩︎\n Attribution: Some of the FastQC example plots were taken from here.↩︎\nChecks for adapters at the ends of reads. Since we have to remove primers anyway, any adapters past the primers will be automatically removed.↩︎\n Will throw a Fail but this is not meaningful here: metabarcoding data has many duplicate sequences by design.↩︎\nAs with the previous module, this will throw a Fail but this is not meaningful here: metabarcoding data has many duplicate sequences by design.↩︎\nUseful in a whole-genome sequencing context or to detect contamination.↩︎\nTherefore, our FastQC analysis will run sequentially (1-by-1) for each file, not in parallel.↩︎\n Using the shell language Bash, specifically↩︎\n And very large jobs can sometimes take hours to start, but our jobs are small so that should not happen.↩︎\nThis is called “command substitution”.↩︎\n This is not the default: Cutadapt is even more commonly used to remove adapters, and then this doesn’t apply↩︎"
  },
  {
    "objectID": "05_dada.html#setting-up",
    "href": "05_dada.html#setting-up",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "1 Setting up",
    "text": "1 Setting up\n\nStart an RStudio Server job at OSC\n\nLog in to OSC at https://ondemand.osc.edu.\nClick on Interactive Apps (top bar) and then RStudio Server (all the way at the bottom).\nFill out the form as follows:\n\nCluster: Pitzer\nR version: 4.3.0\nProject: PAS2714\nNumber of hours: 4\nNode type: any\nNumber of cores: 4\n\nClick Launch and once your job has started, click Connect to RStudio Server.\n\n\n\n\nOpen your RStudio Project\n\nYour RStudio Project in /fs/ess/PAS2714 may have automatically opened. You can see whether a Project is open, and if so, which one, in the top-right of your screen (left screenshot below)\nIf your Project isn’t open, click on the R-in-a-box icon to open it (right screenshot below):\n\n\n\n\n\n\nHere, the project jelmer is open.Your Project name is also your username.\n\n\n\n\n\n\nOpening an RStudio Project\n\n\n\n\n\n\n\n\n\n\nDon’t have an RStudio Project yet? Click here to create one.\n\n\n\n\n\n\nClick File (top bar, below your browser’s address bar) &gt; New Project\nIn the popup window, click Existing Directory.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick Browse... to select your personal dir.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nIn the next window, you should be in your Home directory (abbreviated as ~), from which you can’t click your way to /fs/ess! Instead, you’ll first have to click on the (very small!) ... highlighted in the screenshot below:\n\n\n\n\n\n\n\nType at least part of the path to your personal dir in /fs/ess/PAS2714/users, e.g. like shown below, and click OK:\n\n\n\n\n\n\n\nNow you should be able to browse/click the rest of the way to your personal directory, something like /fs/ess/PAS2714/users/jelmer.\nClick Choose to pick your selected directory.\nClick Create Project.\n\nRStudio should reload and you should now have your new Project “open”.\n\n\n\n\n\n\nCreate a new script\nClick File &gt; New file &gt; R script, and immediately save the new file (File &gt; Save as) as dada.R inside your scripts directory1.\nWe recommend that you copy-and-paste (or type, if you prefer) code from this webpage into your script and then execute the code. That way, you’ll have a nice record of what you did, exactly.\n\n\n\nLoad R packages\nTo save time, we have already installed all the necessary R packages at OSC into a custom library. To add this library for the current R session:\n\n# Set the R library location\n.libPaths(\"/fs/ess/PAS0471/jelmer/R/metabar\")\ndyn.load(\"/fs/ess/PAS0471/jelmer/software/GLPK/lib/libglpk.so.40\", local = FALSE)\n\nThen load the packages:\n\n# Load the packages\nlibrary(dada2)\nlibrary(phyloseq)\nlibrary(tidyverse)\n\n# (You should get a number of package startup messages - not shown)\n\ndada2 - Functions to assemble amplicon sequence varients (ASVS), with single nucleotide resolution, including filtering, dereplication, ASV inference, merging of paired (FWD and REV) and construction of a sequence table and a taxonomy table.\nphyloseq - A package for organizing, analyzing and visualizing microbial community analysis.\ntidyverse - A package for speeding up, organizing, streamlining and visualizing data science."
  },
  {
    "objectID": "05_dada.html#preparing-to-run-the-workflow",
    "href": "05_dada.html#preparing-to-run-the-workflow",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "2 Preparing to run the workflow",
    "text": "2 Preparing to run the workflow\n\nSet the number of cores\nMost dada2 functions can use multiple cores. Because we are on a cluster and we have reserved only part of a node, auto-detection of the number of cores will not be appropriate (the program will detect more cores than we have available). Therefore, we should specify the appropriate number of cores in our function calls below.\nWe will set the number of cores here, assuming you requested 4 cores in your job submission (change if needed):\n\n# Set the number of cores\nn_cores &lt;- 4\n\n\n\nSet file paths\nSetting file paths in the beginning to make things easier to change or troubleshoot in the future.\nInput files:\n\n# Define the input files\n\n# Dir with input FASTQ files (post-cutadapt trimmed reads):\nindir &lt;- \"results/cutadapt\"\n\n# FASTA file with training data - a database your sequences will be compared to (SILVA, UNITE, etc):\n# (Check for an up-to-date version at &lt;https://benjjneb.github.io/dada2/training.html&gt;)\ntax_key &lt;- \"data/ref/silva_nr99_v138.1_train_set.fa.gz\"  \n\n# File with sample metadata (data about your samples, treatments, blocking, etc):\nmetadata_file &lt;- \"data/meta/meta.tsv\"\n\n\n\n\n\n\n\nDon’t have the input files? Click here to copy them.\n\n\n\n\n\nNext to the Console in R, click the Terminal to access a Unix shell. In the Terminal, run the following cp commands to copy the necessary data.\n\nIf you’re missing the cutadapt results because you didn’t manage to run this in the previous session:\ncp -rv /fs/ess/PAS2714/share/results2/cutadapt /fs/ess/PAS2714/users/$USER/results\nYou really should have the data and (other) results already, but if not:\ncp -rv /fs/ess/PAS2714/share/data /fs/ess/PAS2714/users/$USER\ncp -rv /fs/ess/PAS2714/share/results /fs/ess/PAS2714/users/$USER\n\n\n\n\nOutput files:\n\n# Define the output dirs\noutdir &lt;- \"results/dada\"                     # Most output\nfilter_dir &lt;- \"results/dada/filtered_fastq\"  # Filtered FASTQ files\n\n# Create directories if they are not already created:\ndir.create(outdir, showWarnings = FALSE, recursive = TRUE)\ndir.create(filter_dir, showWarnings = FALSE, recursive = TRUE)\n\n\n\nAssign forward and reverse reads\nWe will assign the FASTQ files that we processed with cutadapt to two vectors: one with files with forward reads, and one with files with reverse reads. These files can be distinguished by having “R1” (forward) and “R2” (reverse) in their names.\n\n# Create vectors with FASTQ file names\nfastqs_raw_F &lt;- sort(list.files(indir, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfastqs_raw_R &lt;- sort(list.files(indir, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\nhead(fastqs_raw_F)\n\n[1] \"results/cutadapt/NW102AB_R1.fastq.gz\"\n[2] \"results/cutadapt/NW102C_R1.fastq.gz\" \n[3] \"results/cutadapt/NW103AB_R1.fastq.gz\"\n[4] \"results/cutadapt/NW103C_R1.fastq.gz\" \n[5] \"results/cutadapt/NW201AB_R1.fastq.gz\"\n[6] \"results/cutadapt/NW201C_R1.fastq.gz\" \n\n\n\n\nCheck sample IDs\nWe’ll get the sample IDs from the FASTA file names and from a file with metadata, and will check if they are the same. First we’ll prepare the metadata:\n\n# Read and prepare the metadata\nmetadata_df &lt;- read.table(file = metadata_file, sep = \"\\t\", header = TRUE)\n\ncolnames(metadata_df)[1] &lt;- \"SampleID\"\nrownames(metadata_df) &lt;- metadata_df$SampleID\n\nhead(metadata_df)\n\n        SampleID Location Rotation  Plot Block\nNW102AB  NW102AB    NWARS       CS 102AB   100\nNW102C    NW102C    NWARS       CS  102C   100\nNW103AB  NW103AB    NWARS      CSW 103AB   100\nNW103C    NW103C    NWARS      CSW  103C   100\nNW201AB  NW201AB    NWARS      CSW 201AB   200\nNW201C    NW201C    NWARS      CSW  201C   200\n\n\nLet’s compare the sample IDs from the metadata with the FASTQ filenames:\n\n# Check the sample IDs in the metadata\nmetadata_df$SampleID\n\n [1] \"NW102AB\" \"NW102C\"  \"NW103AB\" \"NW103C\"  \"NW201AB\" \"NW201C\"  \"NW203A\" \n [8] \"NW203BC\" \"NW304A\"  \"NW304BC\" \"NW305AB\" \"NW305C\"  \"NW403A\"  \"NW403BC\"\n[15] \"NW404A\"  \"NW404BC\" \"W101AB\"  \"W101C\"   \"W103AB\"  \"W103C\"   \"W204A\"  \n[22] \"W204BC\"  \"W205A\"   \"W205BC\"  \"W303AB\"  \"W303C\"   \"W304AB\"  \"W304C\"  \n[29] \"W403AB\"  \"W403C\"   \"W404A\"   \"W404BC\" \n\n\n\n# Check the FASTA file names\n# (Note, basename() strips the dir name from the filename)\nhead(basename(fastqs_raw_F))\n\n[1] \"NW102AB_R1.fastq.gz\" \"NW102C_R1.fastq.gz\"  \"NW103AB_R1.fastq.gz\"\n[4] \"NW103C_R1.fastq.gz\"  \"NW201AB_R1.fastq.gz\" \"NW201C_R1.fastq.gz\" \n\n\nTo extract the sample IDs from the FASTQ file names, we remove everything after _R from the file names using the sub() function:\n\n# Extract the sample IDs from the FASTQ file names\n# sub() arguments: sub(pattern, replacement, vector)\nsampleIDs &lt;- sub(\"_R.*\", \"\", basename(fastqs_raw_F))\nhead(sampleIDs)\n\n[1] \"NW102AB\" \"NW102C\"  \"NW103AB\" \"NW103C\"  \"NW201AB\" \"NW201C\" \n\n\nWe can check whether the IDs from the FASTQ files and the metadata data frame are the same:\n\n# Check whether the sample IDs match\nidentical(sort(metadata_df$SampleID), sampleIDs)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nThe above code should have returned TRUE but if it returned FALSE, we should:\n\n\n\n\nCheck whether any samples are missing from the FASTQ files:\n\nsetdiff(sort(metadata_df$SampleID), sampleIDs)\n\ncharacter(0)\n\n\nCheck whether any samples are missing from the metadata:\n\nsetdiff(sampleIDs, sort(metadata_df$SampleID))\n\ncharacter(0)"
  },
  {
    "objectID": "05_dada.html#filtering-and-quality-trimming",
    "href": "05_dada.html#filtering-and-quality-trimming",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "3 Filtering and quality trimming",
    "text": "3 Filtering and quality trimming\nWe will now perform quality filtering (removing poor-quality reads) and trimming (removing poor-quality bases) on the FASTQ files using DADA2’s filterAndTrim() function.\nThe filterAndTrim() function will write the filtered and trimmed reads to new FASTQ files. Therefore, we first define the file names for the new files:\n\n# Define the paths to the filtered (output) FASTQ files\n# (filter_dir directory set and created in the \"set file paths\" step,\n# still have seperate names for FWD and RVS reads at this point.)\nfastqs_filt_F &lt;- file.path(filter_dir, paste0(sampleIDs, \"_F_filt.fastq\"))\nfastqs_filt_R &lt;- file.path(filter_dir, paste0(sampleIDs, \"_R_filt.fastq\"))\n\nWe will use the following arguments of the filterAndTrim() function:\n\ntruncLen - Truncate reads after truncLen bases and discard reads shorter than this. Could be adjusted depending on the primers that are used. Also could be left out in the case of a more variable sequence (Ex: ITS). The trimming length can be different for forward and reverse reads, which is good because reverse reads are often of worse quality.\nIt is also suggested to trim the first 10 nucleotides of each read (trimLeft argument), since these positions are likely to contain errors.\nmaxEE is an important argument that will let DADA2 trim reads based on the maximum numbers of Expected Errors (EE) given the quality scores of the reads’ bases.\ntrunQ - trim read after an instance of a quality score less than or equal to this number\n\n\n# Filter the FASTQ files\nfilter_results &lt;-\n  filterAndTrim(fastqs_raw_F, fastqs_filt_F,\n                fastqs_raw_R, fastqs_filt_R,\n                truncLen = c(250,210),\n                trimLeft = 10,\n                maxN = 0,\n                maxEE = c(2,2),\n                truncQ = 2,\n                rm.phix = FALSE,\n                multithread = n_cores,\n                compress = FALSE, verbose = TRUE) \n\nhead(filter_results)\n\n                    reads.in reads.out\nNW102AB_R1.fastq.gz    12507      8681\nNW102C_R1.fastq.gz     14178      9657\nNW103AB_R1.fastq.gz    11814      7569\nNW103C_R1.fastq.gz     14704     10221\nNW201AB_R1.fastq.gz    12031      8125\nNW201C_R1.fastq.gz     12223      8188"
  },
  {
    "objectID": "05_dada.html#dereplication-and-error-training",
    "href": "05_dada.html#dereplication-and-error-training",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "4 Dereplication and error training",
    "text": "4 Dereplication and error training\nNext, we want to “dereplicate” the filtered FASTQ files. During dereplication, we condense the data by collapsing together all reads that encode the same sequence, which significantly reduces later computation times.\n\n# Dereplicate the FASTQ files\nfastqs_derep_F &lt;- derepFastq(fastqs_filt_F, verbose = FALSE)\nfastqs_derep_R &lt;- derepFastq(fastqs_filt_R, verbose = FALSE)\n\nnames(fastqs_derep_F) &lt;- sampleIDs\nnames(fastqs_derep_R) &lt;- sampleIDs\n\nThe DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution.\nThis is a computationally intensive step and is one of the most time consuming steps of the dada pipeline in my experience (depending on size of dataset could be up to 24 hours or more). The multithread argument here speeds along the process. (If your dataset is very large consider using a “hugemem” node at OSC for this step.)\n\n# Error training\nerr_F &lt;- learnErrors(fastqs_derep_F, multithread = n_cores, verbose = TRUE)\nerr_R &lt;- learnErrors(fastqs_derep_R, multithread = n_cores, verbose = TRUE)\n\n71510640 total bases in 297961 reads from 32 samples will be used for learning the error rates.\nInitializing error rates to maximum possible estimate.\nselfConsist step 1 ................................\n   selfConsist step 2\n   selfConsist step 3\n   selfConsist step 4\n   selfConsist step 5\n   selfConsist step 6\nConvergence after  6  rounds.\n\n59592200 total bases in 297961 reads from 32 samples will be used for learning the error rates.\nInitializing error rates to maximum possible estimate.\nselfConsist step 1 ................................\n   selfConsist step 2\n   selfConsist step 3\n   selfConsist step 4\n   selfConsist step 5\n   selfConsist step 6\n   selfConsist step 7\nConvergence after  7  rounds.\nWe’ll plot errors to verify that error rates have been reasonable well-estimated. Pay attention to the fit between observed error rates (points) and fitted error rates (lines):\n\n# Plot the error profiles\nplotErrors(err_F, nominalQ = TRUE)\nplotErrors(err_R, nominalQ = TRUE)\n\n\n\nClick to see the expected plots\n\nFor the forward reads:\n\nFor the reverse reads:\n\n\nThese plot show error rates from the transition between all base pair transitions:\n\nBlack points are observed error rates.\nThe black line is estimated error rates using machine learning.\nThe red line shows expected error rates under the nominal definition of the quality score.\n\nLooking for the estimated rate to fit most of the observed points and that error rates drop with increased quality scores."
  },
  {
    "objectID": "05_dada.html#infer-asvs",
    "href": "05_dada.html#infer-asvs",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "5 Infer ASVs",
    "text": "5 Infer ASVs\nWe will now run the core dada algorithm, which infers Amplicon Sequence Variants (ASVs) from the sequences.\nThis step is quite computationally intensive, and for this tutorial, we will therefore perform independent inference for each sample (pool = FALSE), which will keep the computation time down. In my experience, this step could take several hours with large datasets.\nPooling will increase computation time, especially if you have many samples, but will improve detection of rare variants seen once or twice in an individual sample, but many times across all samples. Therefore, for your own analysis, you will likely want to use pooling, though “pseudo-pooling” is also an option.\n\n# Infer ASVs\ndada_Fs &lt;- dada(fastqs_derep_F, err = err_F, pool = FALSE, multithread = n_cores)\ndada_Rs &lt;- dada(fastqs_derep_R, err = err_R, pool = FALSE, multithread = n_cores)\n\nSample 1 - 8681 reads in 5956 unique sequences.\nSample 2 - 9657 reads in 5718 unique sequences.\nSample 3 - 7569 reads in 5594 unique sequences.\nSample 4 - 10221 reads in 7071 unique sequences.\nSample 5 - 8125 reads in 4975 unique sequences.\nSample 6 - 8188 reads in 5951 unique sequences.\n# [...output truncated...]\nLet’s inspect one of the resulting objects:\n\ndada_Fs[[1]]\n\ndada-class: object describing DADA2 denoising results\n343 sequence variants were inferred from 5956 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16"
  },
  {
    "objectID": "05_dada.html#merge-read-pairs",
    "href": "05_dada.html#merge-read-pairs",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "6 Merge read pairs",
    "text": "6 Merge read pairs\nIn this step, we will first merge the forward and reverse read pairs: the fragment that we amplified with our primers was short enough to generate lots of overlap among the sequences from the two directions.\n\n# Merge forward and reverse reads\nmergers &lt;- mergePairs(dada_Fs, fastqs_derep_F,\n                      dada_Rs, fastqs_derep_R,\n                      verbose = TRUE)\n\n5463 paired-reads (in 253 unique pairings) successfully merged out of 6865 (in 531 pairings) input.\n6421 paired-reads (in 325 unique pairings) successfully merged out of 7887 (in 606 pairings) input.\n3934 paired-reads (in 169 unique pairings) successfully merged out of 5650 (in 433 pairings) input.\n6144 paired-reads (in 248 unique pairings) successfully merged out of 8082 (in 609 pairings) input.\n5519 paired-reads (in 243 unique pairings) successfully merged out of 6548 (in 478 pairings) input.\n4693 paired-reads (in 181 unique pairings) successfully merged out of 6298 (in 449 pairings) input.\n# [...output truncated...]\nJust like tables can be saved in R using write.table() or write.csv(), R objects can be saved using saveRDS(). The resulting RDS file can then be loaded into an R environment using readRDS(). This is a convenient way to save R objects that require a lot of computation time.\nWe should not be needing the very large dereplicated sequence objects anymore, but to be able to quickly restart our analysis from a new R session if necessary, we now save these objects to RDS files. And after that, we can safely remove these objects from our environment.\n\n# Save the dereplicated objects\nsaveRDS(fastqs_derep_F, file = file.path(outdir, \"fastqs_derep_F.rds\"))\nsaveRDS(fastqs_derep_R, file = file.path(outdir, \"fastqs_derep_R.rds\"))\n\nrm(fastqs_derep_F, fastqs_derep_R) # Remove objects from environment"
  },
  {
    "objectID": "05_dada.html#construct-a-sequence-table",
    "href": "05_dada.html#construct-a-sequence-table",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "7 Construct a sequence table",
    "text": "7 Construct a sequence table\nNext, we construct an Amplicon Sequence Variant table (ASV) table:\n\n# Create the sequence table\nseqtab_all &lt;- makeSequenceTable(mergers)\n\nLet’s check the dimensions of this table, which are the number of samples (rows) and the number of ASVs (columns):\n\n# Check the dimensions of the sequence table\ndim(seqtab_all)\n\n[1]   32 2004\nLet’s inspect the distribution of sequence lengths, these should match the expected length of your amplicon:\n\n# Check the ASV sequence lengths\ntable(nchar(getSequences(seqtab_all)))\n\n 240  249  251  253 \n1999    2    1    2 \n\n\n\n\n\n\nIf you want to filter your ASVs to be within a certain range length (Click to expand)\n\n\n\n\n\nYou can do that as follows:\n\n# Define a minimum ASV length threshold\nmin_len &lt;- 235\n\n# Define a maximum ASV length threshold\nmax_len &lt;- 245\n\n# Filter the sequence table\nseqtab_all &lt;- seqtab_all[, nchar(colnames(seqtab_all)) %in% seq(min_len, max_len)]"
  },
  {
    "objectID": "05_dada.html#remove-chimeras",
    "href": "05_dada.html#remove-chimeras",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "8 Remove chimeras",
    "text": "8 Remove chimeras\nNow, we will remove chimeras. The dada algorithm models and removes substitution errors, but chimeras are another importance source of spurious sequences in amplicon sequencing. Chimeras are formed during PCR amplification. When one sequence is incompletely amplified, the incomplete amplicon primes the next amplification step, yielding a spurious amplicon. The result is a sequence read which is half of one sample sequence and half another.\nFortunately, the accuracy of the sequence variants after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.\n\n# Identify and remove chimeras\nseqtab &lt;- removeBimeraDenovo(seqtab_all,\n                             method = \"consensus\",\n                             multithread = n_cores,\n                             verbose = TRUE)\n\nIdentified 1 bimeras out of 2004 input sequences.\nLet’s check the proportion of sequences that was retained:\n\n# Proportion of retained sequences:\nsum(seqtab) / sum(seqtab_all)\n\n[1] 0.9992489\nWe will save the seqtab object as an RDS file:\n\n# Save the sequence table\nsaveRDS(seqtab, file = file.path(outdir, \"seqtab.rds\"))"
  },
  {
    "objectID": "05_dada.html#generate-a-summary-table",
    "href": "05_dada.html#generate-a-summary-table",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "9 Generate a summary table",
    "text": "9 Generate a summary table\nIn this step, we will generate a summary table of the number of sequences processed and outputs of different steps of the pipeline.\nThis information is generally used to further evaluate characteristics and quality of the run, sample-to-sample variation, and resulting sequencing depth for each sample.\nTo get started, we will define a function getN() that will get the number of unique reads for a sample. Then, we apply getN() to each element of the dada_Fs, dada_Rs, and merged objects, which gives us vectors with the number of unique reads for each sample, during each of these steps:\n\n# Get the numbers of unique reads at each step\ngetN &lt;- function(x) sum(getUniques(x))\n\ndenoised_F &lt;- sapply(dada_Fs, getN)\ndenoised_R &lt;- sapply(dada_Rs, getN)\nmerged &lt;- sapply(mergers, getN)\n\nWe’ll join these vectors together with the filter_results dataframe, and the number of non-chimeric reads:\n\n# Create a summary table\nnreads_summary &lt;- data.frame(filter_results,\n                             denoised_F,\n                             denoised_R,\n                             merged,\n                             nonchim = rowSums(seqtab),\n                             row.names = sampleIDs)\ncolnames(nreads_summary)[1:2] &lt;- c(\"input\", \"filtered\")\n\n\n# Have a look at the first few rows of the summary table\nhead(nreads_summary)\n\n        input filtered denoised_F denoised_R merged nonchim\nNW102AB 12507     8681       7340       7475   5463    5463\nNW102C  14178     9657       8341       8529   6421    6421\nNW103AB 11814     7569       6120       6233   3934    3934\nNW103C  14704    10221       8559       8784   6144    6144\nNW201AB 12031     8125       6965       7060   5519    5519\nNW201C  12223     8188       6797       6799   4693    4693\nFinally, we’ll write this table to file:\n\n# Write the summary table to file\nwrite.table(nreads_summary,\n            file = file.path(outdir, \"nreads_summary.txt\"),\n            sep = \"\\t\", quote = FALSE, row.names = TRUE)"
  },
  {
    "objectID": "05_dada.html#assign-taxonomy-to-asvs",
    "href": "05_dada.html#assign-taxonomy-to-asvs",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "10 Assign taxonomy to ASVs",
    "text": "10 Assign taxonomy to ASVs\nNow, we will assign taxonomy to our ASVs.\nDepending on the marker gene and the data, you will have to choose the appropriate reference file for this step. Several files have been formatted for taxonomy assignments in DADA2 pipeline and are available at the DADA2 website. Here we are using SILVA a comprehensive, quality checked and regularly updated datasets of aligned small (16S/18S, SSU) and large subunit (23S/28S, LSU) ribosomal RNA (rRNA) sequences for all three domains of life (Bacteria, Archaea and Eukarya) (https://www.arb-silva.de/).\n\n# Assign taxonomy to the ASVs\ntax &lt;- assignTaxonomy(seqtab, tax_key, multi = TRUE, verbose = TRUE)\nsaveRDS(tax, file.path(outdir, \"tax.rds\"))\n\nWe can check whether the sequences in our taxonomy table and sequence table match:\n\nif(!identical(getSequences(tax), getSequences(seqtab))) stop(\"Taxonomy mismatch.\")\n\nAnd take a look at how many ASVs belong to each phylum:\n\n# How many ASVs are in each phylum?\ntable(tax[,\"Phylum\"], useNA = \"ifany\")\n\n\n Abditibacteriota   Acidobacteriota  Actinobacteriota    Armatimonadota      Bacteroidota  Bdellovibrionota       Chloroflexi     Crenarchaeota \n                1               235               344                19               180                 7               229                17 \n    Cyanobacteria      Dependentiae  Desulfobacterota   Elusimicrobiota Entotheonellaeota    Fibrobacterota        Firmicutes   Gemmatimonadota \n               24                 2                 7                 2                 6                 1                40                78 \n Latescibacterota            MBNT15 Methylomirabilota       Myxococcota      Nitrospirota   Patescibacteria   Planctomycetota    Proteobacteria \n               19                 1                 5               118                 1                16               194               336 \n          RCP2-54     Spirochaetota       Sumerlaeota Verrucomicrobiota             WPS-2              &lt;NA&gt; \n                4                 1                 1                94                 5                16"
  },
  {
    "objectID": "05_dada.html#generate-output-files",
    "href": "05_dada.html#generate-output-files",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "11 Generate output files",
    "text": "11 Generate output files\nIn this last step, we will generate output files from the DADA2 outputs that are formatted for downstream analysis in phyloseq. First, we will write a FASTA file with the final ASV sequences. (This FASTA file can also be used for phylogenetic tree inference with different R packages.)\n\n# Write the output files\n\n# Prepare sequences and headers:\nasv_seqs &lt;- colnames(seqtab)\nasv_headers &lt;- paste(\"&gt;ASV\", 1:ncol(seqtab), sep = \"_\")\n\n# Interleave headers and sequences:\nasv_fasta &lt;- c(rbind(asv_headers, asv_seqs))\n\n# Write fasta file:\nwrite(asv_fasta, file = file.path(outdir, \"ASVs.fa\"))\n\nNow, we build the final phyloseq object. Notes:\n\nWhile we will not add a phylogenetic tree now, this can also be added to a phyloseq object.\nOur metadata dataframe contains three samples that we don’t have sequences for. However, this is not a problem: phyloseq will match the sample IDs in the metadata with those in the OTU table, and disregard IDs not present in the OTU table.\n\nFirst we need to strip the “R1/R2.fastq.gz” suffixes from the sequence table, in order to make the sample IDs match:\n\n# Fix the sample IDs in the sequence table\nrownames(seqtab) &lt;- sub(\"_R.*\", \"\", rownames(seqtab))\n\nThen we can create the phyloseq object:\n\n# Create the phyloseq object\nps &lt;- phyloseq(otu_table(seqtab, taxa_are_rows = FALSE),\n               sample_data(metadata_df),\n               tax_table(tax))\n\n# Saves the phyloseq object as an .rds file (which can be imported directly by phyloseq):\nsaveRDS(ps, file = file.path(outdir, \"ps.rds\"))"
  },
  {
    "objectID": "05_dada.html#in-closing",
    "href": "05_dada.html#in-closing",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "12 In closing",
    "text": "12 In closing\n\n12.1 Further reading\n\nFor another rundown of the dada2 pipeline take a look at the excellent tutorial at https://benjjneb.github.io/dada2/tutorial.html\nFor a tutorial for reads generated using PacBio sequencing see: https://benjjneb.github.io/LRASManuscript/LRASms_Zymo.html\nTaxonomic references for dada: https://benjjneb.github.io/dada2/training.html\n\n\n\n12.2 Attribution\nThis documented was adapted from Callahan et al. 2006 by Matthew Willman, with further edits by Soledad Benitez Ponce, Timothy Frey and Jelmer Poelstra."
  },
  {
    "objectID": "05_dada.html#bonus-phylogenetic-tree-estimation",
    "href": "05_dada.html#bonus-phylogenetic-tree-estimation",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "13 Bonus: Phylogenetic tree estimation",
    "text": "13 Bonus: Phylogenetic tree estimation\nA phylogenetic tree can be estimated for the ASVs you inferred. Depending on the number of ASVs recovered and the phylogenetic tree algorithm of choice, this step could take many hours. Simpler trees will be less computationally intensive. Depending on the marker gene you are working on, you may or may not choose to perform this step.\nThis step should be conducted with the final sequence table seqtab after removing chimeras. Then, the tree can be included in the phyloseq object, so if you have a tree, you should rerun the phyloseq() function to create an object that includes it.\nFirst, we’ll need to load the phangorn package for estimation of phylogenetic trees in R.\n\n# Load the phangorn package\nlibrary(phangorn)\n\nNow, run the following steps to estimate a phylogenetic tree:\n\n# Extract the ASV sequences from the seqtab object\nseqs &lt;- getSequences(seqtab)\n\n# This propagates to the tip labels of the tree.\n# At this stage ASV labels are full ASV sequence\nnames(seqs) &lt;- seqs \nalignment &lt;- AlignSeqs(DNAStringSet(seqs),\n                       anchor = NA,\n                       iterations = 5,\n                       refinements = 5)\n\n# Compute pairwise distances among ASVs\nphang.align &lt;- phyDat(as(alignment, \"matrix\"), type = \"DNA\")\ndm &lt;- dist.ml(phang.align)\ntreeNJ &lt;- NJ(dm)\nfit &lt;- pml(treeNJ, data = phang.align)\n\n# Fit a GTR model\nfitGTR &lt;- update(fit, k = 4, inv = 0.2)\n\n# ....\nfitGTR &lt;- optim.pml(fitGTR,\n                    model = \"GTR\",\n                    optInv = TRUE,\n                    optGamma = TRUE,\n                    rearrangement = \"stochastic\",\n                    control = pml.control(trace = 0))\n\nTo create a phyloseq object that includes the tree:\n\nTODO"
  },
  {
    "objectID": "05_dada.html#footnotes",
    "href": "05_dada.html#footnotes",
    "title": "Calling ASVs with the DADA2 pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n (you can create that dir in the dialog box if needed↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for an Ohio State University workshop on Amplicon Metabarcoding, held from March 13-15, 2024 in Wooster, Ohio.\nThis workshop is co-organized by:\n\nDr. Soledad Benitez-Ponce, Dept. of Plant Pathology (Personal website)\nDr. Timothy Frey, Dept. of Plant Pathology\nFiama Guevara, Dept. of Plant Pathology\nMelanie Medina Lopez, Dept. of Plant Pathology\nDr. Jelmer Poelstra, Molecular and Cellular Imaging Center (MCIC)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "homework/shell.html#why-the-unix-shell",
    "href": "homework/shell.html#why-the-unix-shell",
    "title": "Homework: Intro to the Unix Shell",
    "section": "1 Why the Unix shell?",
    "text": "1 Why the Unix shell?\nMany of the things you typically do by pointing and clicking can alternatively be done by typing commands. The Unix shell allows you to interact with computers via commands.\nHere are some reasons why you may want to use this seemingly archaic technique:\n\nWorking efficiently with large files\nMaking it easier to repeat (& automate) similar tasks across files, samples, and projects\nAchieving better reproducibility in research\nAt least in bioinformatics, being able to use access the largest and most recent set of approaches and all their options — many graphical user interface programs lag behind in functionality and may cost money as well.\nWorking effectively with remote high-performance computing like at the Ohio Supercomputer Center (OSC)\n\n\n\n\n\n\n\nSide note: Some Unix shell terminology (Click to expand)\n\n\n\n\n\nHere are a few interrelated terms you’re likely to run across:\n\nCommand Line — the most general term, an interface1 where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile you’ve seen that these are not all synonyms, in day-to-day computing/bioinformatics, they are often used interchangeably."
  },
  {
    "objectID": "homework/shell.html#how-to-go-through-this-page",
    "href": "homework/shell.html#how-to-go-through-this-page",
    "title": "Homework: Intro to the Unix Shell",
    "section": "2 How to go through this page",
    "text": "2 How to go through this page\nYou will be using a Unix shell at the Ohio Supercomputer Center (OSC) — see the instructions below to open one.\nPlease follow along actively by typing and executing all commands shown below (unless it explicitly says you shouldn’t run something), not just the section that are labeled as “exercises”. If you skip certain commands, later ones will in many cases not work.\n\nOpening a Unix shell at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, click on the “Clusters” dropdown menu and then click Pitzer Shell Access.\nA Unix shell will open in a new browser tab (see screenshot below). You’re ready to go!\n\n\n\n\n\n\n\n\n\nUsing this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if the latter, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):\n\n\nYou may also want to change the shell’s color scheme by selecting an option other than “Default” in the “Themes:” dropdown menu in the top-right."
  },
  {
    "objectID": "homework/shell.html#the-basics",
    "href": "homework/shell.html#the-basics",
    "title": "Homework: Intro to the Unix Shell",
    "section": "3 The basics",
    "text": "3 The basics\n\n3.1 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-directory&gt;]$\nFor example (note that ~ means your Home directory/folder):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\n“Directory” (or “dir”) for short is Unix-speak for a computer “folder”\n\n\n\n\n\n\n\n\n\n3.2 A few simple commands: date, whoami, pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nTue Mar 5 09:11:51 EST 2024\nThe whoami (who-am-i) command prints your username:\nwhoami\njelmer\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n# [Yours will be different! You are in your Home directory.]\n\nAll 3 of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n\n\n\nWorking directory and paths (we’ll talk more about paths later)\n\n\n\n\nWhen working in a Unix shell, you are always “in” a specific directory: your working directory (“working dir” for short).\nIn a path (= location of a file or directory) such as that output by pwd, directories are separated by forward slashes /.\n\n\n\n\n\n\n\n\n\nCase and spaces\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names.\nAvoid spaces in file and directory names!2 Use e.g. underscores to distinguish words (my_long_filename).\n\n\n\n\n\n\n3.3 cd and command actions & arguments\nIn the above three command line expressions:\n\nWe merely typed a command and nothing else\nThe command provided some information, which was printed to screen\n\nBut many commands perform an action other than providing information. For example, you can use the command cd to Change Directory (i.e. change your working dir). And like many commands that perform an action, cd normally has no output at all.\nLet’s use cd to move to another directory by specifying the path to that directory after the cd command:\ncd /fs/ess/PAS2714\npwd\n/fs/ess/PAS2714\nIn more abstract terms, what we did above was to provide cd with an argument, namely the path of the dir to move to. Arguments generally tell commands what file(s) or directory/ies to operate on.\nAs we’ve seen, then, cd gives no output when it successfully changed the working directory. But let’s also see what happens when it does not succeed — it gives an error:\ncd /fs/ess/PAs2714\nbash: cd: /fs/ess/PAs2714: No such file or directory\n\n\nWhat was the problem with the path we specified? (Click to see the answer)\n\nWe used a lowercase “s” in /PAs2714/ — this should have been /PAS2714/.\nAs pointed out above, everything, including paths, is case-sensitive in the Unix shell!\n\n\n\n\n3.4 ls and command options\n\nThe default behavior of ls\nThe ls command, short for “list”, will list files and directories:\nls\nsandbox   share   users\n(You should still be in /fs/ess/PAS2714. If not, cd there first.)\n\n\n\n\n\n\nSide note: ls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like data and metadata above)\nEntries in black are regular files (like README.md above)\nEntries in red are compressed files (we’ll see an example soon).\n\n\n\n\nBy default, ls will list files and dirs in your current working dir, and in the way shown above. For which dir ls lists files and dirs can be changed with arguments, and how ls shows the output can be changed with options.\n\n\n\nOptions\nIn general, whereas arguments tell a command what to operate on, options will modify its behavior. For example, we can call ls with the option -l (a dash followed by a lowercase L):\nls -l \ntotal 2\ndrwxr-xr-x+ 2 jelmer PAS0471 4096 Mar  1 16:23 sandbox\ndrwxr-xr-x+ 4 jelmer PAS0471 4096 Mar  1 16:13 share\ndrwxrwxrwx+ 3 jelmer PAS0471 4096 Mar  1 16:19 users\nNotice that it lists the same items as above, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -l -h\ntotal 1.5K\ndrwxr-xr-x+ 2 jelmer PAS0471 4.0K Mar  1 16:23 sandbox\ndrwxr-xr-x+ 4 jelmer PAS0471 4.0K Mar  1 16:13 share\ndrwxrwxrwx+ 3 jelmer PAS0471 4.0K Mar  1 16:19 users\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\nConveniently, options can be “pasted together” as follows:\nls -lh\n# (Output not shown, same as above)\n\n\n\nArguments\nArguments to ls should be dirs or files to operate on. For example, if we wanted to see what’s inside the share dir, instead of inside our working dir, we could type3:\nls share\ndata  README.md  results\n\n\n\nIntermezzo: file viewing and a quick intro to the dataset\nTo find out what data is contained in this dir, let’s take a look at the README.md file, which provides some information about the data set we will work with during the workshop.\nThere are several commands to view the contents of files — the simplest is cat, which will print the entire contents of a file to screen:\ncat share/README.md\nThis 16S amplicon metabarcoding data set compares soil bacterial populations\nunder two different rotational schemes (corn-soy) vs (corn-soy-wheat) at\ntwo research farms in Ohio (Northwest Agricultural Research Station(NW) and Western Agricultural Research Station (W)).\nThere are 32 plots (Ex: 102A) in four blocks (100-400).\nPlots were split into A and BC plots to include a cover crop treatment.\nThe head command will only print the first 10 lines of a file. Let’s use that to examine this dataset’s metadata file:\nhead share/data/meta/meta.tsv\nSampleID        Location        Rotation        Plot    Block\nNW102AB NWARS   CS      102AB   100\nNW102C  NWARS   CS      102C    100\nNW103AB NWARS   CSW     103AB   100\nNW103C  NWARS   CSW     103C    100\nNW201AB NWARS   CSW     201AB   200\nNW201C  NWARS   CSW     201C    200\nNW203A  NWARS   CS      203A    200\nNW203BC NWARS   CS      203BC   200\nNW304A  NWARS   CSW     304A    300\n\n\nLet’s dig a little deeper and check the share/data dir:\nls share/data\nfastq  meta  ref\nThe data dir appears to contain three (sub)dirs with different kinds of data. We’ll talk in detail about that later, but for now let’s look inside the fastq dir:\nls share/data/fastq\nNW102AB_R1.fastq.gz  NW201C_R1.fastq.gz   NW305AB_R1.fastq.gz  NW404BC_R1.fastq.gz  W204A_R1.fastq.gz   W303C_R1.fastq.gz   W404A_R1.fastq.gz\nNW102AB_R2.fastq.gz  NW201C_R2.fastq.gz   NW305AB_R2.fastq.gz  NW404BC_R2.fastq.gz  W204A_R2.fastq.gz   W303C_R2.fastq.gz   W404A_R2.fastq.gz\nNW102C_R1.fastq.gz   NW203A_R1.fastq.gz   NW305C_R1.fastq.gz   W101AB_R1.fastq.gz   W204BC_R1.fastq.gz  W304AB_R1.fastq.gz  W404BC_R1.fastq.gz\nNW102C_R2.fastq.gz   NW203A_R2.fastq.gz   NW305C_R2.fastq.gz   W101AB_R2.fastq.gz   W204BC_R2.fastq.gz  W304AB_R2.fastq.gz  W404BC_R2.fastq.gz\nNW103AB_R1.fastq.gz  NW203BC_R1.fastq.gz  NW403A_R1.fastq.gz   W101C_R1.fastq.gz    W205A_R1.fastq.gz   W304C_R1.fastq.gz\nNW103AB_R2.fastq.gz  NW203BC_R2.fastq.gz  NW403A_R2.fastq.gz   W101C_R2.fastq.gz    W205A_R2.fastq.gz   W304C_R2.fastq.gz\nNW103C_R1.fastq.gz   NW304A_R1.fastq.gz   NW403BC_R1.fastq.gz  W103AB_R1.fastq.gz   W205BC_R1.fastq.gz  W403AB_R1.fastq.gz\nNW103C_R2.fastq.gz   NW304A_R2.fastq.gz   NW403BC_R2.fastq.gz  W103AB_R2.fastq.gz   W205BC_R2.fastq.gz  W403AB_R2.fastq.gz\nNW201AB_R1.fastq.gz  NW304BC_R1.fastq.gz  NW404A_R1.fastq.gz   W103C_R1.fastq.gz    W303AB_R1.fastq.gz  W403C_R1.fastq.gz\nNW201AB_R2.fastq.gz  NW304BC_R2.fastq.gz  NW404A_R2.fastq.gz   W103C_R2.fastq.gz    W303AB_R2.fastq.gz  W403C_R2.fastq.gz\nAh, FASTQ files! These contain our sequence data (the reads from the Illumina sequencer), and we’ll go and explore them in a bit.\n\n\nCombining options and arguments\nWe’ll combine options and arguments to take a closer look at our dir with FASTQ files — now the -h option is especially useful and allows us to see that the FASTQ files are around 2-3 Mb in size:\nls -lh share/data/fastq\ntotal 150M\n-rw-r-----+ 1 jelmer PAS0471 2.0M Mar  1 11:24 NW102AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 11:24 NW102AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 11:24 NW102C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.0M Mar  1 11:24 NW102C_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 1.9M Mar  1 11:24 NW103AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 11:24 NW103AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 11:24 NW103C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.1M Mar  1 11:24 NW103C_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 1.9M Mar  1 11:24 NW201AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.5M Mar  1 11:24 NW201AB_R2.fastq.gz\n# [...output truncated...]\n\n\n\n\n\n\nWhy so small?\n\n\n\nThe FASTQ files are so small because we’ve “subsampled” them: these only contain 10% of the reads of the original files. This will allow us to do the demonstrational analyses in the workshops more rapidly.\n\n\n\n\n\n Exercise: Listing files\nList the files in the share/data/ref dir:\n\nWhat is the file size?\nDo you know what kind of file this is?\n\n\n\nClick for the solution\n\nls -lh share/data/ref\ntotal 131M\n-rwxr--r-- 1 jelmer PAS2714 131M Feb 27 11:53 silva_nr99_v138.1_train_set.fa.gz\n\nThe file is 131 Mb large.\nThis is a FASTA file with nucleotide sequences (hence the extension .fa), which has been compressed (hence the extension .gz).\n\n\n\n\n\n\n3.5 Miscellaneous tips\n\nCommand history: If you hit the ⇧ (up arrow) once, you’ll retrieve your most recent command, and if you keep hitting it, you’ll go further back. The ⇩ (down arrow) will go the other way: towards the present.\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nTab completion: file paths can Tab-complete! Try to type a partial path and test it. If you’re not getting it to work, it might be worth Googling this feature and watching a demo video.\nAny text that comes after a # is considered a comment instead of code!\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/fs/ess/PAS2714\n\n\n\nIf your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort in either of these scenarios, press Ctrl+C and you will get your prompt back.\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n("
  },
  {
    "objectID": "homework/shell.html#paths-and-environment-variables",
    "href": "homework/shell.html#paths-and-environment-variables",
    "title": "Homework: Intro to the Unix Shell",
    "section": "4 Paths and environment variables",
    "text": "4 Paths and environment variables\n\n4.1 Paths\n\nAbsolute (full) paths versus relative paths\n\nAbsolute (full) paths (e.g. /fs/ess/PAS2714)\nPaths that begin with a / always start from the computer’s root directory, and are called “absolute paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nRelative paths (e.g. data/fastq)\nPaths that instead start from your current working directory are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\n# Move into the 'PAS2714' dir with an absolute path:\ncd /fs/ess/PAS2714\n\n# Then, move into the 'share/data' dir with a relative path:\ncd share/data                   # Absolute path is /fs/ess/PAS2714/share/data\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory.\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\n# (You should be in /fs/ess/PAS2714/share/data)\nls ..              # One level up, listing /fs/ess/PAS2714/share\ndata  README.md  results\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\nls ../..            # Two levels up, listing /fs/ess/PAS2714\nsandbox  share  users\n\n\n\n\n\n\nThese shortcuts work with all commands\n\n\n\nAll of the above shortcuts (., .., ~) are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise: Path shortcuts\n\nA) Use relative paths to move up to /fs/ess/PAS2714 and back to share/data once again.\n\n\n\n(Click for the solution)\n\ncd ../..\ncd share/data\n\n\nB) List the files in your Home dir without moving there.\n\n\n\n(Click for the solution)\n\nls ~\n# (Output not shown, will vary from person to person)\n\n\n\n\n\n4.2 Environment variables\nYou are likely familiar with the concept of variables in either the Unix shell, R, or another language.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell4.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to the Unix shell\"\nWelcome to the Unix shell\n\n\n\nEnvironment variables are pre-existing variables that have been assigned values automatically. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nB) Print “Hello there, &lt;your username&gt;” (e.g. “Hello there, marcus”) to the screen:\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello there $USER\"\nHello there jelmer"
  },
  {
    "objectID": "homework/shell.html#managing-files-and-dirs",
    "href": "homework/shell.html#managing-files-and-dirs",
    "title": "Homework: Intro to the Unix Shell",
    "section": "5 Managing files and dirs",
    "text": "5 Managing files and dirs\n\n5.1 Create dirs with mkdir\nThe mkdir command creates new directories. For example, to create your own dir within /fs/ess/PAS2714:\ncd /fs/ess/PAS2714/users\n\nmkdir $USER\nLet’s move into our newly created dir and create two directories at once:\ncd $USER\n\nmkdir scripts sandbox\nLet’s check what we did:\nls\nsandbox  scripts\n\n\n\n\n\n\n\nConfused by $USER?\n\n\n\nInstead of $USER, you can also type your literal username. If you do that, make sure that you get your username exactly right, including any capitalization. For example, I (username jelmer) could have run the following commands instead of the ones above with $USER:\nmkdir jelmer\ncd jelmer\n\n\n\n\n\n\n\n\nSide note: Recursive mkdir with -p (Click to expand)\n\n\n\n\n\nBy default, mkdir does not work recursively: that is, it will refuse to make a dir inside a dir that does not yet exist. And if you try to do so, the resulting error might confuse you:\nmkdir sandbox/2024/02/07\nmkdir: cannot create directory ‘sandbox/2024/02/07’: No such file or directory\n\nWhy won’t you do your job, mkdir!? 😡\n\nInstead, we need to use the -p option to mkdir:\nmkdir -p sandbox/2024/02/07\nThe -p option also changes mkdir’s behavior when you try to create a dir that already exists. Without -p that will result in an error, and with -p it doesn’t complain about that (and it won’t recreate/overwrite the dir either).\n\n\n\n\n\n\n5.2 Copy files and dirs with cp\nAbove, you created your own directory — now, let’s get you a copy of the data we saw in the data dir.\nThe cp command copies files and/or directories from one location to another. It has two required arguments: what you want to copy (the source), and where you want to copy it to (the destination). We can summarize its basic syntax as cp &lt;source&gt; &lt;destination&gt;.\nLet’s start by copying a single file twice:\n# You should be in /fs/ess/PAS2714/users/$USER/\n\n# Only provide a dir as the destination =&gt; Don't change the file name:\ncp /fs/ess/PAS2714/sandbox/testfile.txt sandbox/\n\n# Provide a file name as the destination =&gt; Give the copy a new name:\ncp /fs/ess/PAS2714/sandbox/testfile.txt sandbox/testfile_mycopy.txt\n\n# Check the files we created:\nls sandbox\ntestfile_mycopy.txt  testfile.txt\n\ncp is not recursive by default, so if you want to copy a directory and all of its contents, you need to use its -r option. We’ll use that option to copy the dir with FASTQ files:\ncp -rv /fs/ess/PAS2714/share/data /fs/ess/PAS2714/users/$USER/\n‘/fs/ess/PAS2714/share/data’ -&gt; ‘./data’\n‘/fs/ess/PAS2714/share/data/meta’ -&gt; ‘./data/meta’\n‘/fs/ess/PAS2714/share/data/meta/meta.tsv’ -&gt; ‘./data/meta/meta.tsv’\n‘/fs/ess/PAS2714/share/data/ref’ -&gt; ‘./data/ref’\n‘/fs/ess/PAS2714/share/data/ref/silva_nr99_v138.1_train_set.fa.gz’ -&gt; ‘./data/ref/silva_nr99_v138.1_train_set.fa.gz’\n‘/fs/ess/PAS2714/share/data/fastq’ -&gt; ‘./data/fastq’\n‘/fs/ess/PAS2714/share/data/fastq/W404A_R2.fastq.gz’ -&gt; ‘./data/fastq/W404A_R2.fastq.gz’\n‘/fs/ess/PAS2714/share/data/fastq/NW203A_R2.fastq.gz’ -&gt; ‘./data/fastq/NW203A_R2.fastq.gz’\n‘/fs/ess/PAS2714/share/data/fastq/W205BC_R2.fastq.gz’ -&gt; ‘./data/fastq/W205BC_R2.fastq.gz’\n# [...output truncated...]\n\n\n\n\n\n\nAbove we also used the -v option, short for verbose, to make cp tell us what it did\n\n\n\n\n\n\nWe can also get a nice recursive overview of all our files with tree:\ntree -C                 # '-C' for colors, not visible on this site though\n.\n├── data\n│   ├── fastq\n│   │   ├── NW102AB_R1.fastq.gz\n│   │   ├── NW102AB_R2.fastq.gz\n│   │   ├── NW102C_R1.fastq.gz\n│   │   ├── NW102C_R2.fastq.gz\n│   │   ├── NW103AB_R1.fastq.gz\n│   │   ├── NW103AB_R2.fastq.gz\n        ├── [...Other FASTQ files not shown...]\n│   ├── meta\n│   │   └── meta.tsv\n│   └── ref\n│       └── silva_nr99_v138.1_train_set.fa.gz\n├── sandbox\n│   ├── testfile_mycopy.txt\n│   └── testfile.txt\n└── scripts\n\n\n\n5.3 Move with mv, and cp/mv tips\nThe mv command is nearly identical to the cp command, except that it:\n\nMoves rather than copies files and/or dirs\nWorks recursively by default\n\nThere is no separate renaming command, as both cp and mv allow you to provide a different name for the target.\nLet’s start by moving the testfile.txt into our current working dir:\nmv sandbox/testfile.txt .\nAnd we can move and rename at the same time as well — let’s do that to move testfile.txt back and give it a new name at once:\nmv testfile.txt sandbox/testfile_v2.txt\n\n\n\n\n\n\nOverwriting\n\n\n\nBy default, both mv and cp will overwrite files without warning! Use the -i (forinteractive) option to make it let you confirm before overwriting anything.\n\n\n\n\n\n\n\n\nRenaming rules for both cp and mv — if the destination is:\n\n\n\n\nAn existing dir, the file(s) will keep their original names.\nNot an existing dir, the path specifies the new name of the file or dir, depending on what the source is.\n\n\n\n\nExercise: Practice with mv\nIn which directory (in terms of a relative path from your working dir) would the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files\nmv data/fastq fastq\nmv data/fastq .\n\nWhat if you wanted to move the FASTQ files directly into your current working directory (from data/fastq)?\n\n\n\nSolutions (click here)\n\nIn which directory (in terms of relative path from your working dir) will the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files — in the dir fastq_files (we’ve really just renamed the dir fastq to fastq_files)\nmv data/fastq fastq — in fastq (because our source is a dir, so is the destination)\nmv data/fastq . — in fastq also! (we’d need the syntax shown below to move the individual files directly into our current dir)\n\nWhat if you wanted to move the FASTQ files directly into your current working directory?\nFor one file:\nmv data/fastq/ASPC1_A178V_R1.fastq.gz .\nFor all files:\nmv data/fastq/* .\n\n\n\n\n\n5.4 Remove files with rm\nThe rm command removes (deletes) files and directories.\nOne important thing to note upfront is that rm will permanently and irreversibly delete files without the typical “intermediate step” of placing them in a trash bin, like you are used to with your personal computer. With a healthy dosis of fear installed, let’s dive in.\nTo remove one or more files, you can simply pass the file names as arguments to rm as with previous commands. We will also use the -v (verbose) option to have it tell us what it did:\nrm -v sandbox/testfile_v2.txt\nremoved sandbox/testfile_v2.txt\n\n\nRecursive rm\nAs a safety measure, rm will by default only delete files and not directories or their contents — i.e., like mkdir and cp, it refuses to act recursively by default. To remove dirs and their contents, use the -r option:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‘d1’: Is a directory\n# But it does work with the '-r' option:\nrm -rv d1\nremoved directory: ‘d1/d2/d3’\nremoved directory: ‘d1/d2’\nremoved directory: ‘d1’\nYou should obviously be quite careful with rm -r!\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error."
  },
  {
    "objectID": "homework/shell.html#globbing-and-loops",
    "href": "homework/shell.html#globbing-and-loops",
    "title": "Homework: Intro to the Unix Shell",
    "section": "6 Globbing and loops",
    "text": "6 Globbing and loops\n\n6.1 Globbing with shell wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing. Wildcards are symbols that have a special meaning.\nIn globbing, the * wildcard matches any number of any character, including nothing.\nThe example below will match any files that contain the string “_R1”:\n# (You should still be in /fs/ess/PAS2714/users/$USER)\nls data/fastq/*_R1*\ndata/fastq/NW102AB_R1.fastq.gz  data/fastq/NW201C_R1.fastq.gz   data/fastq/NW305AB_R1.fastq.gz  data/fastq/NW404BC_R1.fastq.gz  data/fastq/W204A_R1.fastq.gz   data/fastq/W303C_R1.fastq.gz   data/fastq/W404A_R1.fastq.gz\ndata/fastq/NW102C_R1.fastq.gz   data/fastq/NW203A_R1.fastq.gz   data/fastq/NW305C_R1.fastq.gz   data/fastq/W101AB_R1.fastq.gz   data/fastq/W204BC_R1.fastq.gz  data/fastq/W304AB_R1.fastq.gz  data/fastq/W404BC_R1.fastq.gz\ndata/fastq/NW103AB_R1.fastq.gz  data/fastq/NW203BC_R1.fastq.gz  data/fastq/NW403A_R1.fastq.gz   data/fastq/W101C_R1.fastq.gz    data/fastq/W205A_R1.fastq.gz   data/fastq/W304C_R1.fastq.gz\ndata/fastq/NW103C_R1.fastq.gz   data/fastq/NW304A_R1.fastq.gz   data/fastq/NW403BC_R1.fastq.gz  data/fastq/W103AB_R1.fastq.gz   data/fastq/W205BC_R1.fastq.gz  data/fastq/W403AB_R1.fastq.gz\ndata/fastq/NW201AB_R1.fastq.gz  data/fastq/NW304BC_R1.fastq.gz  data/fastq/NW404A_R1.fastq.gz   data/fastq/W103C_R1.fastq.gz    data/fastq/W303AB_R1.fastq.gz  data/fastq/W403C_R1.fastq.gz\nSome more file matching examples with * — if you would be in your data/fastq dir, then:\n\n\n\nPattern\nMatches files whose names…\n\n\n\n\n*\nContain anything (matches all files)\n\n\n*fastq.gz\nEnd in “.fastq.gz”\n\n\nNW1*\nStart with “NW1”\n\n\n*_R1*\nContain “_R1”\n\n\n\n\n\n Exercise: Practice with *\nWhat pattern would you use if you wanted to select FASTQ files for the samples whose IDs end in AB (e.g. NW102AB)?\n\n\nClick here for the solutions\n\nWe’ll need a * on either side of our pattern, because the file names neither start not end with the pattern:\nls data/fastq/*AB_*\ndata/fastq/NW102AB_R1.fastq.gz  data/fastq/NW103AB_R2.fastq.gz  data/fastq/NW305AB_R1.fastq.gz  data/fastq/W101AB_R2.fastq.gz  data/fastq/W303AB_R1.fastq.gz  data/fastq/W304AB_R2.fastq.gz\ndata/fastq/NW102AB_R2.fastq.gz  data/fastq/NW201AB_R1.fastq.gz  data/fastq/NW305AB_R2.fastq.gz  data/fastq/W103AB_R1.fastq.gz  data/fastq/W303AB_R2.fastq.gz  data/fastq/W403AB_R1.fastq.gz\ndata/fastq/NW103AB_R1.fastq.gz  data/fastq/NW201AB_R2.fastq.gz  data/fastq/W101AB_R1.fastq.gz   data/fastq/W103AB_R2.fastq.gz  data/fastq/W304AB_R1.fastq.gz  data/fastq/W403AB_R2.fastq.gz\n\n\n\n\n\n6.2 For loops\nLoops are a universal element of programming languages, and are used to repeat operations. Here, we’ll only cover the most common type of loop: the for loop.\nA for loop iterates over a collection, such as a list of files, and allows you to perform one or more actions for each element in the collection. In the example below, our “collection” is just a short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\n\n\n\n\n\n\nWhat was actually run under the hood is the following:\n\n\n\n# (Don't run this)\na_number=1\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=2\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=3\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\n\nHere are two key things to understand about for loops:\n\nIn each iteration of the loop, one element in the collection is being assigned to the variable specified after for. In the example above, we used a_number as the variable name, so that variable contained 1 when the loop ran for the first time, 2 when it ran for the second time, and 3 when it ran for the third and last time.\nThe loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\n\n\n\n\n\n\n\nA further explanation of for loop syntax (Click to expand)\n\n\n\n\n\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\nCombining loops and globbing\nA very useful strategy is to loop over files with globbing, for example:\nfor fastq_file in data/fastq/*fastq.gz; do\n    echo \"Running an analysis for file $fastq_file\"...\n    # Additional commands to process the FASTQ file\ndone\nRunning an analysis for file data/fastq/NW102AB_R1.fastq.gz...\nRunning an analysis for file data/fastq/NW102AB_R2.fastq.gz...\nRunning an analysis for file data/fastq/NW102C_R1.fastq.gz...\nRunning an analysis for file data/fastq/NW102C_R2.fastq.gz...\nRunning an analysis for file data/fastq/NW103AB_R1.fastq.gz...\nRunning an analysis for file data/fastq/NW103AB_R2.fastq.gz...\nRunning an analysis for file data/fastq/NW103C_R1.fastq.gz...\n#[...output truncated...]\n\n\n\n Exercise: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\nClick for the solution\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom"
  },
  {
    "objectID": "homework/shell.html#footnotes",
    "href": "homework/shell.html#footnotes",
    "title": "Homework: Intro to the Unix Shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)↩︎\nIt’s certainly possible to have spaces in file names, but it’s a bad idea, and will get you into trouble sooner or later.↩︎\nBeginners will often cd into a dir just to list its contents, but the method shown below is much quicker.↩︎\nAnytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎"
  },
  {
    "objectID": "09_core.html",
    "href": "09_core.html",
    "title": "Core-Microbiome",
    "section": "",
    "text": "THIS PAGE IS STILL UNDER CONSTRUCTION\n#Library needed\nlibrary(\"phyloseq\")\nlibrary(\"ggplot2\")\nlibrary(\"microbiome\")\n\n\nmicrobiome R package (microbiome.github.com)\n    \n\n\n Copyright (C) 2011-2022 Leo Lahti, \n    Sudarshan Shetty et al. &lt;microbiome.github.io&gt;\n\n\n\nAttaching package: 'microbiome'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\nThe following object is masked from 'package:base':\n\n    transform\n\nlibrary(\"ggvenn\")\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: grid"
  },
  {
    "objectID": "09_core.html#defining-a-core-microbiome",
    "href": "09_core.html#defining-a-core-microbiome",
    "title": "Core-Microbiome",
    "section": "Defining a Core Microbiome",
    "text": "Defining a Core Microbiome\nLet’s start defining what is considered a core microbiome. A core microbiome comprises the members shared among two or more microbial assemblages associated with a habitat.\nThere are several approaches to define a core microbiome1:\n\nMembership (shared taxa)\nComposition (relative abundance)\nPhylogenetic and functional redundancy\nPersistence (temporal and spatial)\nConnectivity (co-variation within and shared communities)"
  },
  {
    "objectID": "09_core.html#we-will-learn-two-approaches",
    "href": "09_core.html#we-will-learn-two-approaches",
    "title": "Core-Microbiome",
    "section": "1 We will learn two approaches:",
    "text": "1 We will learn two approaches:\nStart reading the phyloseq object containing your data:\n\nps.dataset &lt;- readRDS(\"results/ps_fulldata/bac22rot_w_ASV.rds\")\n\nTake a look of your metadata and the variables you want to analyse. We will focus on Rotation.\n\nsample_data(ps.dataset)\n\n           Year Location Rotation  Plot TRT Block Sampling.Date\nT22NW102AB 2022    NWARS       CS 102AB   2   100    June, 2022\nT22NW102C  2022    NWARS       CS  102C   7   100    June, 2022\nT22NW103AB 2022    NWARS      CSW 103AB   3   100    June, 2022\nT22NW103C  2022    NWARS      CSW  103C   8   100    June, 2022\nT22NW201AB 2022    NWARS      CSW 201AB   3   200    June, 2022\nT22NW201C  2022    NWARS      CSW  201C   8   200    June, 2022\nT22NW203A  2022    NWARS       CS  203A   7   200    June, 2022\nT22NW203BC 2022    NWARS       CS 203BC   2   200    June, 2022\nT22NW305AB 2022    NWARS       CS 305AB   2   300    June, 2022\nT22NW305C  2022    NWARS       CS  305C   7   300    June, 2022\nT22NW403A  2022    NWARS       CS  403A   7   400    June, 2022\nT22NW403BC 2022    NWARS       CS 403BC   2   400    June, 2022\nT22NW404A  2022    NWARS      CSW  404A   8   400    June, 2022\nT22NW404BC 2022    NWARS      CSW 404BC   3   400    June, 2022\nT22W101AB  2022     WARS       CS 101AB   2   100    June, 2022\nT22W101C   2022     WARS       CS  101C   7   100    June, 2022\nT22W103AB  2022     WARS      CSW 103AB   3   100    June, 2022\nT22W103C   2022     WARS      CSW  103C   8   100    June, 2022\nT22W204A   2022     WARS      CSW  204A   8   200    June, 2022\nT22W204BC  2022     WARS      CSW 204BC   3   200    June, 2022\nT22W205A   2022     WARS       CS  205A   7   200    June, 2022\nT22W205BC  2022     WARS       CS 205BC   2   200    June, 2022\nT22W303AB  2022     WARS      CSW 303AB   3   300    June, 2022\nT22W303C   2022     WARS      CSW  303C   8   300    June, 2022\nT22W304AB  2022     WARS       CS 304AB   2   300    June, 2022\nT22W304C   2022     WARS       CS  304C   7   300    June, 2022\nT22W403AB  2022     WARS       CS 403AB   2   400    June, 2022\nT22W403C   2022     WARS       CS  403C   7   400    June, 2022\nT22W404A   2022     WARS      CSW  404A   8   400    June, 2022\nT22W404BC  2022     WARS      CSW 404BC   3   400    June, 2022\n\n\n\n1.1 Composition (relative abundance)\n\nTransform the ASV count table to compositional data (relative abundance). For that we will use the R package microbiome (previously installed) with the function transform.\n\n\nps.dataset.cm &lt;- microbiome::transform(ps.dataset, \"compositional\")\n\n\nDetermine the core taxa by the variable of interest (Rotation).For this you will subset the data by Rotation variable.\n\n\nps.CS &lt;- subset_samples(ps.dataset.cm, Rotation == \"CS\")\nps.CSW &lt;- subset_samples(ps.dataset.cm, Rotation == \"CSW\")\n\nThen, get the core taxa using the function core_members from the R package microbiome.\n\ncore.taxa.CS &lt;- core_members(ps.CS, detection = 0.001, prevalence = 95/100)\ncore.taxa.CSW &lt;- core_members(ps.CSW, detection = 0.001, prevalence = 95/100)\n\n\n\n\n\n\n\nHow to set detection and prevalence threasholds\n\n\n\nDetection (limit of detection) will be the minimum relative abundance value at which taxa will be selected. This can be set by the user or determined empirically based on sequencing depth.\nPrevalence is the proportion of samples that will be selected given a detection threshold.\n\n\n\n\nExample (click here)\n\nConsider this mock data for relative abundance of Pseudomonas fluorescens in 10 soil microbiome samples:\n\n\n\nSample\nRelative abundance (%)\n\n\n\n\n1\n0.002\n\n\n3\n0.49\n\n\n4\n7.8\n\n\n5\n12.1\n\n\n6\n0.52\n\n\n7\n0.2\n\n\n8\n0.06\n\n\n9\n9.1\n\n\n10\n0.08\n\n\n\nBased on detection values, the prevalence will change. Lower detection threshold equals higher prevalence:\n\n\n\nDetection\nPrevalence\n\n\n\n\n1\n3/10\n\n\n0.1\n6/10\n\n\n0.01\n9/10\n\n\n\n\n\nCreate a list with the selected core taxa for both subsets\n\n\ncores.list &lt;- list(CS = core.taxa.CS, CSW = core.taxa.CSW)\ncores.list #To see the list of core taxa by Rotation\n\n$CS\n [1] \"ASV_1372\" \"ASV_1425\" \"ASV_1426\" \"ASV_1723\" \"ASV_1737\" \"ASV_2316\"\n [7] \"ASV_2440\" \"ASV_2450\" \"ASV_2483\" \"ASV_2545\" \"ASV_2605\" \"ASV_2635\"\n[13] \"ASV_2825\" \"ASV_3076\" \"ASV_3576\" \"ASV_3587\" \"ASV_3592\" \"ASV_3986\"\n[19] \"ASV_4073\" \"ASV_4082\" \"ASV_4242\" \"ASV_4348\" \"ASV_4391\" \"ASV_4478\"\n[25] \"ASV_4504\" \"ASV_4555\" \"ASV_4565\" \"ASV_4642\" \"ASV_5779\" \"ASV_5791\"\n[31] \"ASV_6609\" \"ASV_7160\" \"ASV_7731\" \"ASV_7736\" \"ASV_7822\" \"ASV_7847\"\n[37] \"ASV_7848\" \"ASV_8079\"\n\n$CSW\n [1] \"ASV_1367\" \"ASV_1372\" \"ASV_1388\" \"ASV_1425\" \"ASV_1426\" \"ASV_1723\"\n [7] \"ASV_1737\" \"ASV_1863\" \"ASV_2316\" \"ASV_2440\" \"ASV_2450\" \"ASV_2483\"\n[13] \"ASV_2545\" \"ASV_2605\" \"ASV_2619\" \"ASV_2635\" \"ASV_2825\" \"ASV_2896\"\n[19] \"ASV_3076\" \"ASV_3576\" \"ASV_3592\" \"ASV_3898\" \"ASV_3962\" \"ASV_3986\"\n[25] \"ASV_4059\" \"ASV_4073\" \"ASV_4082\" \"ASV_4242\" \"ASV_4391\" \"ASV_4477\"\n[31] \"ASV_4478\" \"ASV_4504\" \"ASV_4521\" \"ASV_4565\" \"ASV_4642\" \"ASV_4654\"\n[37] \"ASV_4655\" \"ASV_5760\" \"ASV_5791\" \"ASV_6609\" \"ASV_6960\" \"ASV_7160\"\n[43] \"ASV_7349\" \"ASV_7731\" \"ASV_7736\" \"ASV_7822\" \"ASV_7848\" \"ASV_7858\"\n[49] \"ASV_8079\"\n\n\nGet the list of shared taxa from the taxa table of your dataset\n\n\n          Kingdom            Phylum               Class               Order\nASV_1372 Bacteria    Proteobacteria Alphaproteobacteria         Rhizobiales\nASV_1425 Bacteria    Proteobacteria Alphaproteobacteria         Rhizobiales\nASV_1426 Bacteria    Proteobacteria Alphaproteobacteria         Rhizobiales\nASV_1723 Bacteria    Proteobacteria Alphaproteobacteria    Sphingomonadales\nASV_1737 Bacteria    Proteobacteria Alphaproteobacteria    Sphingomonadales\nASV_2316 Bacteria    Proteobacteria Gammaproteobacteria     Xanthomonadales\nASV_2440 Bacteria    Proteobacteria Gammaproteobacteria     Burkholderiales\nASV_2450 Bacteria    Proteobacteria Gammaproteobacteria     Burkholderiales\nASV_2483 Bacteria    Proteobacteria Gammaproteobacteria     Burkholderiales\nASV_2545 Bacteria    Proteobacteria Gammaproteobacteria     Burkholderiales\nASV_2605 Bacteria    Proteobacteria Gammaproteobacteria     Burkholderiales\nASV_2635 Bacteria    Proteobacteria Gammaproteobacteria     Burkholderiales\nASV_2825 Bacteria       Myxococcota          Myxococcia        Myxococcales\nASV_3076 Bacteria       Myxococcota           Polyangia        Polyangiales\nASV_3576 Bacteria        Firmicutes             Bacilli          Bacillales\nASV_3592 Bacteria        Firmicutes             Bacilli          Bacillales\nASV_3986 Bacteria  Actinobacteriota     Thermoleophilia Solirubrobacterales\nASV_4073 Bacteria  Actinobacteriota     Thermoleophilia          Gaiellales\nASV_4082 Bacteria  Actinobacteriota     Thermoleophilia          Gaiellales\nASV_4242 Bacteria  Actinobacteriota      Actinobacteria          Frankiales\nASV_4391 Bacteria  Actinobacteriota      Actinobacteria   Micromonosporales\nASV_4478 Bacteria  Actinobacteriota      Actinobacteria       Micrococcales\nASV_4504 Bacteria  Actinobacteriota      Actinobacteria       Micrococcales\nASV_4565 Bacteria  Actinobacteriota      Actinobacteria   Pseudonocardiales\nASV_4642 Bacteria  Actinobacteriota      Actinobacteria Propionibacteriales\nASV_5791 Bacteria   Acidobacteriota      Acidobacteriae    Acidobacteriales\nASV_6609 Bacteria   Planctomycetota       Phycisphaerae    Tepidisphaerales\nASV_7160 Bacteria Verrucomicrobiota    Verrucomicrobiae  Chthoniobacterales\nASV_7731 Bacteria       Chloroflexi              KD4-96                &lt;NA&gt;\nASV_7736 Bacteria       Chloroflexi              KD4-96                &lt;NA&gt;\nASV_7822 Bacteria   Gemmatimonadota    Gemmatimonadetes    Gemmatimonadales\nASV_7848 Bacteria   Gemmatimonadota    Gemmatimonadetes    Gemmatimonadales\nASV_8079 Bacteria Methylomirabilota    Methylomirabilia     Rokubacteriales\n                       Family                  Genus       Species\nASV_1372    Xanthobacteraceae           Pseudolabrys          &lt;NA&gt;\nASV_1425    Xanthobacteraceae         Bradyrhizobium       elkanii\nASV_1426    Xanthobacteraceae         Bradyrhizobium     japonicum\nASV_1723    Sphingomonadaceae           Sphingomonas          &lt;NA&gt;\nASV_1737    Sphingomonadaceae           Sphingomonas daechungensis\nASV_2316     Xanthomonadaceae             Arenimonas          &lt;NA&gt;\nASV_2440              SC-I-84                   &lt;NA&gt;          &lt;NA&gt;\nASV_2450              SC-I-84                   &lt;NA&gt;          &lt;NA&gt;\nASV_2483              TRA3-20                   &lt;NA&gt;          &lt;NA&gt;\nASV_2545    Nitrosomonadaceae                 mle1-7          &lt;NA&gt;\nASV_2605       Comamonadaceae          Piscinibacter          &lt;NA&gt;\nASV_2635       Comamonadaceae          Piscinibacter          &lt;NA&gt;\nASV_2825        Myxococcaceae             Archangium       gephyra\nASV_3076        Polyangiaceae          Aetherobacter          &lt;NA&gt;\nASV_3576          Bacillaceae               Bacillus     circulans\nASV_3592          Bacillaceae               Bacillus          &lt;NA&gt;\nASV_3986 Solirubrobacteraceae        Solirubrobacter          &lt;NA&gt;\nASV_4073                 &lt;NA&gt;                   &lt;NA&gt;          &lt;NA&gt;\nASV_4082          Gaiellaceae                Gaiella          &lt;NA&gt;\nASV_4242      Acidothermaceae           Acidothermus          &lt;NA&gt;\nASV_4391   Micromonosporaceae                   &lt;NA&gt;          &lt;NA&gt;\nASV_4478    Cellulomonadaceae           Cellulomonas          &lt;NA&gt;\nASV_4504   Intrasporangiaceae             Oryzihumus        terrae\nASV_4565   Pseudonocardiaceae         Pseudonocardia          &lt;NA&gt;\nASV_4642      Nocardioidaceae           Nocardioides          &lt;NA&gt;\nASV_5791                 &lt;NA&gt;                   &lt;NA&gt;          &lt;NA&gt;\nASV_6609    WD2101 soil group                   &lt;NA&gt;          &lt;NA&gt;\nASV_7160  Chthoniobacteraceae Candidatus Udaeobacter          &lt;NA&gt;\nASV_7731                 &lt;NA&gt;                   &lt;NA&gt;          &lt;NA&gt;\nASV_7736                 &lt;NA&gt;                   &lt;NA&gt;          &lt;NA&gt;\nASV_7822    Gemmatimonadaceae           Gemmatimonas          &lt;NA&gt;\nASV_7848    Gemmatimonadaceae           Gemmatimonas          &lt;NA&gt;\nASV_8079                 &lt;NA&gt;                   &lt;NA&gt;          &lt;NA&gt;\n         Species_exact confidence\nASV_1372          &lt;NA&gt;       0.80\nASV_1425          &lt;NA&gt;       0.95\nASV_1426          &lt;NA&gt;       0.57\nASV_1723          &lt;NA&gt;       0.99\nASV_1737 daechungensis       0.81\nASV_2316          &lt;NA&gt;       1.00\nASV_2440          &lt;NA&gt;       1.00\nASV_2450          &lt;NA&gt;       1.00\nASV_2483          &lt;NA&gt;       0.99\nASV_2545          &lt;NA&gt;       0.99\nASV_2605          &lt;NA&gt;       0.90\nASV_2635          &lt;NA&gt;       0.82\nASV_2825          &lt;NA&gt;       0.65\nASV_3076         rufus       0.94\nASV_3576          &lt;NA&gt;       0.63\nASV_3592          &lt;NA&gt;       0.73\nASV_3986          &lt;NA&gt;       1.00\nASV_4073          &lt;NA&gt;       1.00\nASV_4082          &lt;NA&gt;       1.00\nASV_4242          &lt;NA&gt;       1.00\nASV_4391          &lt;NA&gt;       1.00\nASV_4478          &lt;NA&gt;       0.97\nASV_4504          &lt;NA&gt;       0.57\nASV_4565          &lt;NA&gt;       0.95\nASV_4642          &lt;NA&gt;       1.00\nASV_5791          &lt;NA&gt;       1.00\nASV_6609          &lt;NA&gt;       1.00\nASV_7160          &lt;NA&gt;       1.00\nASV_7731          &lt;NA&gt;       1.00\nASV_7736          &lt;NA&gt;       1.00\nASV_7822          &lt;NA&gt;       0.99\nASV_7848          &lt;NA&gt;       1.00\nASV_8079          &lt;NA&gt;       1.00\n\n\nGet the list of unique taxa in Rotation CS\n\n\n          Kingdom           Phylum            Class             Order\nASV_3587 Bacteria       Firmicutes          Bacilli        Bacillales\nASV_4348 Bacteria Actinobacteriota   Actinobacteria Micromonosporales\nASV_4555 Bacteria Actinobacteriota   Actinobacteria Pseudonocardiales\nASV_5779 Bacteria  Acidobacteriota   Acidobacteriae  Acidobacteriales\nASV_7847 Bacteria  Gemmatimonadota Gemmatimonadetes  Gemmatimonadales\n                     Family         Genus      Species Species_exact confidence\nASV_3587        Bacillaceae      Bacillus  aryabhattai          &lt;NA&gt;       0.96\nASV_4348 Micromonosporaceae          &lt;NA&gt;         &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_4555 Pseudonocardiaceae Amycolatopsis mediterranei          &lt;NA&gt;       1.00\nASV_5779               &lt;NA&gt;          &lt;NA&gt;         &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_7847  Gemmatimonadaceae  Gemmatimonas         &lt;NA&gt;          &lt;NA&gt;       1.00\n\n\nGet the list of unique taxa in Rotation CSW\n\n\n          Kingdom            Phylum               Class               Order\nASV_1367 Bacteria    Proteobacteria Alphaproteobacteria         Rhizobiales\nASV_1388 Bacteria    Proteobacteria Alphaproteobacteria         Rhizobiales\nASV_1863 Bacteria    Proteobacteria Alphaproteobacteria     Acetobacterales\nASV_2619 Bacteria    Proteobacteria Gammaproteobacteria     Burkholderiales\nASV_2896 Bacteria       Myxococcota           Polyangia        Polyangiales\nASV_3898 Bacteria  Actinobacteriota     Thermoleophilia Solirubrobacterales\nASV_3962 Bacteria  Actinobacteriota     Thermoleophilia Solirubrobacterales\nASV_4059 Bacteria  Actinobacteriota     Thermoleophilia          Gaiellales\nASV_4477 Bacteria  Actinobacteriota      Actinobacteria       Micrococcales\nASV_4521 Bacteria  Actinobacteriota      Actinobacteria       Micrococcales\nASV_4654 Bacteria  Actinobacteriota      Actinobacteria Propionibacteriales\nASV_4655 Bacteria  Actinobacteriota      Actinobacteria Propionibacteriales\nASV_5760 Bacteria   Acidobacteriota      Acidobacteriae    Acidobacteriales\nASV_6960 Bacteria Verrucomicrobiota    Verrucomicrobiae      Pedosphaerales\nASV_7349 Bacteria   Acidobacteriota    Vicinamibacteria  Vicinamibacterales\nASV_7858 Bacteria   Gemmatimonadota    Gemmatimonadetes    Gemmatimonadales\n                    Family          Genus Species Species_exact confidence\nASV_1367 Xanthobacteraceae           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       0.97\nASV_1388 Xanthobacteraceae           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_1863  Acetobacteraceae           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_2619    Comamonadaceae     Acidovorax  avenae          &lt;NA&gt;       1.00\nASV_2896           BIrii41           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_3898             67-14           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_3962             67-14           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       0.96\nASV_4059              &lt;NA&gt;           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_4477 Cellulomonadaceae   Cellulomonas    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_4521    Micrococcaceae   Arthrobacter    &lt;NA&gt;          &lt;NA&gt;       0.82\nASV_4654   Nocardioidaceae      Kribbella    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_4655   Nocardioidaceae      Kribbella    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_5760              &lt;NA&gt;           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_6960   Pedosphaeraceae ADurb.Bin063-1    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_7349              &lt;NA&gt;           &lt;NA&gt;    &lt;NA&gt;          &lt;NA&gt;       1.00\nASV_7858 Gemmatimonadaceae   Gemmatimonas    &lt;NA&gt;          &lt;NA&gt;       1.00\n\n\n\nPlot a Venn Diagram\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 Membership and Composition\nThis approach takes into account presence/absence and relative abundance to select shared taxa. The threshold could arbitrary be set but here we will follow the 95th percentile approach[^2]. That means that we will assign core membership to taxa that fell within the 95th percentile of the ranked frequency and ranked relative abundance within the whole data.\n\n\n\n\n\n\nSide note: 95th Percentile approach. (Click to expand)\n\n\n\n\n\nDiagram of how the 95th percentile frequency and abundance core taxa were identified. The distribution of total relative abundance and frequency of taxa is assessed and the those ranked within the 95th percentile (top 5%) are selected as core.\n\n\n\n\n\n\n\n\n\nCreate a data frame from the phyloseq object\n\n\ndf &lt;- psmelt(ps.dataset.cm)\n\n\nTake the total Relative Abundance sum of each genus\n\n\ndf.RA &lt;- aggregate(df$Abundance, by = list(Genus = df$Genus), FUN=sum) \ndf.RA &lt;- dplyr::rename(df.RA, TotalRA = `x`)\n\n\nDetermining 95th quantile of the RA data\n\n\n\n      95% \n0.1663236 \n\n\n\nSelecting for genera that fit the 95th cutoff\n\n\ntaxa.95 &lt;- subset(df.RA, TotalRA &gt;= quantile.95.abund) \ntaxa.95.genus &lt;- taxa.95$Genus\n\n\nDetermine the frequency 95th cutoff. We go back and use the original phyloseq object and agglomerate the data by Genus tax rank (it could be any tax rank you want e.g., Phylum, Family)\n\n\nCreate a table of presence/abundance of ASVs. We will use 1 if present, 0 if not.\n\n\ndf.PA &lt;- 1*((df.ASVs.genus&gt;0)==1)\nOccupancy &lt;- rowSums(df.PA)/ncol(df.PA) #Calculate occupancy\ndf.Freq &lt;- as.data.frame(cbind(df.PA, Occupancy))\ndf.Freq &lt;- tibble::rownames_to_column(df.Freq, 'Genus')\n\n\nDetermining 95th quantile of the presence/abundance table\n\n\n\n95% \n  1 \n\n\n\nSelect the genera that fit the 95th cutoff\n\n\n#\nFreq.95 &lt;- subset(df.Freq, Occupancy &gt;= quantile.95.freq) #select cutoff \nFreq.95.genus &lt;- Freq.95$Genus\n\nQuantile.95 &lt;- merge.data.frame(Freq.95, taxa.95, by = \"Genus\")  \n\n\n\n\n\n\n\nSide note: You can plot a Venn Diagram to see how many taxa made the cutoffs. (Click to expand)\n\n\n\n\n\nUse the Core.list data:\n\nCore.95.list &lt;- list(Frequency = Freq.95.genus, Relative_abundance = taxa.95.genus)\nplot.VennD &lt;- ggvenn(Core.95.list,\n            stroke_size = 0.5, set_name_size = 3.2, text_size = 3)\n\nprint(plot.VennD)\n\n\n\n\n\n\n\n\n\n\n\n\nGet the list of your core taxa based on the 95th percentile\n\n\nSubset your original phyloseq object for core taxa\n\n\nFix the relative abundance of the core taxa to continue with the plot\n\n\nCreate a bubble plot You can customize your plot using ggplot2 package available options."
  },
  {
    "objectID": "09_core.html#footnotes",
    "href": "09_core.html#footnotes",
    "title": "Core-Microbiome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCantoran et al. 2023 DOI:https://doi.org/10.1093/femsec/fiad098↩︎"
  }
]