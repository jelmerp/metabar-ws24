[
  {
    "objectID": "homework/R.html#why-r",
    "href": "homework/R.html#why-r",
    "title": "Homework: Intro to R",
    "section": "1 Why R?",
    "text": "1 Why R?\nR is a versatile, open source programming/scripting language that‚Äôs particularly useful for statistics and data visualization.\nYes, there is a learning curve, and many of us just want to get on with our analysis ‚Äì\nbut investing in learning R will pay off:\n\nR gives you greater flexibility to do anything you want.\nA greater reproducibility of scripting vs clicking.\nR is highly interdisciplinary ‚Äì e.g.¬†very useful for analyzing sequencing data but can also be used to create maps and perform GIS analyses, and so on!\nR is more than a platform to perform analysis. Combined with Markdown (a simple text markup language), you can use R to produce sophisticated reports, and create slide decks and websites such as this one!\n\nFurthermore, R:\n\nIs freely available on all platforms, and open source.\nHas a large and welcoming user community.\n\n\n\nLearning Objectives\n\nGet some basic familiarity with R and RStudio\nLearn why and how to use RStudio Projects\nUnderstand objects, functions, and how to use them\nUnderstand the concepts of vector and data.frame\nExplore the structure and the content of a data.frame\nLearn to subset/slice vectors and data frames\nLearn about how R handles missing data"
  },
  {
    "objectID": "homework/R.html#getting-set-up",
    "href": "homework/R.html#getting-set-up",
    "title": "Homework: Intro to R",
    "section": "2 Getting set up",
    "text": "2 Getting set up\n\n2.1 Start an RStudio session at OSC\n\nLog in to OSC at https://ondemand.osc.edu.\nClick on Interactive Apps (top bar) and then RStudio Server (all the way at the bottom).\nFill out the form as follows:\n\nCluster: Pitzer\nR version: 4.3.0\nProject: PAS2714\nNumber of hours: 4\nNode type: any\nNumber of cores: 1\n\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick the big blue Launch button at the bottom\nNow, you should be sent to a new page with a box at the top for your RStudio Server ‚Äújob‚Äù, which should initially be ‚ÄúQueued‚Äù (waiting to start).\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nYour job should start running very soon, with the top bar of the box turning green and saying ‚ÄúRunning‚Äù.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick Connect to RStudio Server at the bottom of the box, and an RStudio Server instance will open in a new browser tab. You‚Äôre ready to go!\n\n\n\n\n2.2 R vs.¬†RStudio\nR simply provides a ‚Äúconsole‚Äù to type your commands. However, because we want to save our commands in scripts, examine output such a graphics, and so on, we would like an environment that provides all of this side-by-side.\nWe will use RStudio, an excellent ‚ÄúIntegrated Development Environment‚Äù (IDE) for R. In RStudio, we have a single interface to write code, navigate files on our computer, inspect the objects we create, and visualize plots.\nRStudio is divided into 4 ‚Äúpanes‚Äù1:\n\nTop-left: The editor for your scripts and documents.\nBottom-left: The R console.\nTop-right: Your environment/history.\nBottom-left: Your files/plots/packages/help/viewer.\n\n\n\n\nThe RStudio pane layout"
  },
  {
    "objectID": "homework/R.html#interacting-with-r",
    "href": "homework/R.html#interacting-with-r",
    "title": "Homework: Intro to R",
    "section": "3 Interacting with R",
    "text": "3 Interacting with R\n\n3.1 R as a calculator\nThe lower-left RStudio pane, i.e.¬†the R console, is where you can interact with R directly. The &gt; sign is the R ‚Äúprompt‚Äù. It indicates that R is ready for you to type something.\nLet‚Äôs start by performing a division:\n\n203 / 2.54\n\n[1] 79.92126\n\n\nR does the calculation and prints the result, and then you get the &gt; prompt again. (The [1] may look a bit weird when there is only one output element; this is how you can keep count of output elements.)\nWith the standard symbols you can use R as a general calculator:\n\n203 * 2.54   # Multiplication\n\n[1] 515.62\n\n203 + 2.54   # Addition\n\n[1] 205.54\n\n\n\n\n\n3.2 Trying some random things‚Ä¶\n\n   203     - 2.54\n\n[1] 200.46\n\n\nThis works: so R just ignores any extra spaces. How about:\n\n203 +\n\n\n\n Now the prompt is a +. What is going on? (Click for the answer)\n\nR is waiting for you to finish the command, since you typed an incomplete command: something has to come after the + to be added to what came before.\nWhile it was obvious here, you will often type incomplete commands without realizing you did so. Just remember that when you see the + prompt, you are missing something in your command: often, you‚Äôll have forgotten a closing parenthesis ) or you accidentally opened up an unwanted opening parenthesis (.\n\nPress the Esc button to get your prompt back.\n\nAnd if we just type a number:\n\n203\n\n[1] 203\n\n\nR will print the number back to us! It turns out that the default, implicit action that R will perform on anything you type is to print it back to us (it is calling a function called print() under the hood).\n\nRather than a number, what if we want R to print back to us some text, which in programming lingo is called a ‚Äúcharacter string‚Äù?\n\nFantastic\n\nError in eval(expr, envir, enclos): object 'Fantastic' not found\n\n\n\n\n What seems to be going wrong here? (Click for the answer)\n\nWhenever you type a character string, R expects to find an ‚Äúobject‚Äù with that name. (Or, if you would use parentheses after the string, like string(), it will expect a function.)\nBecause no object called Fantastic exists, R throws an error. To refer to a literal string instead, we need to use quotes (see below).\n\n\nWe can get R to print character strings back to us, and work with them in other ways, as long as we quote the strings:\n\n\"Fantastic\"\n\n[1] \"Fantastic\"\n\n\n\n\"I'm really liking R so far.\"\n\n[1] \"I'm really liking R so far.\"\n\n\nSo, R treats numbers and character strings differently: unlike numbers, character strings need to be quoted. This avoids confusion with objects (we‚Äôll learn about those in a minute) because unquoted character strings are assumed to be objects, and also allows for ‚Äúspecial characters‚Äù like spaces.\n\n\n\n\n\n\nQuote types\n\n\n\nDouble quotes (\"Fantastic\") and single quotes ('Fantastic') can be used interchangeably in R. Double quotes are preferred by most ‚Äústyle guides‚Äù."
  },
  {
    "objectID": "homework/R.html#getting-organized",
    "href": "homework/R.html#getting-organized",
    "title": "Homework: Intro to R",
    "section": "4 Getting Organized",
    "text": "4 Getting Organized\n\n4.1 Need for Scripts and RStudio Projects\nWe can go along like this, typing commands directly into the R console. But to better keep track of what you‚Äôre doing, it‚Äôs a good idea to write your code in files, i.e.¬†‚Äúscripts‚Äù. And when we start creating scripts, we need to worry about how we organize the scripts and data for a project.\nIt is good practice to keep a set of related data, analyses, and text self-contained in a single folder, and use that folder as the working directory ‚Äî in the Unix shell and in R alike. RStudio provides a helpful way to keep your working directory constant through its ‚ÄúProjects‚Äù. When you use a Project, your working directory will be the top-level directory of that project.\n\n\n\n4.2 Create a new RStudio Project\nTo create a new RStudio Project inside your personal dir in /fs/ess/PAS2714/users (e.g., for me, the dir /fs/ess/PAS2714/users/jelmer)\n\n\n\n\n\n\nDon‚Äôt have a personal dir there? (Click to expand)\n\n\n\n\n\nIf you followed the shell homework, you should have created your own dir within /fs/ess/PAS2714/users.\nIf you don‚Äôt have one, you can quickly create it as follows:\n\ndir.create(paste0(\"/fs/ess/PAS2714/users/\", Sys.getenv(\"USER\")))\n\n\n\n\n\nClick File (top bar, below your browser‚Äôs address bar) &gt; New Project\nIn the popup window, click Existing Directory.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick Browse... to select your personal dir.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nIn the next window, you should be in your Home directory (abbreviated as ~), from which you can‚Äôt click your way to /fs/ess! Instead, you‚Äôll first have to click on the (very small!) ... highlighted in the screenshot below:\n\n\n\n\n\n\n\nType at least part of the path to your personal dir in /fs/ess/PAS2714/users, e.g.¬†like shown below, and click OK:\n\n\n\n\n\n\n\nNow you should be able to browse/click the rest of the way to your personal directory, something like /fs/ess/PAS2714/users/jelmer.\nClick Choose to pick your selected directory.\nClick Create Project.\n\nRStudio should reload and you should now have your new Project ‚Äúopen‚Äù. Your working directory should be the Project‚Äôs directory. We can check this using getwd():\n\ngetwd()\n\n/fs/ess/PAS2714/users/jelmer\nFrom now on, we will not change our working directory, and refer to all files relative to our project‚Äôs top-level directory.\n\n\n\n4.3 Create an R script\n\nCreate a new R script (File &gt; New File &gt; R Script)\nClick File &gt; Save As to save the script in the scripts dir that you should see within your personal dir.2 Give it a descriptive name like intro-to-R.R.\n\nFrom now on, type your commands into this script and execute the commands from there.\nTo send code from your script to the console, press Ctrl/Cmd + Enter. This will copy the line of code that your cursor is at to the R console and execute it, and then the cursor will move to the next line.\n\n\n\n4.4 Commenting\nYou can use # signs to comment your code:\n\n# Divide by 2.54 to get the wingspan in inches:\n203 / 2.54    # Original measurement was in cm\n\n\nAnything to the right of a # is ignored by R, meaning it won‚Äôt be executed\nYou can use # both at the start of a line (entire line is a comment) or anywhere in a line following code (rest of the line is a comment)\nIn your R script, comments are formatted differently so you can clearly distinguish them from code\n\nWe recommend that you use lots of comments in your R scripts! They are useful not only for others that you may share your code with, but also for yourself when you look back at your code a day, a month, or a year later."
  },
  {
    "objectID": "homework/R.html#r-objects",
    "href": "homework/R.html#r-objects",
    "title": "Homework: Intro to R",
    "section": "5 R objects",
    "text": "5 R objects\n\n5.1 Assigning stuff to objects\nWe can assign pretty much anything to an object with the assignment operator, &lt;-3. (This is a smaller-than sign &lt; followed by a dash -.)\nA few examples:\n\nwingspan_cm &lt;- 203\nconversion &lt;- 2.54\n\nType that into your script, and use Ctrl/Cmd + Enter twice to send it to the console.\n\n\n\n\n\n\nThe Environment tab\n\n\n\nThe objects you create get added to your ‚Äúenvironment‚Äù, which RStudio shows in the Environment tab in the top-right panel ‚Äî check that wingspan_cm and conversion are indeed there.\n\n\nAfter you‚Äôve assigned a number to an object, you can use it in calculations like so:\n\nwingspan_cm / conversion\n\n[1] 79.92126\n\n\nOr, similarly:\n\nwingspan_inch &lt;- wingspan_cm / conversion\nwingspan_inch\n\n[1] 79.92126\n\n\nThis illustrates that when you execute code with objects, R substitutes the object name that you provide by its contents under the hood. In other words, the object is just a reference to the underlying value(s).\n\n\n5.2 Object names\nObjects can be given almost any name such as x, current_temperature, or subject_id. Some pointers on naming:\n\nBecause R is case sensitive, wingspan_inch is different from Wingspan_inch!\nAn object name cannot contain spaces ‚Äî so for readability, you should separate words using:\n\nUnderscores: wingspan_inch (this is called ‚Äúsnake case‚Äù)\nPeriods: wingspan.inch\nCapitalization: wingspanInch or WingspanInch (‚Äúcamel case‚Äù)\n\nYou will make things easier for yourself by naming objects in a consistent way, e.g.¬†by sticking to a case type.\nObject names can contain but cannot start with a number: x2 is valid but 2x is not4.\nMake object names descriptive yet not too long ‚Äî this is not always easy!\n\n\n Exercises: objects and strings\nA) Which of the following do you think would work and which would return an error:\n\nsession_topic &lt;- \"introduction\"\n\n\nsession_topic &lt;- introduction\n\nTry both to see which works and what error you get for the other one. Also, try to describe in words what the correct line of code is doing.\n\n\nSolution (click here)\n\nThe first of the two options was correct R code, while the second returns an error.\nIn general, keep in mind that unquoted character strings represent objects whereas quoted character strings are ‚Äúliterals‚Äù. Here, we wanted to assign the literal string \"introduction\" to the object session_topic ‚Äî so the former should be quoted and the latter not.\nAn error is produced when you run the second option, because the object introduction does not exist (unless, of course, you had created an object of that name!):\n\nsession_topic &lt;- introduction\n\nError in eval(expr, envir, enclos): object 'introduction' not found\n\n\n\n\nB) Having run the code above, which of the following would make R print \"introduction\"?\n\n\"session_topic\"\n\n\nsession_topic\n\n\n\nSolution (click here)\n\nThe second option is the correct one: here, we want to have R print the value of the object session_topic (which we had just created in exercise A), so we shouldn‚Äôt use quotes.\n\nsession_topic\n\n[1] \"introduction\"\n\n\n\n\nC) Do you think the following code would successfully add 5 and 7? If not, what might happen instead?\n\n\"5\" + \"7\"\n\n\n\nSolution (click here)\n\nIn the code above, the ‚Äúnumbers‚Äù are saved not as numbers (in R lingo: as a numeric) but as character strings (character).\nR can‚Äôt add character strings, so it will return an error:\n\n\"5\" + \"7\"\n\nError in \"5\" + \"7\": non-numeric argument to binary operator\n\n\n(Perhaps you expected it to combine/‚Äúconcatenate‚Äù the two strings in some way ‚Äî this is in fact what Python would do. Or to automatically convert the characters to numbers, since you‚Äôre clearly wanting them to be numbers ‚Äî but it doesn‚Äôt do that either.)\n\n\nD) (Bonus) What is the value of y after running the following lines of code?\n\nx &lt;- 50\ny &lt;- x * 2\nx &lt;- 80\n\n\n\n Solution (click here)\n\nObjects don‚Äôt get linked to each other, so if you change one, it won‚Äôt affect the values of any others. Therefore, y will keep the value 100."
  },
  {
    "objectID": "homework/R.html#functions",
    "href": "homework/R.html#functions",
    "title": "Homework: Intro to R",
    "section": "6 Functions",
    "text": "6 Functions\nEarlier, we divided 203 by 2.54, but what if we wanted to round the resulting number? Like for many things you may want to do in R, there is a function for that.\nFunctions are used by typing their name followed by parentheses:\n\nround(203 / 2.54)\n\n[1] 80\n\n\nHere, round() is a function that rounds a number. The value in the parentheses is called a function ‚Äúargument‚Äù, which is used in the execution of the function.\n\n\n6.1 Using named arguments\nFunctions can have more than one argument, and some of them may have default values.\nThere are some functions that take many arguments and it can get confusing trying to keep them in order. In that case, it is better to explicitly name the arguments.\nWhen you type a function name and pause for a moment, the arguments, their names, and their default values (i.e., the value if the argument is left unspecified) will be shown.\n\n\n\n\n\n\n\n What is the second argument for round() and what is its default value? (Click here)\n\nround has a second argument digits whose default is 0, such that numbers will be rounded to whole integers.\n\nBelow is an example using named arguments with round(). When the arguments are named, the order doesn‚Äôt matter! You might also enter the first few important arguments positionally, and later ones by naming them.\n\nround(x = 1.538462, digits = 2)\n\n[1] 1.54\n\nround(digits = 2, x = 1.538462)\n\n[1] 1.54\n\nround(1.538462, digits = 2)\n\n[1] 1.54\n\n\nAlso here, we can directly plug in objects:\nwingspan_in &lt;- 203 / 2.54\nround(wingspan_in)\nOr ‚Äúnest‚Äù functions ‚Äî here we are adding the log() function to compute the natural log:\nlog(round(203 / 2.54 ))\n\n\n What is the order of execution in the last command? (Click for the solution)\n\nround() is executed first, and the output of round() is used as the input of log()."
  },
  {
    "objectID": "homework/R.html#getting-help",
    "href": "homework/R.html#getting-help",
    "title": "Homework: Intro to R",
    "section": "7 Getting help",
    "text": "7 Getting help\nAs we saw, when we typed round and paused for a moment, we got a pop-up with information about the function. Alternatively, you could type:\n\n?round\n\n‚Ä¶ and the documentation for the function will show up in the lower-right pane.\nThis documentation is often a bit too detailed, and can be terse, so it takes some practice to read. Usage, Arguments, and at the bottom, Examples, are most useful.\nGoogling, even if you don‚Äôt know whether a function exists, will work too (e.g.¬†‚Äúrounding a number in r‚Äù)."
  },
  {
    "objectID": "homework/R.html#vectors",
    "href": "homework/R.html#vectors",
    "title": "Homework: Intro to R",
    "section": "8 Vectors",
    "text": "8 Vectors\nA vector is the most common and basic data structure in R, and is composed of a series of values of the same type5. We can assign a series of values to a vector using the c() function (for ‚Äúcombine‚Äù). For example:\n\nwingspans_cm &lt;- c(11.8, 203, 18.2, 27.9)\n\nA vector can also contain characters ‚Äì but again, quoting is important, or R will think the strings are objects:\n\nbirds &lt;- c(\"hummingbird\", \"bald_eagle\", \"chickadee\", \"cardinal\")\n\nAs mentioned, all of a vector‚Äôs elements have to be of the same type of data. The function class() indicates what kind of data you are working with:\n\nclass(wingspans_cm)\n\n[1] \"numeric\"\n\nclass(birds)\n\n[1] \"character\"\n\n\n\n\n8.1 Data types in R\nThe classes we saw above are different types of atomic vectors, R‚Äôs simplest data type. The 4 most common atomic vector types are:\n\n\"numeric\" (or \"double\") ‚Äì floating point numbers (numbers with decimals)\n\"integer\" ‚Äì integer numbers (no decimals)\n\"character\" ‚Äì character strings\n\"logical\" ‚Äì TRUE and FALSE (also known as boolean)\n\nAlso worth mentioning in this context is:\n\nfactor ‚Äì Character strings with a discrete set of possible values, used mostly for statistical tests and plotting6.\n\n\n\n\n\n\n\nSide note: Vector coercion ‚Äì when not all elements are of the same type. (Click to expand)\n\n\n\n\n\nWhat happens if we try to mix vector types (e.g., ‚Äúcharacter and numeric‚Äù) in a single vector? R converts them to all be the same type, and it does so without telling us about it. For example:\n\nnum_char &lt;- c(1, 2, 3, \"a\")\nclass(num_char)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n8.2 Vectorization!\nLet‚Äôs say we wanted to convert our vector of wingspans to inches: dividing each length in centimeters by 2.54. It turns out that this is as easy as dividing the vector by 2.54:\n\n# wingspans_cm &lt;- c(11.8, 203, 18.2, 27.9)  # Still working with the same wingspans_cm vector\n\nwingspans_in &lt;- wingspans_cm / 2.54\nwingspans_in\n\n[1]  4.645669 79.921260  7.165354 10.984252\n\n\nThis works because R ‚Äúvectorizes‚Äù operations whenever it can. This means that in this case, each element in the vector weights_cm will be divided by 2.54 ‚Äì this number is recycled to match the number of weights. Very useful!\nSimilarly, we can use two vectors of equal length to quickly operate on each element of the vector:\n\nsizes_cm &lt;- c(7.62, 90, 13.1, 21.8)\n\nratio &lt;- wingspans_cm / sizes_cm\nratio\n\n[1] 1.548556 2.255556 1.389313 1.279817\n\n\n\n\n Exercise: Temperature conversion\nRecall that you can convert a temperature of, for example, 26 Fahrenheit to Celcius as follows:\n\n# (26¬∞F - 32) / 1.8 = -3.3¬∞C\n(26 - 32) / 1.8\n\n[1] -3.333333\n\n\nCreate and store a vector with the values 26, 21, 24, 32, 33, 41, 51, representing temperatures in Fahrenheit.\nNow convert all the values in this vector from Fahrenheit to Celcius.\n\n\nClick here for the solution\n\nThanks to R‚Äôs vectorization, you don‚Äôt have to convert these values 1-by-1:\n\ntemps_f &lt;- c(26, 21, 24, 32, 33, 41, 51)\n\n(temps_f - 32) / 1.8\n\n[1] -3.3333333 -6.1111111 -4.4444444  0.0000000  0.5555556  5.0000000 10.5555556\n\n\n\n\n\n\n\n8.3 Other data structures in R\nWhile vectors can be composed of one of several data types, they, in turn, are one of several data structures that R uses. Other important ones are7:\n\ndata.frame ‚Äì A rectangular data structure where each column can be a different data type.\nmatrix ‚Äì A rectangular data structure of a single data type.\nlist ‚Äì A very flexible data structure that we will not further discuss here."
  },
  {
    "objectID": "homework/R.html#data-frames",
    "href": "homework/R.html#data-frames",
    "title": "Homework: Intro to R",
    "section": "9 Data Frames",
    "text": "9 Data Frames\nA data frame (formal object type: data.frame) is a rectangular data structure in which:\n\nRows are observations and columns are variables.\nEach column can be of a different type (numeric, character, etc.)\nSince each column is a vector, all the values (cells) within a column are of the same type.\nAll columns have the same length.\n\n\n\n\n\n\n\n9.1 Create, write, and read a data frame\nWe can easily create a data frame by hand using the data.frame() function and ‚Äúcolumn_name = column_vector‚Äù notation for each column:\n\nbirds_df &lt;- data.frame(species = birds,\n                       wingspan = wingspans_cm,\n                       size = sizes_cm,\n                       n_eggs = c(2, 2, 7, 4)) \n\n\nbirds_df\n\n      species wingspan  size n_eggs\n1 hummingbird     11.8  7.62      2\n2  bald_eagle    203.0 90.00      2\n3   chickadee     18.2 13.10      7\n4    cardinal     27.9 21.80      4\n\n\nMost often, however, you‚Äôll be reading your data frames from files. And you‚Äôll also want to save your modified data frames.\nSo let‚Äôs practice writing and reading a data frame to and from a ‚ÄúCSV‚Äù file ‚Äî a plain-text, tabular file in which columns are delimited by commas (‚ÄúComma-Separated Values‚Äù file).\n\n# Write a data frame to CSV format:\nwrite.csv(x = birds_df, file = \"sandbox/bird-data.csv\", row.names = FALSE)\n\n\n\n\n\n\n\nCurious what the file itself looks like? (Click to expand)\n\n\n\n\n\nIf you want to do the following yourself: next to the R Console, there is tab called Terminal, which will open a Unix shell (!). In there, type the following:\ncat sandbox/bird-data.csv\n\"species\",\"wingspan\",\"size\",\"n_eggs\"\n\"hummingbird\",11.8,7.62,2\n\"bald_eagle\",203,90,2\n\"chickadee\",18.2,13.1,7\n\"cardinal\",27.9,21.8,4\n\n\n\nNow we read our data frame back in from the file:\n\nbirds_df_2 &lt;- read.csv(\"sandbox/bird-data.csv\")\n\nbirds_df_2\n\n      species wingspan  size n_eggs\n1 hummingbird     11.8  7.62      2\n2  bald_eagle    203.0 90.00      2\n3   chickadee     18.2 13.10      7\n4    cardinal     27.9 21.80      4\n\n\n\n\n\n9.2 Inspecting a Data Frame\nUse str() to look at the ‚Äústructure‚Äù of the data ‚Äî it tells us the number rows and columns, and for each column, gives information about the data type and shows the first few values:\n\nstr(birds_df)\n\n'data.frame':   4 obs. of  4 variables:\n $ species : chr  \"hummingbird\" \"bald_eagle\" \"chickadee\" \"cardinal\"\n $ wingspan: num  11.8 203 18.2 27.9\n $ size    : num  7.62 90 13.1 21.8\n $ n_eggs  : num  2 2 7 4\n\n\nFor larger data frames, the head() function, which will print the first 6 rows, is also useful (but is of no real use for a tiny data frame like birds_df):\n\nhead(birds_df)\n\n      species wingspan  size n_eggs\n1 hummingbird     11.8  7.62      2\n2  bald_eagle    203.0 90.00      2\n3   chickadee     18.2 13.10      7\n4    cardinal     27.9 21.80      4\n\n\nFinally, in RStudio, you can open a data frame in a spreadsheet-like manner by clicking on an object in the ‚ÄúEnvironment‚Äù pane, or equivalently, using View():\n\nView(birds_df)\n\n\n\n\n9.3 Functions to get an overview of data frames\n\nSize:\n\nnrow() ‚Äì Number of rows\nncol() ‚Äì Number of columns\ndim() ‚Äì Dimensions: c(number of rows, number of columns)\nlength() ‚Äì For a dataframe: number of columns. For a vector: number of elements.\n\nContent:\n\nhead() ‚Äì shows the first 6 rows\ntail() ‚Äì shows the last 6 rows\n\nNames:\n\nnames() or colnames() ‚Äì column names\nrownames() ‚Äì row names\n\nSummary:\n\nstr() ‚Äì structure of the object and information about the class, length and content of each column\nsummary() ‚Äì summary statistics for each column\n\n\n\n\n Exercise: Cars\nmtcars is an example data frame that is always available in R.\n\nUse head() to print the first 6 rows of the mtcars dataframe.\n\n\n\nClick here for the solution\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\nHow many rows and columns does mtcars contain?\n\n\n\nClick here for the solution\n\nIt contains 32 rows and 11 columns:\n\ndim(mtcars)\n\n[1] 32 11\n\n\nOr:\n\nnrow(mtcars)\n\n[1] 32\n\nncol(mtcars)\n\n[1] 11\n\n\n\n\nGet a vector with the column names of mtcars.\n\n\n\nClick here for the solution\n\n\ncolnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\""
  },
  {
    "objectID": "homework/R.html#subsetting",
    "href": "homework/R.html#subsetting",
    "title": "Homework: Intro to R",
    "section": "10 Subsetting",
    "text": "10 Subsetting\n\n10.1 Basic subsetting of data frames and vectors\nWe can pull out parts of vectors and data frames using square brackets.\n\nVectors\nFor example, for vectors:\n\n# Remind ourselves what this vector contains\nwingspans_cm\n\n[1]  11.8 203.0  18.2  27.9\n\n\n\n# Get the first element\nwingspan_cm[1]\n\n[1] 203\n\n\n\n# Get the third element\nwingspan_cm[3]\n\n[1] NA\n\n\nYou can pull out larger ‚Äúslices‚Äù from the vector by providing vectors of indices:\n\n# Get the first and the third element\nwingspan_cm[c(1, 3)]\n\n[1] 203  NA\n\n\nThe : operator gives you a sequence of consecutive values, which you can also using for slicing:\n\n# Get the second through the fourth element\nwingspan_cm[2:4]\n\n[1] NA NA NA\n\n\n\n\n\nData frames\nTo subset data frames, we need to provide two values: row and column, with a comma between them.\nFor example, to get the element in the 1st row, 1st column:\n\nbirds_df[1, 1]\n\n[1] \"hummingbird\"\n\n\nTo get the element in the 2nd row, 3rd column:\n\nbirds_df[2, 3]\n\n[1] 90\n\n\nTo get the entire 2nd row, leave the column part blank:\n\nbirds_df[2, ]\n\n     species wingspan size n_eggs\n2 bald_eagle      203   90      2\n\n\nAnd to extract the entire 3rd column, leave the row part blank:\n\nbirds_df[, 3]\n\n[1]  7.62 90.00 13.10 21.80\n\n\nTo extract a column, you can also refer to it by name, in multiple ways:\n\nbirds_df$size\n\n[1]  7.62 90.00 13.10 21.80\n\n\n\nbirds_df[, \"size\"]\n\n[1]  7.62 90.00 13.10 21.80\n\n\n\n\n\n Exercise: Subsetting\n\nExtract the 1st-3rd rows and the 4th colum from birds_df.\n\n\n\nClick here for the solution\n\n\nbirds_df[1:3, 4]\n\n[1] 2 2 7\n\n\nOr:\n\nbirds_df[c(1, 2, 3), 4]\n\n[1] 2 2 7\n\n\n\n\nUse the $ notation to extract the n_eggs column from birds_df.\n\n\n\nClick here for the solution\n\n\nbirds_df$n_eggs\n\n[1] 2 2 7 4"
  },
  {
    "objectID": "homework/R.html#miscellaneous",
    "href": "homework/R.html#miscellaneous",
    "title": "Homework: Intro to R",
    "section": "11 Miscellaneous",
    "text": "11 Miscellaneous\n\n11.1 Missing data\nAs R was designed to analyze datasets, it includes the concept of missing data (which is uncommon in other programming languages). Missing data are represented as NA (for ‚ÄúNot Available‚Äù).\n\nheights &lt;- c(2, 4, 4, NA, 6)\n\nWhen doing operations on numbers, most functions will return NA if the data you are working with include missing values. It is a safer behavior as otherwise you may overlook that you are dealing with missing data. You can add the argument na.rm=TRUE to calculate the result while ignoring the missing values.\n\nmean(heights)\n\n[1] NA\n\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\n\n\n\n\n11.2 Packages\nThe functions that we have been using so far (and many, many more) are available in any R session as soon as you start R (we refer to this functionality as ‚Äúbase R‚Äù). However, when doing specialized analyses such as in microbiomics, rather than coding up everything using the basic building blocks in R, we can load add-on code that will allow us to use ‚Äúhigh-level‚Äù functions specifically geared towards the effective analyses of such data.\nThis type of add-on code is distributed in R packages. The default repository for R packages is CRAN, and you install CRAN packages with the install.packages() function:\n\n# Don't run this\ninstall.packages(\"tidyverse\")\n\nIf you‚Äôre doing bioinformatic analyses in R, as we will be doing, you will encounter packages that are not on CRAN but are on ‚ÄúBioconductor‚Äù. To install a package from Bioconductor, use the BiocManager package ‚Äì for example:\n\n# Don't run this\ninstall.packages(\"BiocManager\")  # Install the BiocManager package\nBiocManager::install(\"dada2\")    # Install the dada2 package from Bioconductor\n\n\n\n\n\n\n\nInstallation issues at OSC\n\n\n\nThe installation of some packages is tricky at OSC nowadays, and if you were to try the commands above, they would likely fail. With a bit of extra effort, we can install these packages, but during the workshop, we will be using a custom collection of pre-installed packages.\n\n\n\n\n\n11.3 Saving your data\nSome very brief notes on saving your data in R:\n\nWe already saw the use of write.csv() to save data frames, and you can also use one of readr‚Äôs writing functions.\nTo save R objects ‚Äúas is‚Äù, which can be useful when you‚Äôre working with complex S4 objects that may have taken a long time to generate, like a phyloseq object, you can use:\n\n# Don't run this\n\n# Save an object:\nsaveRDS(my_phyloseq_object, \"my_phyloseq_object.RDS\")\n\n# Load it again in a new R session:\nmy_phyloseq_object &lt;- readRDS(\"my_phyloseq_object.RDS\")\n\nA general recommendation is to not rely on your R session to keep things around, especially ‚Äúovernight‚Äù. Devise your workflow such that you are always saving important objects and results outside of R, and can always use your R script to restart from where you left off.\n\n\n\n\n\n\n\nOptional: Change a setting to not let R save your Workspace\n\n\n\nAlong the line of the above, the default behavior of saving and restoring your ‚ÄúWorkspace‚Äù, which are all the items (objects) that you create during an R session, is bad practice. Instead, you should always recreate your environment from a script and/or saved files with individual pieces of data.\nChange the following setting to prevent R from saving your Workspace whenever you close R:\n\nClick Tools (top bar, below your browser‚Äôs address bar) &gt; Global Options\nIn the pop-up window (stay on the General tab), change the settings under the ‚ÄúWorkspace‚Äù heading to:\n\n\n\n\n\n\n\n\n\n\n\n11.4 S4 Objects\nWhile the object types we have discussed so far are so-called ‚ÄúS3‚Äù objects, we will also be seeing ‚ÄúS4‚Äù objects in this workshop. S4 object are commonly used by bioinformatics packages, for instance phyloseq.\nIn a nutshell, S4 objects allow for complicated, multifaceted datasets (e.g.¬†multiple dataframes with and metadata) to be represented in a single object in a standardized way.\nUnlike S3 objects, S4 objects are usually not manipulated by simple assignment with &lt;-, but with specialized functions that are sure to adhere to the strict object definitions."
  },
  {
    "objectID": "homework/R.html#where-to-go-from-here",
    "href": "homework/R.html#where-to-go-from-here",
    "title": "Homework: Intro to R",
    "section": "12 Where to go from here",
    "text": "12 Where to go from here\nThis document only scratched the surface of R, but it has hopefully provided a good starting point for working with R.\nHere are some potential next steps:\n\nLearn about plotting with ggplot2. Start with these two OSU Code Club sessions:\n\nggplot part I\nggplot part II\n\nLearn about data wrangling with tidyverse packages, especially dplyr and tidyr. Start with these two OSU Code Club sessions:\n\nIntroduction to the tidyverse\nTidyverse 2: More dplyr Data Wrangling\n\n\nBoth of those topics and some other material are also covered in this excellent Carpentries workshop R for Reproducible Scientific Analysis.\nIf you want to start with a book, I would recommend Wickham & Grolemund‚Äôs ‚ÄúR for Data Science‚Äù, which is freely available on the web in a really nice format here.\n\n\n\n\n\n\nWant to try the tidyverse (includes ggplot2) at OSC?\n\n\n\nIf you want to try using the tidyverse in RStudio at OSC now, then load it as follows:\n\n.libPaths(\"/fs/ess/PAS0471/jelmer/R/metabar\")\ndyn.load(\"/fs/ess/PAS0471/jelmer/software/GLPK/lib/libglpk.so.40\", local = FALSE)\nlibrary(tidyverse)"
  },
  {
    "objectID": "homework/R.html#bonus-conditional-subsetting",
    "href": "homework/R.html#bonus-conditional-subsetting",
    "title": "Homework: Intro to R",
    "section": "13 Bonus: conditional subsetting",
    "text": "13 Bonus: conditional subsetting\nAnother common way of subsetting is by using a logical vector of the same length as the original vector: any TRUE will select the element with the same index, while FALSE will not:\n\nwingspans_cm\n\n[1]  11.8 203.0  18.2  27.9\n\n# This will extract the 1st and the 4th element\nwingspans_cm[c(TRUE, FALSE, FALSE, TRUE)]\n\n[1] 11.8 27.9\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 20:\n\n# This will return a logical vector with TRUE for indices that meet the condition\nwingspans_cm &gt; 20\n\n[1] FALSE  TRUE FALSE  TRUE\n\n\n\n# We can use such a vector to select only the values above 20\nwingspans_cm[wingspans_cm &gt; 20]\n\n[1] 203.0  27.9\n\n\n\n\n== and %in%\nYou can test for equality with ==:\n\n\"chickadee\" == \"chickadee\"\n\n[1] TRUE\n\n# Which element(s) of the birds vector equal \"chickadee\"\nbirds == \"chickadee\"\n\n[1] FALSE FALSE  TRUE FALSE\n\n# Extract the element(s) of the birds vector that equal \"chickadee\"\nbirds[birds == \"chickadee\"]\n\n[1] \"chickadee\"\n\n\nIf you want to use a search vector with multiple items, use %in% instead:\n\n# Which element(s) of the birds vector match any of the 3 birds on the right-hand side\nbirds %in% c(\"chickadee\", \"hummingbird\", \"shoebill\")\n\n[1]  TRUE FALSE  TRUE FALSE\n\n# Extract those elements\nbirds[birds %in% c(\"chickadee\", \"hummingbird\", \"shoebill\")] \n\n[1] \"hummingbird\" \"chickadee\"  \n\n\n\n\n\n Exercise: Conditional subsetting\nGiven the following inhabitants of a house, and a list of species to keep:\n\ninhabitants &lt;- c(\"rat\", \"rat\", \"dog\", \"mouse\", \"cat\", \"cat\")\nkeep &lt;- c(\"dog\", \"cat\")\n\n\nWhat do you think the following would return? (Test it and see if you were right.)\n\ninhabitants %in% keep\n\n\nkeep %in% inhabitants\n\n\n\n\nClick here for the solution\n\n\ninhabitants %in% keep\n\n[1] FALSE FALSE  TRUE FALSE  TRUE  TRUE\n\n\n\nkeep %in% inhabitants\n\n[1] TRUE TRUE\n\n\n\n\nExtract the dogs and cats from the inhabitants vector with logical subsetting.\n\n\n\nClick here for the solution\n\n\ninhabitants[inhabitants %in% keep]\n\n[1] \"dog\" \"cat\" \"cat\"\n\n\n\n\n\n\nAttribution\nThis material was modified after material from The Carpentries, especially from this Data Carpentry workshop and this ‚ÄúR for Ecology‚Äù workshop."
  },
  {
    "objectID": "homework/R.html#footnotes",
    "href": "homework/R.html#footnotes",
    "title": "Homework: Intro to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe placement of these panes and their content can be customized.‚Ü©Ô∏é\nYou created this in the shell introduction. If you don‚Äôt have it, create it now.‚Ü©Ô∏é\nIn RStudio, typing Alt + - will write &lt;- in a single keystroke. You can also use = as assignment, but that symbol can have other meanings, and so I recommend sticking with the &lt;- combination.‚Ü©Ô∏é\nThere are also some other restrictions that are beyond the scope of this introduction. For example, there are some names that cannot be used because they are the names of fundamental keywords in R (e.g., if, else, for, see here for a complete list). In general, it‚Äôs also best not to use the names of existing functions, even though this is possible.‚Ü©Ô∏é\nTechnically, a vector can have a length of 1, so our earlier single numbers were vectors too.‚Ü©Ô∏é\nThis is technically a type of data structure.‚Ü©Ô∏é\nNote, that technically, factors are a data structure, but they are more intuitively thought of as a data type‚Ü©Ô∏é"
  },
  {
    "objectID": "homework/instructions.html",
    "href": "homework/instructions.html",
    "title": "Homework Instructions",
    "section": "",
    "text": "Before the workshop starts, please work your way through the following two pages on this website:\nEach of these may take around 2 hours to complete with no prior experience1."
  },
  {
    "objectID": "homework/instructions.html#footnotes",
    "href": "homework/instructions.html#footnotes",
    "title": "Homework Instructions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n That‚Äôs a bit of a guess, we welcome feedback about how long it took you to do this.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amplicon Metabarcoding Workshop",
    "section": "",
    "text": "Day\nTime\nTopic\nBy\n\n\n\n\nWed\n1 - 2 pm\nLecture: Intro to metabarcoding\nTim\n\n\n\n2:15 - 3 pm\nLab: Intro to OSC\nJelmer\n\n\n\n3 - 4:30 pm\nLab: QC & trimming\nJelmer\n\n\nThu\n1 - 2 pm\nLecture: ASV/OTU calling and pipeline considerations\nSoledad\n\n\n\n2:15 - 3:15 pm\nLab: Calling ASVs with the DADA2 pipeline\nTim\n\n\n\n3:30 - 4:30 pm\nLab: Alpha & beta diversity\nTim\n\n\nFri\n1 - 1:30 pm\nLab: Differential abundance with DESeq2\nMelanie\n\n\n\n1:30 - 3 pm\nLab: Network analysis\nMelanie\n\n\n\n3 - 4 pm\nLab: Core microbiome analysis\nFiama\n\n\n\n4 - 4:30 pm\nLab: Metadata and data submission\nFIama\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "03_qc-trim.html#introduction",
    "href": "03_qc-trim.html#introduction",
    "title": "Read QC and Trimming",
    "section": "Introduction",
    "text": "Introduction\nThe first series of steps our analysis workflow concerns the quality control (QC) & ‚Äúpre-processing‚Äù of the reads.\nThe QC part will leave the data, which is stored in FASTQ files, untouched ‚Äî whereas the pre-processing involves the removal of unwanted bits of sequence: in our case, amplicon primers. After the pre-processing, we will still have FASTQ files, just with somewhat less content.\nSpecifically, we will go through the following steps:\n\nQC with FastQC\nSummarizing FastQC results with MultiQC\nRemoving primers with Cutadapt\n\n\n\n\n\n\n\n\nYou should have an active VS Code session. If not, follow these steps (Click to expand)\n\n\n\n\n\nStart a new VS Code session with an open terminal:\n\nLog in to OSC‚Äôs OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2714\nThe starting directory: /fs/ess/PAS2714/&lt;user&gt; (replace &lt;user&gt; with your username)\nNumber of hours: 3\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking ¬†  ¬† =&gt; Terminal =&gt; New Terminal."
  },
  {
    "objectID": "03_qc-trim.html#fastq-files",
    "href": "03_qc-trim.html#fastq-files",
    "title": "Read QC and Trimming",
    "section": "1 FASTQ files",
    "text": "1 FASTQ files\n\n1.1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines ‚Äî at least from Illumina sequencing, you will almost always receive the sequences in this format. Like most genomic data files, these are plain text files, and each sequence that is read by the sequencer (i.e., each ‚Äúread‚Äù) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g.¬†uniquely identifies the read\nThe sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\nThe ‚ÄúQ‚Äù in FASTQ stands for ‚Äúquality‚Äù, to contrast this format with FASTA, a more basic and generic format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nFASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call. Specifically, they correspond to a numeric ‚ÄúPhred‚Äù quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretations for Illumina data:\n\n\n\n\n\n\n\n\n\nPhred quality score\nError probability\nRough interpretation\nASCII character\n\n\n\n\n10\n1 in 10\nterrible\n+\n\n\n20\n1 in 100\nbad\n5\n\n\n30\n1 in 1,000\ngood\n?\n\n\n40\n1 in 10,000\nexcellent\n?\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding ‚ÄúASCII character‚Äù (last column in the table). This allows for a single-character representation of each possible score ‚Äî as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read. (For your reference, here is a complete lookup table ‚Äî look at the top table, ‚ÄúBASE=33‚Äù).\n\n\n\n\n\n\n1.2 Our FASTQ files\n\nListing our FASTQ files\nFirst, let‚Äôs take a look at our list of FASTQ files:\nls -lh data/fastq\ntotal 150M\n-rw-r-----+ 1 jelmer PAS0471 2.0M Mar  1 17:09 NW102AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 17:09 NW102AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 17:09 NW102C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.0M Mar  1 17:09 NW102C_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 1.9M Mar  1 17:09 NW103AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 17:09 NW103AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 17:09 NW103C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.1M Mar  1 17:09 NW103C_R2.fastq.gz\n# [...output truncated...]\nNote in the file listing above that:\n\nThere are two files per sample: _R1 (forward reads) and _R2 (reverse reads). This indicates that we have data from paired-end reads, as is customary with amplicon metabarcoding.\nThe files all have a .gz extension, indicating they have been compressed with the gzip utility.\n\n\n\n\n\n1.3 Viewing FASTQ files\nNext, we‚Äôll take a peak inside one of these FASTQ files.\nThe head command prints the first lines of a file. Let‚Äôs use it try to and print 8 lines, which should show us two reads:\nhead -n 8 data/fastq/NW102AB_R1.fastq.gz\nÔøΩ\n‘Ω€íÔøΩ8ÔøΩEÔøΩÔøΩ_1fÔøΩ\"ÔøΩQDÔøΩJÔøΩÔøΩDÔøΩfs{ÔøΩÔøΩÔøΩÔøΩYkÔøΩÔøΩÔøΩÔøΩdÔøΩÔøΩ*ÔøΩÔøΩ\n|ÔøΩÔøΩxÔøΩÔøΩÔøΩlﬁ¥ÔøΩjÔøΩNÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩŸîÔøΩbUsÔøΩNgÔøΩ«¨ÔøΩÔøΩÔøΩi;_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ|&lt;ÔøΩvÔøΩÔøΩÔøΩÔøΩ3ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩ€ßÔøΩÔøΩ3ƒêHy∆ïÔøΩbIŒüDÔøΩ%ÔøΩÔøΩÔøΩÔøΩSr#~ÔøΩÔøΩ7ÔøΩÔøΩŒΩÔøΩÔøΩ1yÔøΩAi,4\nw\\]\"bÔøΩ#QÔøΩÔøΩÔøΩÔøΩ8ÔøΩÔøΩ+[eÔøΩ3dÔøΩ4HÔøΩÔøΩÔøΩÃíÔøΩlÔøΩ9LVMXÔøΩÔøΩU*ÔøΩMÔøΩÔøΩÔøΩÔøΩ_?ÔøΩÔøΩÔøΩ\\[\"ÔøΩÔøΩ7ÔøΩs\\&lt;_ÔøΩÔøΩÔøΩ:ÔøΩ$ÔøΩÔøΩÔøΩNÔøΩÔøΩvÔøΩ}^ÔøΩÔøΩÔøΩÔøΩswÔøΩ|ÔøΩn;&lt;ÔøΩ&lt;ÔøΩoPÔøΩÔøΩÔøΩÔøΩ\niÔøΩÔøΩkÔøΩÔøΩqÔøΩ÷∞(GÔøΩœ´ÔøΩÔøΩLÔøΩ^ÔøΩÔøΩ=ÔøΩÔøΩ&lt;ÔøΩÔøΩÔøΩKÔøΩÔøΩjÔøΩ_/ÔøΩ[€≠VÔøΩns:ÔøΩÔøΩUÔøΩÔøΩGÔøΩzÔøΩ›éÔøΩjÔøΩÔøΩÔøΩÔøΩ&ÔøΩÔøΩ~ÔøΩFÔøΩÔøΩŸ§ZNÔøΩ'ÔøΩÔøΩr2z}ÔøΩf\\#ÔøΩÔøΩ:ÔøΩ9$ÔøΩÔøΩÔøΩÔøΩÔøΩHÔøΩ›ÇÔøΩ\"ÔøΩ@MÔøΩÔøΩÔøΩÔøΩHÔøΩCÔøΩ\nÔøΩ0ÔøΩppÔøΩÔøΩÔøΩ1ÔøΩOÔøΩÔøΩIÔøΩHÔøΩPÎêÑÔøΩ.»¢eÔøΩÔøΩQÔøΩ&gt;ÔøΩÔøΩÔøΩ\nÔøΩ'ÔøΩ;@D8ÔøΩÔøΩÔøΩ#ÔøΩÔøΩStÔøΩ7kÔøΩgÔøΩÔøΩ|ÔøΩA‰âªÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩdÔøΩ_cÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩa\\ÔøΩ|ÔøΩ_ÔøΩmnÔøΩ]ÔøΩ9NÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩlÔøΩŸ¢ZNÔøΩcÔøΩ9uÔøΩÔøΩÔøΩÔøΩÔøΩnÔøΩÔøΩnÔøΩ`ÔøΩÔøΩ\n\"gÕ∫ÔøΩ\n    ÔøΩÔøΩÔøΩHÔøΩ?2@ÔøΩFCÔøΩS$nÔøΩÔøΩÔøΩ‘íhÔøΩ       n‘ôjÔøΩÔøΩÊúõÔøΩÔøΩf      ÔøΩ?N@ÔøΩCzUlTÔøΩ&ÔøΩhÔøΩPt!ÔøΩr|ÔøΩÔøΩ9~)ÔøΩÔøΩÔøΩeÔøΩAÔøΩ77ÔøΩh{ÔøΩÔøΩ~ÔøΩÔøΩ     ÔøΩÔøΩ\n# [...output truncated...]\n\n\nOuch! üò≥ What went wrong here? (Click for the solution)\n\nWhat happened here is that we are directly seeing the contents of the compressed file, which is simply not human-readable.\n\n\n\n\n\n\n\n\nNo need to decompress\n\n\n\nTo get around the problem we just encountered with head, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones. Fortunately, we don‚Äôt need to decompress them:\n\nAlmost any bioinformatics tool will accept compressed FASTQ files.\nWe can still view these files in compressed form, as shown below.\n\n\n\nInstead, we‚Äôll use the less command, which will automatically display gzip-compressed files in human-readable form:\nless -S data/fastq/NW102AB_R1.fastq.gz\n@M02815:77:000000000-KPK85:1:2101:3678:10660 1:N:0:CCTAAGAC+TTCTAGCT\nCGAGCAATCCACTCGAGTGCCAGCAGCCGCAGTAATACGGAGGGTGCGAGCGTTGTCCGGAATCACTGGGCGTAAAGGGCGCGTAGGCGGCGCGGATAGTCGGCGGTGAAAGCCCGGAGCTCAACTCCGGGTCGGCCGTCGATACTTCCGGGCTTGAGCACTGTAGAGGCAGATGGAATTCCGGGTGTAGCGGTGGAATGCGTAGAGATCCGGAAGAACACCGGTGGCGAAGGCGGTCTGCTGGGCAGTTGCTGACGCTGATGCGCGACAGCGTGGGGAGCAAACAGGATTAGATACC\n+\nCCCCCGGGGGGGGGGGGGGFGGGGGGGGGG+CFGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGEGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGFGGFGFFFFEBFFGFFFDGFGFGBFGFGFGFFFF6?FFFGBF?FBFFF\n@M02815:77:000000000-KPK85:1:2108:2535:14400 1:N:0:CCTAAGAC+TTCTAGCT\nCGAGCAATCCACTCGAGTGTCAGCCGCCGCGGTAATACAGAGGTCCCGAGCGTTGTTCGGATTCATTGGGCGTAAAGGGTGCGTAGGCGGCGGGGAAAGTCTGATGTGAAATCCTGGGGCTCAACCCTGGAACTGCATTGGATACTTCCTTGCTAGAGTACTGGAGAGGAAACTGGAATTTACGGTGTAGCAGTGAAATGCGTAGAGATCGTAAGGAAGACCAGTGGCGAAGGCGAGTTTCTGGACAGTTACTGACGCTGAGGCACGAAGGCCAGGGGAGCAAACGGGATTAGATACC\n+\nCCCCCCGFGFGGGC-FFFGFGFFGGDFFGGGGGECGEGGAEGGGGGGGFGGDGG7CFFGGDCCFGGFCF8FGGGGGGCEGDGGGGGCGGGGGGDEGGGGBFGGDFGGGDG&lt;DFGGGGCEGGGD:FFGGGGFFGFGGFFFFGGGFGGCFGGFGGGGG9CGCGGGG7FGGC:FFGGGGGFGG&lt;?FCGGGGGGGGGGG9CG&lt;ACC?EG5CFGGGGF8CCCC:C@FGCFGGGGGC58=EEG8??77:9@:&lt;3A&gt;7AGFGGGGC?DFC?5&lt;5&gt;&gt;BGGGFGGGGG&gt;4?C42::3:DG=&gt;&lt;&lt;*)*\n\n\n\n\n\n\nless -S suppresses line-wrapping: lines in the file will not be ‚Äúwrapped‚Äù across multiple lines\n\n\n\n\n\n\n\n\n\n\nExercise: Explore the file with less\nless doesn‚Äôt print stuff to screen but instead opens it in a ‚Äúpager‚Äù. After running the command above, you should be viewing the file inside the less pager.\nYou can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d down half a page).\nNotice you won‚Äôt get your shell prompt back until you press q to quit less."
  },
  {
    "objectID": "03_qc-trim.html#running-fastqc-for-1-sample",
    "href": "03_qc-trim.html#running-fastqc-for-1-sample",
    "title": "Read QC and Trimming",
    "section": "2 Running FastQC for 1 sample",
    "text": "2 Running FastQC for 1 sample\n\n2.1 Intro to FastQC\nFastQC is a ubiquitous tools for quality control of FASTQ files. Running FastQC or a similar program is the first step in nearly any high-throughput sequencing project. FastQC is also a good introductory example of a tool with a command-line interface.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser with about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: A FastQC per-base quality score graph for files with reasonably good (left) and very poor (right) quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding) and the x-axis shows the position along the read.\n\n\n\n\n\n\n2.2 Building our FastQC command\nTo run FastQC, we can use the command fastqc.\nIf you want to analyze one of your FASTQ files with default FastQC settings, a complete FastQC command to do so would simply be fastqc followed by the name of the file:\n# (Don't run this)\nfastqc data/fastq/NW102AB_R1.fastq.gz\nHowever, an annoying FastQC default behavior is that it writes its output files in the dir where the input files are ‚Äî in general, it‚Äôs not great practice to directly mix your primary data and results like that!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen. Let‚Äôs try that:\nfastqc -h\nbash: fastqc: command not found...\nHowever, there is a wrinkle: while FastQC is installed at OSC1, we have to first ‚Äúload it‚Äù. The way we will do this here is with a a so-called ‚ÄúConda environment‚Äù that has FastQC installed along with the other programs we will need today.\nHere‚Äôs how we can load that Conda software environment ‚Äî we first load OSC‚Äôs (mini)conda installation, and then we can load (‚Äúactivate‚Äù) the Conda environment that I created for you:\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n\n\n\n\n\nConda and software management\n\n\n\nWe won‚Äôt have time to get into this now, but you want to learn more about Conda / software usage at supercomputers, see this reference page elsewhere on the website.\n\n\n\n Exercise: FastQC help and output dir\nAgain try to print FastQC‚Äôs help info, and figure out which option you can use to specify a custom output directory.\n\n\nClick for the solution\n\nfastqc -h and fastqc --help will both work to show the help info.\nYou‚Äôll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\n\n\nWith the added --outdir (or -o) option, let‚Äôs try to run the following FastQC command:\n# We'll have to create the outdir ourselves, in this case\nmkdir -p results/fastqc\n\nfastqc --outdir results/fastqc data/fastq/NW102AB_R1.fastq.gz\napplication/gzip\nStarted analysis of NW102AB_R1.fastq.gz\nApprox 5% complete for NW102AB_R1.fastq.gz\nApprox 10% complete for NW102AB_R1.fastq.gz\nApprox 15% complete for NW102AB_R1.fastq.gz\n[...truncated...]\nAnalysis complete for NW102AB_R1.fastq.gz\nSuccess!! üéâ\n\n\n\n2.3 FastQC output files\nLet‚Äôs take a look at the files in the output dir we specified:\nls -lh results/fastqc\ntotal 1.2M\n-rw-r--r-- 1 jelmer PAS0471 713K Feb  4 14:02 NW102AB_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 431K Feb  4 14:02 NW102AB_R1_fastqc.zip\n\nThere is a .zip file, which contains tables with FastQC‚Äôs data summaries\nThere is an .html (HTML) file, which contains plots ‚Äî this is what we‚Äôll look at next\n\n\n\n Exercise: Another FastQC run\nRun FastQC for the corresponding R2 FASTQ file. Would you use the same output dir?\n\n\nClick for the solution\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don‚Äôt need to worry about overwriting files, and it will be easier to have all the results in a single dir.\nTo run FastQC for the R2 (reverse-read) file:\nfastqc \\\n  --outdir results/fastqc \\\n  data/fastq/NW102AB_R2.fastq.gz\nStarted analysis of NW102AB_R2.fastq.gz\nApprox 5% complete for NW102AB_R2.fastq.gz\nApprox 10% complete for NW102AB_R2.fastq.gz\nApprox 15% complete for NW102AB_R2.fastq.gz\n[...truncated...]\nAnalysis complete for NW102AB_R2.fastq.gz\nls -lh results/fastqc\n-rw-r--r-- 1 jelmer PAS0471 241K Mar 13 14:50 NW102AB_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 256K Mar 13 14:50 NW102AB_R1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 234K Mar 13 14:53 NW102AB_R2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 244K Mar 13 14:53 NW102AB_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs."
  },
  {
    "objectID": "03_qc-trim.html#interpreting-fastqc-output",
    "href": "03_qc-trim.html#interpreting-fastqc-output",
    "title": "Read QC and Trimming",
    "section": "3 Interpreting FastQC output",
    "text": "3 Interpreting FastQC output\n\n3.1 FastQC HTML modules\nWe‚Äôll now go through a couple of the FastQC plots/modules, with first some example plots2 with good/bad results for reference.\n\nOverview of module results\nFastQC has ‚Äúpass‚Äù (checkmark in green), ‚Äúwarning‚Äù (exclamation mark in orange), and ‚Äúfail‚Äù (cross in red) assessments for each module, as you can see below.\nThese assessments are handy, but a ‚Äúwarning‚Äù/‚Äúfail‚Äù is not necessarily the bad news it may appear to be:\n\nSome of these modules are perhaps overly strict.\nSome warnings and fails are easily remedied or simply not a very big deal.\nFastQC assumes that your data is derived from whole-genome shotgun sequencing ‚Äî some other types of data like RNA-seq data will therefore always trigger a couple of warnings and fails.\n\n\n\n\n\n\n\n\n\nBasic statistics\nThis shows, for example, the number of sequences (reads) and the read length range for your file:\n\n\n\n\n\n\n\n\nPer base quality sequence quality\nThis figure visualize the mean per-base quality score (y-axis) along the length of the reads (x-axis). Note that:\n\nA decrease in sequence quality along the reads is normal.\nR2 (reverse) reads are usually worse than R1 (forward) reads.\n\n\n\nGood / acceptable: \n\nBad: \n\n\nTo interpret the quality scores along the y-axis, note the color scaling in the graphs (green is good, etc.), and see this table for details:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent\n\n\n\n\n\n\nPer sequence quality scores\nThis shows the same quality scores we saw above, but now simply as a density plot of per-read averages, with the quality score now along the x-axis, and the number of reads with that quality score along the y-axis:\n\n\nGood:\n\n\n\n\nBad: \n\n\n\n\n\nSequence length distribution\nWill throw a warning as soon as not all sequences are of the same length (like below), but this is quite normal.\n\n\n\n\n\n\n\n\nAdapter content\nChecks for known adapter sequences. When some of the insert sizes are shorter than the read length, adapters can end up in the sequence ‚Äì these should be removed!\n\n\nGood: \n\nBad: \n\n\n\n\n\n\n3.2 Checking your FastQC results\nFirst, you‚Äôll unfortunately have to download FastQC‚Äôs output HTML files to your computer:\n\nFind the FastQC HTML files in the file explorer in the VS Code side bar.\nRight-click on one of them, click Download... and follow the prompt to download the file somewhere to your computer (doesn‚Äôt matter where).\nRepeat this for the second file.\nThen, open your computer‚Äôs file browser, find the downloaded files, and double-click on one. It should be opened in your default web browser.\n\n\n Exercise: Interpreting your FastQC results\n\nOpen the HTML file for the R1 FASTQ file and go through the modules we discussed above. Can you make sense of it? Does the data look good to you, overall?\nNow open the HTML file for the R2 FASTQ file and take a look just at the quality scores. Does it look any worse than the R1?"
  },
  {
    "objectID": "03_qc-trim.html#running-fastqc-for-all-samples",
    "href": "03_qc-trim.html#running-fastqc-for-all-samples",
    "title": "Read QC and Trimming",
    "section": "4 Running FastQC for all samples",
    "text": "4 Running FastQC for all samples\nIf we want to run FastQC for all samples, it will be much better to write a shell script and submit that as a so-called Slurm batch job, rather than running FastQC ‚Äúinteractively‚Äù like we did for the first sample.\nThis is especially true for a complete data set, which would have much larger FASTQ files and possibly more samples.\n\n4.1 Building our script\nFirst, we will put our earlier FastQC code inside a script.\n\nOpen a new file in VS Code: click , then File, then New File.\nSave the file (e.g.¬†press Ctrl/‚åò+S) in your scripts directory as fastqc.sh.\nPaste the following code (same as we used above) in the script:\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Create the output dir\nmkdir -p results/fastqc\n\n# Run FastQC\nfastqc --outdir results/fastqc data/fastq/NW102AB_R1.fastq.gz\n\nHowever, we will need to modify our call to fastqc ‚Äî we will loop over all FASTQ files as follows:\n# Run FastQC (replacement for fastqc line above)\nfor fastq_file in data/fastq/*fastq.gz; do\n    fastqc --outdir results/fastqc \"$fastq_file\"\ndone\n\nWe are looping over all FASTQ files with the globbing pattern data/fastq/*fastq.gz. The loop will run as many times as we have FASTQ files.\nIn every iteration of the loop, the \"$fastq_file\" variable will contain 1 FASTQ file name, and we will run fastqc for that file3.\n\n\nWe will be submitting this script as a batch job to the Slurm compute job scheduler. To do so, we should also add some lines at the top of the script:\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-fastqc.out\n\nThe first line #!/bin/bash merely indicates that this is a shell script4 and not say an R or Python script.\nThe lines starting with #SBATCH tell Slurm some details about our compute job request (much like we did when we filled out the form to start a VS Code session):\n\nWe always need to specify an ‚Äúaccount‚Äù, i.e.¬†OSC project, that should be billed.\nThe only other option (of many possible!) we will use here is to specify the output file: this is where any output will go that would otherwise be printed to screen, such as the FastQC progress output we saw above.\n\n\n\nWe will also add the following line to change some shell script settings, which will cause the script to stop running if any errors occur:\n# Strict bash settings\nset -euo pipefail\n\nAll in all, our script should read:\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-fastqc.out\n\n# Strict bash settings\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Create the output dir\nmkdir -p results/fastqc\n\n# Run FastQC for all FASTQ files\nfor fastq_file in data/fastq/*fastq.gz; do\n    fastqc --outdir results/fastqc \"$fastq_file\"\ndone\n\n# Report\necho \"Done with script fastqc.sh\"\ndate\n\n\n\n4.2 Submitting the script\nSubmit the script to Slurm (‚Äúsubmit it to the queue‚Äù) with the sbatch command:\nsbatch scripts/fastqc.sh\nSubmitted batch job 27047185\nAfter some seconds (sometimes up to a minute or so5), the Slurm job should start and create the output file that we specified at the top of the script: slurm-fastqc.out.\n\n\n\n4.3 Checking the output\nEarlier, FastQC logging output (‚Äú10% complete‚Äù, etc.) was printed to screen, but because this job now runs remotely on another compute node, such output will end up in the ‚ÄúSlurm log file‚Äù whose name we specified in the script. Let‚Äôs take a look:\nless slurm-fastqc.out\napplication/gzip\nStarted analysis of NW102AB_R1.fastq.gz\nApprox 5% complete for NW102AB_R1.fastq.gz\nApprox 15% complete for NW102AB_R1.fastq.gz\nApprox 20% complete for NW102AB_R1.fastq.gz\nApprox 30% complete for NW102AB_R1.fastq.gz\nApprox 35% complete for NW102AB_R1.fastq.gz\nApprox 45% complete for NW102AB_R1.fastq.gz\nApprox 50% complete for NW102AB_R1.fastq.gz\nApprox 60% complete for NW102AB_R1.fastq.gz\nApprox 70% complete for NW102AB_R1.fastq.gz\nApprox 75% complete for NW102AB_R1.fastq.gz\nApprox 85% complete for NW102AB_R1.fastq.gz\nApprox 90% complete for NW102AB_R1.fastq.gz\nAnalysis complete for NW102AB_R1.fastq.gz\napplication/gzip\nStarted analysis of NW102AB_R2.fastq.gz\nApprox 5% complete for NW102AB_R2.fastq.gz\nApprox 15% complete for NW102AB_R2.fastq.gz\nApprox 20% complete for NW102AB_R2.fastq.gz\n#[...output truncated...]\nThat looks good, in the output I printed above we can see that FastQC ran to completion for one FASTQ file and then started a second ‚Äî and this will go on and on, for all of our 64 FASTQ files.\nYou will know that the job has successfully finished when the last few lines of the Slurm log file read ‚ÄúDone with script fastqc.sh‚Äù and print the date and time (as per the last lines of our script!):\ntail slurm-fastqc.out\nApprox 75% complete for W404BC_R2.fastq.gz\nApprox 85% complete for W404BC_R2.fastq.gz\nApprox 90% complete for W404BC_R2.fastq.gz\nApprox 95% complete for W404BC_R2.fastq.gz\nAnalysis complete for W404BC_R2.fastq.gz\nDone with script fastqc.sh\nWed Mar  6 13:34:10 EST 2024\n\nOf course, we should also check the main output files ‚Äî the HTMLs and zip files:\nls results/fastqc\nNW102AB_R1_fastqc.html  NW103C_R1_fastqc.zip    NW203A_R2_fastqc.html   NW304BC_R2_fastqc.zip   NW403BC_R1_fastqc.html  W101AB_R1_fastqc.zip   W103C_R2_fastqc.html   W205A_R2_fastqc.zip    W304AB_R1_fastqc.html  W403C_R1_fastqc.zip\nNW102AB_R1_fastqc.zip   NW103C_R2_fastqc.html   NW203A_R2_fastqc.zip    NW305AB_R1_fastqc.html  NW403BC_R1_fastqc.zip   W101AB_R2_fastqc.html  W103C_R2_fastqc.zip    W205BC_R1_fastqc.html  W304AB_R1_fastqc.zip   W403C_R2_fastqc.html\nNW102AB_R2_fastqc.html  NW103C_R2_fastqc.zip    NW203BC_R1_fastqc.html  NW305AB_R1_fastqc.zip   NW403BC_R2_fastqc.html  W101AB_R2_fastqc.zip   W204A_R1_fastqc.html   W205BC_R1_fastqc.zip   W304AB_R2_fastqc.html  W403C_R2_fastqc.zip\nNW102AB_R2_fastqc.zip   NW201AB_R1_fastqc.html  NW203BC_R1_fastqc.zip   NW305AB_R2_fastqc.html  NW403BC_R2_fastqc.zip   W101C_R1_fastqc.html   W204A_R1_fastqc.zip    W205BC_R2_fastqc.html  W304AB_R2_fastqc.zip   W404A_R1_fastqc.html\nNW102C_R1_fastqc.html   NW201AB_R1_fastqc.zip   NW203BC_R2_fastqc.html  NW305AB_R2_fastqc.zip   NW404A_R1_fastqc.html   W101C_R1_fastqc.zip    W204A_R2_fastqc.html   W205BC_R2_fastqc.zip   W304C_R1_fastqc.html   W404A_R1_fastqc.zip\nNW102C_R1_fastqc.zip    NW201AB_R2_fastqc.html  NW203BC_R2_fastqc.zip   NW305C_R1_fastqc.html   NW404A_R1_fastqc.zip    W101C_R2_fastqc.html   W204A_R2_fastqc.zip    W303AB_R1_fastqc.html  W304C_R1_fastqc.zip    W404A_R2_fastqc.html\nNW102C_R2_fastqc.html   NW201AB_R2_fastqc.zip   NW304A_R1_fastqc.html   NW305C_R1_fastqc.zip    NW404A_R2_fastqc.html   W101C_R2_fastqc.zip    W204BC_R1_fastqc.html  W303AB_R1_fastqc.zip   W304C_R2_fastqc.html   W404A_R2_fastqc.zip\nNW102C_R2_fastqc.zip    NW201C_R1_fastqc.html   NW304A_R1_fastqc.zip    NW305C_R2_fastqc.html   NW404A_R2_fastqc.zip    W103AB_R1_fastqc.html  W204BC_R1_fastqc.zip   W303AB_R2_fastqc.html  W304C_R2_fastqc.zip    W404BC_R1_fastqc.html\nNW103AB_R1_fastqc.html  NW201C_R1_fastqc.zip    NW304A_R2_fastqc.html   NW305C_R2_fastqc.zip    NW404BC_R1_fastqc.html  W103AB_R1_fastqc.zip   W204BC_R2_fastqc.html  W303AB_R2_fastqc.zip   W403AB_R1_fastqc.html  W404BC_R1_fastqc.zip\nNW103AB_R1_fastqc.zip   NW201C_R2_fastqc.html   NW304A_R2_fastqc.zip    NW403A_R1_fastqc.html   NW404BC_R1_fastqc.zip   W103AB_R2_fastqc.html  W204BC_R2_fastqc.zip   W303C_R1_fastqc.html   W403AB_R1_fastqc.zip   W404BC_R2_fastqc.html\nNW103AB_R2_fastqc.html  NW201C_R2_fastqc.zip    NW304BC_R1_fastqc.html  NW403A_R1_fastqc.zip    NW404BC_R2_fastqc.html  W103AB_R2_fastqc.zip   W205A_R1_fastqc.html   W303C_R1_fastqc.zip    W403AB_R2_fastqc.html  W404BC_R2_fastqc.zip\nNW103AB_R2_fastqc.zip   NW203A_R1_fastqc.html   NW304BC_R1_fastqc.zip   NW403A_R2_fastqc.html   NW404BC_R2_fastqc.zip   W103C_R1_fastqc.html   W205A_R1_fastqc.zip    W303C_R2_fastqc.html   W403AB_R2_fastqc.zip\nNW103C_R1_fastqc.html   NW203A_R1_fastqc.zip    NW304BC_R2_fastqc.html  NW403A_R2_fastqc.zip    W101AB_R1_fastqc.html   W103C_R1_fastqc.zip    W205A_R2_fastqc.html   W303C_R2_fastqc.zip    W403C_R1_fastqc.html\nThat‚Äôs a lot of files! Do we need to check all of them? Luckily not, thanks to MultiQC."
  },
  {
    "objectID": "03_qc-trim.html#summarizing-qc-results-with-multiqc",
    "href": "03_qc-trim.html#summarizing-qc-results-with-multiqc",
    "title": "Read QC and Trimming",
    "section": "5 Summarizing QC results with MultiQC",
    "text": "5 Summarizing QC results with MultiQC\nHere are some challenges you may run into after running FastQC:\n\nWhen you have many FASTQ files, you‚Äôll generate a lot of FastQC HTML files to sort through (as we did above).\nEven if you do diligently go through each file, it‚Äôs not that easy to compare the results across samples in detail, since they are not drawn in the same graphs.\n\nMultiQC addresses these problems by aggregating FastQC results from many files, and summarizing them into a single HTML file with (still) one graph per FastQC module.\n\n\n\n\n\n\nNot just for FastQC results! MultiQC can recognize and process the output of dozens of bioinformatics tools.\n\n\n\n\n\n\nMultiQC‚Äôs graphs are also interactive, but here is a static example:\n\n\n\n\n\n\n\n5.1 Running MultiQC\nWe will only need to run MultiQC once (because it will aggregate all FastQC results at once), and that will only take a few seconds ‚Äî therefore, we can run the command interactively without using a script.\nLet‚Äôs start by running MultiQC (command multiqc) with the --help option:\nmultiqc --help\n# (Only the top part of the output is shown in the screenshot below)\n\n\n\n\n\nAs the first couple of help lines in the paler gray color explain, MultiQC will search the [ANALYSIS DIRECTORY], a dir that we pass to it as an argument at the end of the command line. That is, if we tell MultiQC about the results/fastqc directory like so, it should find and then aggregate all the FastQC results in there:\n# (Don't run this - we'll complete the command in a second)\nmultiqc results/fastqc\nThe default output directory of MultiQC is the current working directory, so just like with FastQC, we do want to use the option for the output dir ‚Äî this is our final command and you can go ahead and execute it:\n# Run MultiQC to summarize the FastQC results\nmultiqc --outdir results/multiqc results/fastqc\n\n\n\n\n\n\n\n\n5.2 MultiQC output\nOnce its done, you should have the following files in the output dir:\nls -lh results/multiqc\ntotal 1.7M\ndrwxr-xr-x 2 jelmer PAS2250 4.0K Mar  13 14:57 multiqc_data\n-rw-r--r-- 1 jelmer PAS2250 1.7M Mar  13 14:57 multiqc_report.html\nGo ahead and find the HTML file in VS Code‚Äôs file browser, right-click on it and then download it to your computer, and click on the file in your own computer to open it in your browser (i.e., just like we did with the FastQC output).\n\n Exercise: Explore the MultiQC results\nCheck for example whether patterns are consistent across samples, or if there are any outliers."
  },
  {
    "objectID": "03_qc-trim.html#cutadapt",
    "href": "03_qc-trim.html#cutadapt",
    "title": "Read QC and Trimming",
    "section": "6 Cutadapt",
    "text": "6 Cutadapt\nWhen you prepare samples for amplicon metabarcoding, you amplify a specific region with primers, and these primers will be included in the sequences that you receive. Before we go any further, we need to remove these primer sequences, which we can do with the program Cutadapt.\nWe will write a script with a loop to run Cutadapt for all samples and submit it a batch job like we did with FastQC.\n Open a new text file and save it as scripts/cutadapt.sh.\n\n6.1 Primer sequences\nWhen we run Cutadapt, we need to tell it about our primer sequences as well as their reverse complements. We‚Äôll start by storing the primer sequences in variables:\n# Primer sequences\nprimer_f=GTGTGYCAGCMGCCGCGGTAA\nprimer_r=GGACTACNVGGGTWTCTAAT\nThere are many ways of getting the reverse complement of a sequence, including manually building it up, but here we‚Äôll use a trick with the tr command to change each base into its complement, followed by the rev command to get the reverse complement ‚Äî for example, for the forward primer:\necho \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev\nTTACCGCGGCKGCTGRCACAC\nBelow, we‚Äôll get the reverse complement for both primers, and will store those in a variable as well using the construct variable=$(command)6:\n# Get the reverse-complements of the primers\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\n\n# Check the sequences\necho \"$primer_f_rc\"\necho \"$primer_r_rc\"\nTTACCGCGGCKGCTGRCACAC\nATTAGAWACCCBNGTAGTCC\n\n\n\n\n\n\nMore about the above tr command\n\n\n\nTODO\n\n\n\n\n\n6.2 Building the Cutadapt command\nFirst, here is how we can tell Cutadapt about the primer sequences:\nTODO add some details about the syntax\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\"\n\n\n\n\n\n\nSpreading commands across multiple lines with \\\n\n\n\nAbove, I spread the command across multiple lines, which makes it a little easier to read. You can run the command exactly like that: the backslashes (\\) at the end of all except the last line tell the shell that our command will continue on the next line.\n\n\nWe will also:\n\nTell Cutadapt to only keep sequences that contain the primer7, with the --trimmed-only option.\nInstruct Cutadapt to use 8 ‚Äúcores‚Äù with --cores 8, which can speed up the run by up to 8-fold. For our small FASTQ files, this isn‚Äôt really necessary, but for a larger dataset, that can save quite some time.\n\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\" \\\n    --trimmed-only \\\n    --cores 8\nFinally, let‚Äôs also add the output files (--output for R1 and --paired-output for R2) and the input files (as positional arguments at the end of the command) for a single example sample. With that, we have a final example command of running Cutadapt for a single sample:\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\" \\\n    --trimmed-only \\\n    --cores 8 \\\n    --output results/cutadapt/NW102AB_R1.fastq.gz \\\n    --paired-output results/cutadapt/NW102AB_R1.fastq.gz \\\n    data/fastq/NW102AB_R1.fastq.gz \\\n    data/fastq/NW102AB_R2.fastq.gz\n\n\n\n6.3 Our Cutadapt loop\nIn our script, we will run Cutadapt inside a loop, similar to how we ran FastQC. However, this case is a bit more complicated, because we need to run Cutadapt for one sample and therefore two FASTQ files at a time, rather than for one FASTQ file at a time.\nWe will do that by looping over the R1 (forward read) files only, and inside the loop, inferring the name of the R2 file:\n# Loop over the R1 files\nfor R1_in in data/fastq/*R1.fastq.gz; do\n    # Get the R2 file name with \"parameter expansion\"\n    # This does a search-and-replace: replace \"_R1\" with \"_R2\"\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    \n    # Define the output files\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Run Cutadapt\n    cutadapt \\\n            -a \"$primer_f\"...\"$primer_r_rc\" \\\n            -A \"$primer_r\"...\"$primer_f_rc\" \\\n            --trimmed-only \\\n            --cores 8 \\\n            --output \"$R1_out\" \\\n            --paired-output \"$R2_out\" \\\n            \"$R1\" \"$R2\"\ndone\n\n\n\n6.4 The final script\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-cutadapt.out\n#SBATCH --cpus-per-task=8\n\n# Strict bash settings\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Primer sequences\nprimer_f=GTGTGYCAGCMGCCGCGGTAA\nprimer_r=GGACTACNVGGGTWTCTAAT\n\n# Get the reverse-complements of the primers\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\n\n# Create the output dir\noutdir=results/cutadapt\nmkdir -p \"$outdir\"\n\n# Loop over the R1 files\nfor R1_in in data/fastq/*R1.fastq.gz; do\n    # Get the R2 file name\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    \n    # Define the output files\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Run Cutadapt\n    cutadapt \\\n            -a \"$primer_f\"...\"$primer_r_rc\" \\\n            -A \"$primer_r\"...\"$primer_f_rc\" \\\n            --trimmed-only \\\n            --cores 8 \\\n            --output \"$R1_out\" \\\n            --paired-output \"$R2_out\" \\\n            \"$R1\" \"$R2\"\ndone\n\n# Report\necho \"Done with script cutadapt.sh\"\ndate\n\n\n\n6.5 Check the output\nless slurm-cutadapt.sh\nx\ntail slurm-cutadapt.sh\n22      4       0.0     2       0 3 1\n24      1       0.0     2       0 1\n25      1       0.0     2       0 0 1\n34      1       0.0     2       0 0 1\n38      2       0.0     2       0 2\n42      1       0.0     2       0 0 1\n44      1       0.0     2       0 1\n47      1       0.0     2       0 1\nDone with script cutadapt.sh\nWed Mar  6 14:46:56 EST 2024\ngrep \"with adapter:\" slurm-cutadapt.sh\n  Read 1 with adapter:                  12,798 (99.6%)\n  Read 2 with adapter:                  12,541 (97.6%)\n  Read 1 with adapter:                  14,499 (99.7%)\n  Read 2 with adapter:                  14,211 (97.7%)\n  Read 1 with adapter:                  12,174 (99.7%)\n  Read 2 with adapter:                  11,835 (97.0%)\n  Read 1 with adapter:                  15,054 (99.7%)\n  Read 2 with adapter:                  14,737 (97.6%)\n# [...output truncated...]\n\n\n\n\n\n\nWant to quickly see the lowest % of reads with adapter? (Click to expand)\n\n\n\n\n\ngrep \"with adapter:\" slurm-cutadapt.out | cut -d\"(\" -f2 | sort -n | head\n97.0%)\n97.3%)\n97.5%)\n97.5%)\n97.5%)\n97.5%)\n97.6%)\n\n\n\nls results/cutadapt\nNW102AB_R1.fastq.gz  NW103C_R1.fastq.gz   NW203A_R1.fastq.gz   NW304BC_R1.fastq.gz  NW403A_R1.fastq.gz   NW404BC_R1.fastq.gz  W103AB_R1.fastq.gz  W204BC_R1.fastq.gz  W303AB_R1.fastq.gz  W304C_R1.fastq.gz   W404A_R1.fastq.gz\nNW102AB_R2.fastq.gz  NW103C_R2.fastq.gz   NW203A_R2.fastq.gz   NW304BC_R2.fastq.gz  NW403A_R2.fastq.gz   NW404BC_R2.fastq.gz  W103AB_R2.fastq.gz  W204BC_R2.fastq.gz  W303AB_R2.fastq.gz  W304C_R2.fastq.gz   W404A_R2.fastq.gz\nNW102C_R1.fastq.gz   NW201AB_R1.fastq.gz  NW203BC_R1.fastq.gz  NW305AB_R1.fastq.gz  NW403BC_R1.fastq.gz  W101AB_R1.fastq.gz   W103C_R1.fastq.gz   W205A_R1.fastq.gz   W303C_R1.fastq.gz   W403AB_R1.fastq.gz  W404BC_R1.fastq.gz\nNW102C_R2.fastq.gz   NW201AB_R2.fastq.gz  NW203BC_R2.fastq.gz  NW305AB_R2.fastq.gz  NW403BC_R2.fastq.gz  W101AB_R2.fastq.gz   W103C_R2.fastq.gz   W205A_R2.fastq.gz   W303C_R2.fastq.gz   W403AB_R2.fastq.gz  W404BC_R2.fastq.gz\nNW103AB_R1.fastq.gz  NW201C_R1.fastq.gz   NW304A_R1.fastq.gz   NW305C_R1.fastq.gz   NW404A_R1.fastq.gz   W101C_R1.fastq.gz    W204A_R1.fastq.gz   W205BC_R1.fastq.gz  W304AB_R1.fastq.gz  W403C_R1.fastq.gz\nNW103AB_R2.fastq.gz  NW201C_R2.fastq.gz   NW304A_R2.fastq.gz   NW305C_R2.fastq.gz   NW404A_R2.fastq.gz   W101C_R2.fastq.gz    W204A_R2.fastq.gz   W205BC_R2.fastq.gz  W304AB_R2.fastq.gz  W403C_R2.fastq.gz"
  },
  {
    "objectID": "03_qc-trim.html#footnotes",
    "href": "03_qc-trim.html#footnotes",
    "title": "Read QC and Trimming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list‚Ü©Ô∏é\n Attribution: Some of the FastQC example plots were taken from here.‚Ü©Ô∏é\nTherefore, our FastQC analysis will run sequentially (1-by-1) for each file, not in parallel.‚Ü©Ô∏é\nUsing the shell language Bash, specifically‚Ü©Ô∏é\n And very large jobs can sometimes take hours to start, but our jobs are small so that should not happen.‚Ü©Ô∏é\nThis is called ‚Äúcommand substitution‚Äù.‚Ü©Ô∏é\n This is not the default: Cutadapt is even more commonly used to remove adapters, and then this doesn‚Äôt apply‚Ü©Ô∏é"
  },
  {
    "objectID": "ref/ref_software.html#overview",
    "href": "ref/ref_software.html#overview",
    "title": "Software management",
    "section": "Overview",
    "text": "Overview\nAt supercomputers like OSC, there are often system-wide installations of a number of bioinformatics programs. We do need to ‚Äúload‚Äù such programs before we can use them. However, OSC‚Äôs collection of bioinformatics programs is unfortunately not comprehensive, and some of the available programs only come in relatively old versions.\nWe therefore also need another way to make bioinformatics programs available to ourselves. Two common methods are the Conda software management program and containers. We will talk about loading MCIC‚Äôs Conda environments, while the at-home reading covers installing software yourself with Conda, and using containers downloaded from the internet."
  },
  {
    "objectID": "ref/ref_software.html#loading-software-at-osc-with-lmod-modules",
    "href": "ref/ref_software.html#loading-software-at-osc-with-lmod-modules",
    "title": "Software management",
    "section": "1 Loading software at OSC with Lmod modules",
    "text": "1 Loading software at OSC with Lmod modules\nOSC administrators manage software with the ‚ÄúLmod‚Äù system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it. That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nWe can load, unload, and search for available software modules using the module command and its various subcommands.\n\n1.1 Checking whether a program is available\nThe OSC website has a list of installed software. You can also search for available software in the shell using two subtly different module subcommands1:\n\nmodule spider lists all modules that are installed.\nmodule avail lists modules that can be directly loaded given the current environment (i.e., taking into account which other software has been loaded).\n\nSimply running module spider or module avail would spit out the full lists of installed/available programs ‚Äî it is more useful to add a search term as an argument to these commands ‚Äî below, we‚Äôll search for the Conda distribution ‚Äúminiconda‚Äù, with each of these two subcommands:\nmodule spider miniconda\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  miniconda3:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        miniconda3/4.10.3-py37\n        miniconda3/4.12.0-py38\n        miniconda3/4.12.0-py39\n        miniconda3/23.3.1-py310\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"miniconda3\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider miniconda3/4.12.0-py39\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nmodule avail miniconda\n------------------------------------------------------------------------------------------------------ /apps/lmodfiles/Core -------------------------------------------------------------------------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py38    miniconda3/4.12.0-py39    miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\nAs stated at the bottom of the output below, the (D) in the module avail output above marks the default version of the program: this is the version of the program that will be loaded if we don‚Äôt specify a version ourselves (see examples below). The module spider command does not provide this information.\n\n\n\n1.2 Loading software\nAll other Lmod software functionality is also accessed using module subcommands. For instance, to make a program available to us we use the load subcommand:\n# Load a module:\nmodule load miniconda3               # Load the default version\nmodule load miniconda3/23.3.1-py310  # Load a specific version\n\n\n\n\n\n\nModules do not remain loaded across separate shell sessions\n\n\n\nModule loading does not persist across shell sessions. Whenever you get a fresh shell session (including but not limited to after logging into OSC again), you will have to (re)load any modules you want to use!\n\n\nTo check which modules have been loaded, use module list. Its output will also include automatically loaded modules, so for example, if you loaded miniconda3/23.3.1-py310, you should see the following list where the miniconda3 module is listed as the 6th entry:\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/23.3.1-py310\nOccasionally, when you run into conflicting (mutually incompatible) modules, it can be useful to unload modules, which you can do as follows:\nmodule unload miniconda3    # Unload a specific module\nmodule purge                # Unload all modules\n\n\n\n1.3 A practical example: FastQC again\nHere, we‚Äôll load the module for FastQC again. First, let‚Äôs confirm that we indeed cannot currently use FastQC by running the fastqc command with the --help option:\nfastqc --help\nbash: fastqc: command not found\n\n\n\n\n\n\nHelp!\n\n\n\nMany command-line programs can be run with with a --help (and/or -h) flag, and this is often a good thing to try first, since it will tell use whether we can use the program ‚Äî and if we can, we immediately get some usage information.\n\n\nNext, let‚Äôs check whether FastQC is available at OSC, and if so, in which versions:\nmodule avail fastqc\nfastqc/0.11.8\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be a reason to specify the version when we load the FastQC module?\n\n\n\n\n\nWhen we use the module load command inside a script, specifying a version would:\n\nEnsure that when we run the same script a year later, the same version would be used (assuming it hasn‚Äôt been removed) ‚Äî otherwise, it‚Äôs possible a newer version would has been installed in the meantime, which might produce different results.\nMake it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet‚Äôs load the FastQC module:\nmodule load fastqc/0.11.8\nNow, we can retry our --help attempt:\nfastqc --help\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN  \n# [...truncated...]\n\nOn your own: load miniconda3\nThe miniconda3 module will allow us to use Conda software environments, which we‚Äôll talk about more below.\n\nLet‚Äôs start with a clean sheet by running module purge.\nLoad the default version of miniconda3, and then check which version was loaded.\n\n\n\n\n\n\n\nSolution (Click here)\n\n\n\n\n\nmodule load miniconda3\n\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/4.10.3-py37\nThe version 4.10.3-py37 was loaded.\n\n\n\n\nNow load the latest version of miniconda3 without unloading the earlier version first. What output do you get?\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\nLmod detected that you tried to load a different version of a software that was already loaded, so it changes the version and tells you about it:\nmodule load miniconda3/23.3.1-py310\nThe following have been reloaded with a version change:\n  1) miniconda3/4.10.3-py37 =&gt; miniconda3/23.3.1-py310"
  },
  {
    "objectID": "ref/ref_software.html#when-software-isnt-installed-at-osc",
    "href": "ref/ref_software.html#when-software-isnt-installed-at-osc",
    "title": "Software management",
    "section": "2 When software isn‚Äôt installed at OSC",
    "text": "2 When software isn‚Äôt installed at OSC\nIt‚Äôs not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than what is available. In that case, the following two are generally your best options:\n\nConda, which creates software environments that you can activate much like the Lmod modules.\nContainers, which are self-contained software environments that include operating systems, akin to mini virtual machines. While Docker containers are most well-known, OSC uses Apptainer (formerly known as Singularity) containers.\n\n\n\n\n\n\n\nOther options to install software / get it installed\n\n\n\n\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through Lmod / module commands).\n‚ÄúManually‚Äù install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to ‚Äúcompile‚Äù (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don‚Äôt have ‚Äúadministrator privileges‚Äù2 and will often have difficulties with ‚Äúdependencies‚Äù3.\n\n\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally for reproducible and portable software environments. They also allow you to easily maintain distinct ‚Äúenvironments‚Äù, each with a different version of the same software, or with mutually incompatible software.\nNext, we‚Äôll talk about Conda and using the MCIC‚Äôs Conda environments. The at-home reading includes installing software yourself with Conda, and using containers downloaded from the internet."
  },
  {
    "objectID": "ref/ref_software.html#intro-to-conda-using-mcics-conda-environments",
    "href": "ref/ref_software.html#intro-to-conda-using-mcics-conda-environments",
    "title": "Software management",
    "section": "3 Intro to Conda & using MCIC‚Äôs Conda environments",
    "text": "3 Intro to Conda & using MCIC‚Äôs Conda environments\nThe Conda software can create so-called environments in which one can install one or more software packages.\nAs you can see in the at-home reading below, as long as a program is available in one of the online Conda repositories (which is nearly always for bioinformatics programs), then installing it is quite straightforward, doesn‚Äôt require admin privileges, and is done with a procedure that is nearly identical regardless of the program you are installing.\nHowever, at OSC, you will probably not even have to install anything yourself, at least not if you are following ‚Äústandard‚Äù workflows with common data like RNAseq. To this end, I maintain an ‚ÄúMCIC collection‚Äù of Conda environments that anyone can use.\nA Conda environment is just a directory, and since all the environments in this collection are in the same place at OSC, you can list the MCIC Conda environments as follows:\nls /fs/ess/PAS0471/jelmer/conda\nabricate-1.0.1  bedops-2.4.39  checkm-1.2.0   entrez-direct    htseq-2.0.2          longstitch-1.0.3  nanopolish-0.13.2    prokka            repeatmasker-4.1.2.p1         samtools                star\nagat-0.9.1      bedtools       clinker        evigene          inspector-1.2.0      mafft             ncbi-datasets        pseudofinder      repeatmodeler-2.0.3           scoary                  subread-2.0.1\nalv             bioawk         clonalframeml  fastp            interproscan-5.55    maskrc-svg        nextdenovo-env       purge_dups-1.2.6  resfinder                     seqkit                  tgsgapcloser\namrfinderplus   biopython      codan-1.2      fastqc           iqtree               medaka-1.7.2      nextflow             pycoqc-2.5.2      resistomeanalyzer-2018.09.06  seqtk                   tracy-0.7.1\nantismash       bit            cogclassifier  fastq-dl         justorthologs-0.0.2  metaxa-2.2.3      orna-2.0             qiime2-2022.8     rgi-5.2.1                     signalp-6.0             transabyss-2.0.1\nariba-2.14.6    blast          cutadapt       fasttree-2.1.11  kallisto-0.48.0      minibusco         orthofinder          qualimap-env      r-metabar                     sistr-1.1.1             transdecoder-5.5.0\nastral-5.7.8    bowtie2-2.5.0  deeploc        filtlong-env     kat-2.4.2            minimap2-2.24     orthofisher          quast-5.0.2       rnaquast-2.2.1                smartdenovo-env         treetime\naswcli          bracken-2.6.1  deeptmhmm      flye-2.9.1       knsp-3.1             mlst              panaroo              quickmerge-env    roary-3.13                    snippy-4.6.0            trimgalore\nbactopia        braker2-env    deeptmhmm2     fmlrc2-0.1.7     kofamscan            mlst_check        phylofisher          racon-1.5.0       r-rnaseq                      snp-sites-2.5.1         trimmomatic-0.39\nbactopia-dev    busco          diamond        gcta             kraken2-2.1.2        mobsuite          pilon-1.24           ragtag-2.1.0      rsem-1.3.3                    soapdenovo-trans-1.0.4  trinity-2.13.2\nbakta           bwa-0.7.17     dwgsim         gffread-0.12.7   krakentools-1.2      multiqc           pkgs                 rascaf            rseqc-env                     sortmerna-env           unicycler\nbase            bwa-mem-2.2.1  eggnogmapper   gubbins          krona                mummer4           plasmidfinder-2.1.6  rcorrector-1.0.5  r_tree                        sourmash                virulencefinder\nbbmap           cactus         emboss         hisat2           liftoff-1.6.3        nanolyse-1.2.1    plink2               r-deseq           sabre-1.0                     spades-3.15.5           wtdbg-2.5\nbcftools        cgmlst         entap-0.10.8   hmmer            links-2.0.1          nanoplot          porechop             recognizer-1.8.3  salmon                        sra-tools\nThis is organized similarly to the Lmod modules in that there‚Äôs generally one separate environment for one program (and all its dependencies), and the environment is named after that program.\nThe naming of the environments is unfortunately not entirely consistent: many environments include the version number of the program, but many others do not. (Generally speaking, for environments without version numbers, you should expect the version of the program to be very recent, as I try to keep these up-to-date4).\nThis collection includes Conda environments for several programs we need during RNAseq analysis that are not installed at OSC, such as MultiQC, TrimGalore, and SortMeRNA.\n\n\n3.1 Activating Conda environments\nConda itself is already installed at OSC through Miniconda, but we always need to load its module before we can use it:\nmodule load miniconda3\nAs mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system. But whereas we use the term ‚Äúload‚Äù for Lmod modules, we use ‚Äúactivate‚Äù for Conda environments ‚Äî it means the same thing.\nAlso like Lmod, there is a main command (conda) and several subcommands (deactivate, create, install, update) for different functionality. However, for historical reasons, the most foolproof way to activate a Conda environment is to use source activate rather than the expected conda activate ‚Äî for instance:\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n(multiqc) [jelmer@p0085 rnaseq-intro]$\n\n\n\n\n\n\nConda environment indicator\n\n\n\nWhen we have an active Conda environment, its name is displayed in front of our prompt, as depicted above with (multiqc).\n\n\nAfter we have activated the MultiQC environment, we should be able to actually use the program. To test this, we‚Äôll simply run the multiqc command with the --help option like we did for FastQC:\n\nmultiqc --help\n\n /// MultiQC üîç | v1.15                                                                                                                                                                                                            \n                                                                                                                                                                                                                                   \n Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]                                                                                                                                                                                     \n                                                                                                                                                                                                                                   \n MultiQC aggregates results from bioinformatics analyses across many samples into a single report.                                                                                                                                 \n It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.                                                       \n To run, supply with one or more directory to scan for analysis results. For example, to run in the current working directory, use 'multiqc .'                                                                                     \n                                                                                                                                                                                                                                   \n‚ï≠‚îÄ Main options ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ --force            -f  Overwrite any existing reports                                                                                                                                                                           ‚îÇ\n‚îÇ --config           -c  Specific config file to load, after those in MultiQC dir / home dir / working dir. (PATH)                                                                                                                ‚îÇ\n‚îÇ --cl-config            Specify MultiQC config YAML on the command line (TEXT)                                                                                                                                                   ‚îÇ\n‚îÇ --filename         -n  Report filename. Use 'stdout' to print to standard out. (TEXT)                                                                                                                                           ‚îÇ\n‚îÇ --outdir           -o  Create report in the specified output directory. (TEXT)                                                                                                                                                  ‚îÇ\n‚îÇ --ignore           -x  Ignore analysis files (GLOB EXPRESSION)                                                                                                                                                                  ‚îÇ\n‚îÇ --ignore-samples       Ignore sample names (GLOB EXPRESSION)                                                                                                                                                                    ‚îÇ\n‚îÇ --ignore-symlinks      Ignore symlinked directories and files                                                                                                                                                                   ‚îÇ\n‚îÇ --file-list        -l  Supply a file containing a list of file paths to be searched, one per row                                                                                                                                ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n[...truncated...]\n\nUnlike Lmod / module load, Conda will by default only keep a single environment active. Therefore, when you have one environment activate and then activate another, you will switch environments:\n# After running this command, the multiqc env will be active\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# After running his command, the trimgalore env will be active...\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# ...but the multiqc env will no longer be:\nmultiqc --help\nbash: multiqc: command not found...\nHowever, the conda activate --stack option enables you to have multiple Conda environments active at once:\n# Assuming you had trimgalore activated, now add the multiqc env:\nconda activate --stack /fs/ess/PAS0471/jelmer/conda/multiqc\n\nmultiqc --help\n# (Output not shown, but this should print help info)\n\ntrim_galore --help\n# (Output not shown, but this should print help info)\nNote that the command is conda activate --stack and not source activate --stack!\n\n\n\n3.2 Lines to add to your shell script\nAs mentioned above for Lmod modules, you need to load them in every shell session you want to use them ‚Äî and the same is true for Conda environments. While Conda enviroments that are loaded in your interactive shell environment will ‚Äúcarry over‚Äù to the environment in which your script runs (even when you submit them to the Slurm queue with sbatch; topic of the next session), it is good practice to always include the necessary code to load/activate programs in your shell scripts.\nWhen the program you will run in a script is in an Lmod module, this only involves a module load call ‚Äî e.g., for FastQC:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load fastqc\nWhen the program you will run in a script is in a Conda environment, this entails a module load command to load Conda itself, followed by a source activate command to load the relevant Conda environment ‚Äî e.g.¬†for MultiQC:\n#!/bin/bash\n\n# Load software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# Strict/safe Bash settings \nset -euo pipefail\n\n\n\n\n\n\nPerils of Conda environments inside scripts\n\n\n\n\nIn the example above, the set -euo pipefail line was moved below the source activate command, because the Conda activation procedure can otherwise result in ‚Äúunbound variable‚Äù errors.\nAnother unfortunate aspect of Conda environments at OSC is the following. Problems can occur when you have a Conda environment active in your interactive shell while you submit a script as a batch job that activates a different environment.\nTherefore, it is generally a good idea to not have any Conda environments active in your interactive shell when submitting batch jobs5. To deactivate the currently active Conda environment, simply type conda deactivate without any arguments:\nconda deactivate"
  },
  {
    "objectID": "ref/ref_software.html#bonus-i-creating-your-own-conda-environments",
    "href": "ref/ref_software.html#bonus-i-creating-your-own-conda-environments",
    "title": "Software management",
    "section": "Bonus I: Creating your own Conda environments",
    "text": "Bonus I: Creating your own Conda environments\nWhen you want to create your own Conda environments and install programs, make sure to load the most recent miniconda3 module, which is currently not the default one. This is because installation has become much quicker and less likely to fail than in earlier versions. (Note that when we are just loading environments, like above, the version doesn‚Äôt matter).\nAs of August 2023, the most recent miniconda version is 23.3.1-py310 (recall that you can list available versions with module spider):\nmodule load miniconda3/23.3.1-py310\n\nOne-time Conda configuration\nBefore we can create our own environments, we first have to do some one-time configuration6. This will set the Conda ‚Äúchannels‚Äù (basically, software repositories) that we want to use when we install programs, including the relative priorities among channels (since one program may be available from multiple channels).\nWe can do this configuration with the config subcommand ‚Äî run the following commands in your shell:\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\nLet‚Äôs check whether the configuration was successfully saved:\nconda config --get channels\n--add channels 'defaults'   # lowest priority\n--add channels 'bioconda'\n--add channels 'conda-forge'   # highest priority\n\n\n\n3.3 Example: Creating an environment for Trim Galore!\nTo practice using Conda, we will now create a Conda environment with the program Trim Galore! installed. Trim Galore! is a commonly used tool for quality trimming and adapter trimming of FASTQ files ‚Äî we‚Äôll learn more about it in a later session, since we will use it on our RNAseq data. It does not have a system-wide installation at OSC, unfortunately.\nHere is the command to all at once create a new Conda environment and install Trim Galore! into that environment:\n\n# (Don't run this)\nconda create -y -n trim-galore -c bioconda trim-galore\n\nLet‚Äôs break that command down:\n\ncreate is the Conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation once Conda has determined what needs to be installed.\nFollowing the -n option, we can specify the name of the environment, so -n trim-galore means that we want our environment to be called trim-galore. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a ‚Äúchannel‚Äù (repository) from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we‚Äôve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe trim-galore at the end of the line simply tells Conda to install the package of that name. This is a ‚Äúpositional‚Äù argument to the command (note that there‚Äôs no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\nSpecifying a version\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name, and may also want to include that version number in the Conda environment‚Äôs name ‚Äî try running the command below:\nconda create -y -n trim-galore-0.6.10 -c bioconda trim-galore=0.6.10\nCollecting package metadata (current_repodata.json): done  \nSolving environment: done\n# [...truncated...]\n\n\n\n\n\n\nSee the full output when I ran this command (Click to expand)\n\n\n\n\n\n\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 23.3.1\n  latest version: 23.7.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.7.2\n\n\n\n## Package Plan ##\n\n  environment location: /fs/project/PAS0471/jelmer/conda/trimgalore-0.6.10\n\n  added / updated specs:\n    - trim-galore=0.6.10\n\n\nThe following packages will be downloaded:\n\n    | package            | build                                            |\n    | ------------------ | ------------------------------------------------ |\n    | bz2file-0.98       | py_0           9 KB  conda-forge                 |\n    | cutadapt-1.18      | py37h14c3975_1         206 KB  bioconda          |\n    | fastqc-0.12.1      | hdfd78af_0        11.1 MB  bioconda              |\n    | pigz-2.6           | h27826a3_0          87 KB  conda-forge           |\n    | python-3.7.12      | hf930737_100_cpython        57.3 MB  conda-forge |\n    | trim-galore-0.6.10 | hdfd78af_0          45 KB  bioconda              |\n    | xopen-0.7.3        | py_0          11 KB  bioconda                    |\n    ------------------------------------------------------------\n                                           Total:        68.8 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.9-hd590300_0 \n  bz2file            conda-forge/noarch::bz2file-0.98-py_0 \n  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 \n  ca-certificates    conda-forge/linux-64::ca-certificates-2023.7.22-hbcca054_0 \n  cairo              conda-forge/linux-64::cairo-1.16.0-hbbf8b49_1016 \n  cutadapt           bioconda/linux-64::cutadapt-1.18-py37h14c3975_1 \n  expat              conda-forge/linux-64::expat-2.5.0-hcb278e6_1 \n  fastqc             bioconda/noarch::fastqc-0.12.1-hdfd78af_0 \n  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-hab24e00_0 \n  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n  freetype           conda-forge/linux-64::freetype-2.12.1-hca18f0e_1 \n  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n  giflib             conda-forge/linux-64::giflib-5.2.1-h0b41bf4_3 \n  graphite2          conda-forge/linux-64::graphite2-1.3.13-h58526e2_1001 \n  harfbuzz           conda-forge/linux-64::harfbuzz-7.3.0-hdb3a94d_0 \n  icu                conda-forge/linux-64::icu-72.1-hcb278e6_0 \n  keyutils           conda-forge/linux-64::keyutils-1.6.1-h166bdaf_0 \n  krb5               conda-forge/linux-64::krb5-1.21.2-h659d440_0 \n  lcms2              conda-forge/linux-64::lcms2-2.15-haa2dc70_1 \n  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.40-h41732ed_0 \n  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n  libcups            conda-forge/linux-64::libcups-2.3.3-h4637d8d_4 \n  libdeflate         conda-forge/linux-64::libdeflate-1.18-h0b41bf4_0 \n  libedit            conda-forge/linux-64::libedit-3.1.20191231-he28a2e2_2 \n  libexpat           conda-forge/linux-64::libexpat-2.5.0-hcb278e6_1 \n  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n  libgcc-ng          conda-forge/linux-64::libgcc-ng-13.1.0-he5830b7_0 \n  libglib            conda-forge/linux-64::libglib-2.76.4-hebfc3b9_0 \n  libgomp            conda-forge/linux-64::libgomp-13.1.0-he5830b7_0 \n  libiconv           conda-forge/linux-64::libiconv-1.17-h166bdaf_0 \n  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-2.1.5.1-h0b41bf4_0 \n  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0 \n  libpng             conda-forge/linux-64::libpng-1.6.39-h753d276_0 \n  libsqlite          conda-forge/linux-64::libsqlite-3.42.0-h2797004_0 \n  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.1.0-hfd8a6a1_0 \n  libtiff            conda-forge/linux-64::libtiff-4.5.1-h8b53f26_0 \n  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n  libwebp-base       conda-forge/linux-64::libwebp-base-1.3.1-hd590300_0 \n  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n  libzlib            conda-forge/linux-64::libzlib-1.2.13-hd590300_5 \n  ncurses            conda-forge/linux-64::ncurses-6.4-hcb278e6_0 \n  openjdk            conda-forge/linux-64::openjdk-20.0.0-h8e330f5_0 \n  openssl            conda-forge/linux-64::openssl-3.1.2-hd590300_0 \n  pcre2              conda-forge/linux-64::pcre2-10.40-hc3806b6_0 \n  perl               conda-forge/linux-64::perl-5.32.1-4_hd590300_perl5 \n  pigz               conda-forge/linux-64::pigz-2.6-h27826a3_0 \n  pip                conda-forge/noarch::pip-23.2.1-pyhd8ed1ab_0 \n  pixman             conda-forge/linux-64::pixman-0.40.0-h36c2ea0_0 \n  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n  python             conda-forge/linux-64::python-3.7.12-hf930737_100_cpython \n  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n  setuptools         conda-forge/noarch::setuptools-68.0.0-pyhd8ed1ab_0 \n  sqlite             conda-forge/linux-64::sqlite-3.42.0-h2c6b66d_0 \n  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 \n  trim-galore        bioconda/noarch::trim-galore-0.6.10-hdfd78af_0 \n  wheel              conda-forge/noarch::wheel-0.41.1-pyhd8ed1ab_0 \n  xopen              bioconda/noarch::xopen-0.7.3-py_0 \n  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002 \n  xorg-inputproto    conda-forge/linux-64::xorg-inputproto-2.3.2-h7f98852_1002 \n  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.6-h8ee46fc_0 \n  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n  xorg-libxi         conda-forge/linux-64::xorg-libxi-1.7.10-h7f98852_0 \n  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.3.0-hd590300_1 \n  xorg-libxtst       conda-forge/linux-64::xorg-libxtst-1.2.3-h7f98852_1002 \n  xorg-recordproto   conda-forge/linux-64::xorg-recordproto-1.14.2-h7f98852_1002 \n  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5 \n  zstd               conda-forge/linux-64::zstd-1.5.2-hfc55251_7 \n\n\n\nDownloading and Extracting Packages\n                                                                                                                                                                                                                                   \nPreparing transaction: done                                                                                                                                                                                                        \nVerifying transaction: done                                                                                                                                                                                                        \nExecuting transaction: done                                                                                                                                                                                                        \n#                                                                                                                                                                                                                                  \n# To activate this environment, use                                                                                                                                                                                                \n#                                                                                                                                                                                                                                  \n#     $ conda activate trimgalore-0.6.10\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\n\n\n\nNow, you should be able to activate the enviroment (using just it‚Äôs name ‚Äì see the box below):\n# Activate the environment:\nsource activate trim-galore\n\n# Test if TrimGalore can be run - note, the command is 'trim_galore': \ntrim_galore --help\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nSpecifying the full path to the environment dir\n\n\n\nYou may have noticed above that we merely gave the enviroment a name (trim-galore or trim-galore-0.6.10), and did not tell it where to put this environment. Similarly, we were able to activate the environment with just its name. Conda assigns a personal default directory for its environments, somewhere in your Home directory.\nYou can install environments in a different location with the -p (instead of -n) option, for example:\nmkdir -p /fs/scratch/PAS0471/$USER/conda\nconda create -y -p /fs/scratch/PAS0471/$USER/conda/trim-galore -c bioconda trim-galore\nAnd when you want to load someone else‚Äôs Conda environments, you‚Äôll always have to specify the full path to environment‚Äôs dir, like you did when loading an MCIC Conda environment above.\n\n\n\n\n\n\n3.4 Finding the Conda installation info online\nMinor variations on the conda create command above can be used to install almost any program for which a Conda package is available, which is the vast majority of open-source bioinformatics programs!\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its Conda package‚Äôs name is\nWhich Conda channel we should use\nWhich versions are available\n\nMy strategy to finding this out is to simply Google the program name together with ‚Äúconda‚Äù, e.g.¬†‚Äúcutadapt conda‚Äù if I wanted to install the CutAdapt program. Let‚Äôs see that in action:\n\n\n\nClick on that first link (it should always be the first Google hit):\n\n\n\n\nBuild the installation command\nI always take the top of the two example installation commands as a template, which is here: conda install -c bioconda cutadapt.\nYou may notice the install subcommand, which we haven‚Äôt yet seen. This would install Cutadapt into the currently activated Conda environment. Since our strategy here ‚Äìand my general strategy‚Äì is to create a new environment each time you‚Äôre installing a program, just installing a program into whatever environment is currently active is not a great idea. To use the install command with a new environment, the strategy would be to first create an ‚Äúempty‚Äù environment, and then run the install command.\nHowever, we saw above that we can do all of this in a single command. To build this create-plus-install command, all we need to do is replace install in the example command on the Conda website by create -y -n &lt;env-name&gt;. Then, our full command (without version specification) will be:\nconda create -y -n cutadapt -c bioconda cutadapt\nTo see which version will be installed by default, and to see which older versions are available:\n\n\n\nFor almost any other program, you can use the exact same procedure to find the Conda package and install it!\n\n\n\n\n\n\nA few more Conda commands to manage your environments\n\n\n\n\nExport a plain-text ‚ÄúYAML‚Äù file that contains the instructions to recreate your currently-active environment (useful for reproducibility!)\nconda env export &gt; my_env.yml\nAnd you can use the following to create a Conda environment from such a YAML file:\nconda env create -n my_env --force --file my_env.yml\nRemove an environment entirely:\nconda env remove -n cutadapt\nList all your conda environments:\nconda env list\nList all packages (programs) installed in an environment ‚Äî due to dependencies, this can be a long list, even if you only actively installed one program:\nconda list -p /fs/ess/PAS0471/jelmer/conda/multiqc\n\n\n\n\n\n\n\n\n\nUse one environment per program (as here) or one per research project\n\n\n\nBelow are two reasonable ways to organize your Conda environments, and their respective advantages:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you‚Äôre working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nEven though it might seem easier, a third alternative, to simply install all programs across all projects in one single environment, is not recommended. This doesn‚Äôt benefit reproducibility, and your environment is likely to stop functioning properly sooner or later.\n(A side note: even when you want to install a single program, multiple programs are in fact nearly always installed: the programs that your target program depends on, i.e.¬†‚Äúdependencies‚Äù.)"
  },
  {
    "objectID": "ref/ref_software.html#bonus-ii-using-apptainer-containers",
    "href": "ref/ref_software.html#bonus-ii-using-apptainer-containers",
    "title": "Software management",
    "section": "Bonus II: Using Apptainer containers",
    "text": "Bonus II: Using Apptainer containers\nBesides Conda, containers are another way to use bioinformatics programs at OSC that don‚Äôt have system-wide installations.\nContainers are similar to Virtual Machines and different from Conda environments in that they come with an entire operating system. This makes creating your own container ‚Äúimage‚Äù (see box below on terminology) much more involved than creating a Conda environment, and we will not cover that here.\nHowever, there are pre-existing container images available for most bioinformatics programs, and they can be easily found, downloaded, and used.\n\n\n\n\n\n\nContainer terminology\n\n\n\n\nContainer image: File (Apptainer) or files (Docker) that contain the container application.\nContainer (sensu stricto): A running container image.\nDefinition file (Apptainer) / Dockerfile (Docker): A plain text file that contains the recipe to build a container image.\n\n\n\nAmong container platforms, Apptainer (formerly known as Singularity) and especially Docker are the most widely used ones. At supercomputers like OSC, however, only Apptainer containers can be used. Luckily, the Apptainer program can work with Docker container images: it will convert them on the fly.\n\nFinding container images online\nThere are several online repositories with publicly available container images, but I would recommend BioContainers https://biocontainers.pro/registry or Quay.io https://quay.io/biocontainers.\nFor example, let‚Äôs look on the BioContainers website for a TrimGalore container image:\n\n\n\n\nThe search result on the BioContainers website after entering ‚Äútrim galore‚Äù in the search box.\n\n\n\nClick on the only entry that is shown, trim-galore, which will get you to a page like this:\n\n\n\n\n\nAs you can see, this website also includes Conda installation instructions ‚Äî to see the container results, scroll down and you should see this:\n\n\n\n\nAfter scrolling down on the results page, you should see a recent available container image. Note that the command shown is singularity run, but we will use the more up-to-date apptainer run later.\n\n\n\nThe version tag that is shown (0.6.9--hdfd78af_0 above) pertains to the version of TrimGalore, but the result that is shown here is not will always the container image(s) with the most recent version. To see a list of all available images, click on the Packages and Containers tab towards the top, and then sort by Last Update:\n\n\n\n\nThe logo with the large S depicts Singularity/Apptainer containers.\n\n\n\nWhenever both a Singularity/Apptainer and a Docker image for the desired version of the program is available, use the Singularity/Apptainer image. This is because those don‚Äôt have to be converted, while Docker images do. But when the version you want is only available as a Docker image, that will work too: as mentioned above, it will be automatically converted to the proper format.\n\n\n\nRunning a container image\nWhen you‚Äôve found a container image that you want to use, copy its URL from the BioContainers website. For example, for the most recent TrimGalore version as of September 2023: https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0.\nYou could also copy the full command ‚Äî however, we will modify that in two ways:\n\nWe will use the more up-to-date apptainer command7\nWe‚Äôll use the exec subcommand instead of run, which allows us to enter a custom command to run in the container (the run subcommand would only run some preset default action, which is rarely useful for our purposes).\n\nAs such, our base command to run TrimGalore in the container will be as follows:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0\n# (Don't run this, we'll need to add a TrimGalore command)\n\n\n\n\n\n\nYou can‚Äôt use the Docker URL as-is\n\n\n\nIf you want to use a Docker container, the listed quasi-URL on BioContainers will start with ‚Äúquay.io‚Äù. In your apptainer exec command, you need to preface this URL with docker://. For instance:\napptainer exec docker://quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_0\n\n\nAfter the code above, we would finish our command by simply entering a TrimGalore command in the exact same way as we would when running TrimGalore outside of the context of a container. For example, to just print the help info like we‚Äôve been doing before, the TrimGalore command is:\ntrim_galore --help\nAnd to run that inside the container, our full command will be:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0 \\\n    trim_galore --help\nINFO:    Downloading network image\n321.4MiB / 321.4MiB [===================================================================================================================================] 100 % 3.0 MiB/s 0s\nWARNING: Environment variable LD_PRELOAD already has value [], will not forward new value [/apps/xalt/xalt/lib64/libxalt_init.so] from parent process environment\n\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nNote\n\n\n\n\nThe Apptainer/Singularity software does not need to be loaded at OSC, it is always automatically loaded.\nThe \\ in the code above allows us to continue a command on another line.\n\n\n\nSo, all that is different from running a program inside a container versus a locally installed program, is that you prefix apptainer exec &lt;URL&gt; when using a container.\nThe first time you run this command, the container will be downloaded, which can take a few minutes (by default it will be downloaded to ~/.apptainer/cache, but you can change this by setting the $APPTAINER_CACHEDIR environment variable). After that, the downloaded image will be used and the command should be executed about as instantaneously as when running TrimGalore outside of a container.\nYou will keep seeing the warning WARNING: Environment variable LD_PRELOAD [...] whenever you run a container, but this is nothing to worry about.\nFinally, the --help option above can also simply be replaced by a host of other TrimGalore options and arguments so as to actually trim a pair of FASTQ files, i.e.¬†with input and output files. You can just specify the paths to those files in the same way as without a container, this will work out of the box!\n\n\n\n\n\n\nWhen to use a Container versus Conda\n\n\n\n\nCurrently, my default is to first try installation with Conda. But I will try a container when installing a program through Conda fails, or my Conda environment misbehaves (e.g., memory errors with dumped cores).\nWhen you need multiple programs in quick succession or in a single command (e.g., you‚Äôre piping the output of one program into a second program), it can be more convenient to have those programs installed in a single environmnent or container. Pre-built multi-program containers are not as easy to find. And since building your own Conda environment is easier than building your own container, this is a situation where you might prefer Conda."
  },
  {
    "objectID": "ref/ref_software.html#footnotes",
    "href": "ref/ref_software.html#footnotes",
    "title": "Software management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, we call module the command and e.g.¬†spider the subcommand. But sometimes the subcommands are also simply called commands.‚Ü©Ô∏é\nWhen your personal computer asks you to ‚Äúauthenticate‚Äù while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC, you don‚Äôt have such privileges.‚Ü©Ô∏é\nOther software upon which the software that you are trying to install depends.‚Ü©Ô∏é\nIt isn‚Äôt feasible to keep separate environments around for many different versions of a program, mostly because Conda environments contain a very large number of files, and OSC has file number quotas. This is why I have in many cases chosen the strategy of just updating the version within the same environment.‚Ü©Ô∏é\nUnless you first deactivate any active environments in your script.‚Ü©Ô∏é\nThat is, these settings will be saved somewhere in your OSC home directory, and you never have to set them again unless you need to make changes.‚Ü©Ô∏é\nThough note that as of September 2023, the singularity command does still work, and it will probably continue to work for a while.‚Ü©Ô∏é"
  },
  {
    "objectID": "02_osc.html#a-computational-infrastructure-for-genomics",
    "href": "02_osc.html#a-computational-infrastructure-for-genomics",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 A computational infrastructure for genomics",
    "text": "1 A computational infrastructure for genomics\nA laptop or desktop computer is often not sufficient to work with large-scale genomics data. Additionally, many of the specialized programs that help you analyze your data can only be run through a ‚Äúcommand-line interface‚Äù.\nThose are some of the reasons that a typical computational infrastructure to do what we may call ‚Äúcommand-line genomics‚Äù involves the following components:\n\nA supercomputer ‚Äî in our case, the Ohio Supercomputer Center (OSC) [this session]\nA text editor ‚Äî I recommend and will demonstrate VS Code [this session]\nThe Unix shell (terminal) [homework and next session]\nR1 for interactive statistical analysis and visualization [homework and Thu + Fri]\n\nWe will be using all of these components during this workshop. This session will provide an introduction to supercomputers in general and to the Ohio Supercomputer Center (OSC) specifically. In all of the lab sessions at this workshop, we‚Äôll continue to work at OSC, so you will get a fair bit of experience with it."
  },
  {
    "objectID": "02_osc.html#high-performance-computing",
    "href": "02_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a ‚Äúcompute cluster‚Äù or simply a ‚Äúcluster‚Äù) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you‚Äôre working with genomics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\n\nOSC websites and ‚ÄúProjects‚Äù\n\n\n\nOSC has three main websites ‚Äî we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC‚Äôs computing power and storage space goes through OSC ‚ÄúProjects‚Äù:\n\nA project can be tied to a research project or lab, or be educational like this course‚Äôs project, PAS2714.\nEach project has a budget in terms of ‚Äúcompute hours‚Äù and storage space2.\nAs a user, it‚Äôs possible to be a member of multiple different projects."
  },
  {
    "objectID": "02_osc.html#the-structure-of-a-supercomputer-center",
    "href": "02_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet‚Äôs start with some (super)computing terminology, going from smaller things to bigger things:\n\nCore / Processor / CPU / Thread\nComponents of a computer (node) that can each (semi-)indendepently be asked to perform a computing task like running a bioinformatics program. For our purposes, we can treat these terms as synonyms.\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: ‚ÄúPitzer‚Äù and ‚ÄúOwens‚Äù.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nOne for each‚Ä¶\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nOSC Project\n\n\n\nDuring the course, we will be working in the project directory of the course‚Äôs OSC Project PAS2714: /fs/ess/PAS2714. (We‚Äôll talk more about these different file systems in week 5.)\n\n\n\n\n\n\nDirectory is just another word for folder, often written as ‚Äúdir‚Äù for short\n\n\n\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and cannot be ‚Äúreserved‚Äù.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a ‚Äújob‚Äù). The Slurm job scheduler, which we will learn to use in week 5, will then assign resources to your request.\n\n\n\n\n\n\nCompute node types\n\n\n\nCompute nodes come in different shapes and sizes. Standard, default nodes work fine for the vast majority of analyses, even with large-scale omics data. But you will sometimes need non-standard nodes, such as when you need a lot of RAM memory or need GPUs3.\n\n\n\n\n\n\n\n\nSide note: What works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We‚Äôll learn much more about these later on in the course, but here is an overview:\n\n‚ÄúNon-interactive‚Äù computing is common\nIt is common to write and ‚Äúsubmit‚Äù scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can‚Äôt install ‚Äúthe regular way‚Äù, and a lot of installed software needs to be ‚Äúloaded‚Äù.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to ‚Äúcompute nodes‚Äù to run most analyses."
  },
  {
    "objectID": "02_osc.html#osc-ondemand",
    "href": "02_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n‚ÄúInteractive Apps‚Äù: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2714, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2714 project‚Äôs ‚Äúscratch‚Äù directory (/fs/scratch/PAS2714)\nThe PAS2714 project‚Äôs ‚Äúproject‚Äù directory (/fs/ess/PAS2714)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n In the Files dropdown menu, click on our focal directory /fs/ess/PAS2714.\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files4 ‚Äî see the buttons across the top.\n\n\n\n\n\n\nDidn‚Äôt create your own dir in the Unix shell homework? Click here and follow these instructions to do it now.\n\n\n\n\n\n\nClick on the users dir in /fs/ess/PAS714.\nCreate your own dir by clicking the New Directory button towards the top.\nPlease give it the exact same name as your OSC username (also match the capitalization!).\n\n(If you‚Äôre not sure what your username is ‚Äî look at the right side of the blue top bar, ‚ÄúLogged in as‚Äù:)\n\n\n\n\n\n\n\n\n\n\n\n4.2 Clusters: Unix shell access\n\n\n\n\n\n\nSystem Status within Clusters (Click to expand)\n\n\n\n\n\nIn the ‚ÄúClusters‚Äù dropdown menu, click on the item at the bottom, ‚ÄúSystem Status‚Äù:\n\n\n\n\n\nThis page shows an overview of the live, current usage of the two clusters ‚Äî that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the ‚Äúqueue‚Äù (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\n\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI‚Äôm selecting a shell on the Pitzer supercomputer (‚ÄúPitzer Shell Access‚Äù), which will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nHowever, from now on, we‚Äôll be accessing a Unix shell inside the VS Code text editor, which also gives us some additional functionality in a user-friendly way.\n\n\n\n4.3 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\nSelect VS Code using the ‚ÄúCode Server‚Äù button:\n\n\n\n\n\n\n\nInteractive Apps like VS Code and RStudio run on compute nodes (not login nodes). Because compute nodes always need to be ‚Äúreserved‚Äù, we have to fill out a form and specify the following details:\n\nThe OSC ‚ÄúProject‚Äù that we want to bill for the compute node usage: PAS2714.\nThe ‚ÄúNumber of hours‚Äù we want to make a reservation for: 3\nThe ‚ÄúWorking Directory‚Äù for the program: your personal folder in /fs/ess/PAS2714/users.\nThe ‚ÄúCodeserver Version‚Äù: 4.8 (most recent)\n\n\n\n\n\n\n\n\nClick on Launch at the bottom, which will send your request to the ‚Äúcompute job‚Äù scheduler.\nFirst, your job will be ‚ÄúQueued‚Äù ‚Äî that is, waiting for the job scheduler to allocate compute node resources to it:\n\n\n\n\n\n\n\nYour job is typically granted resources within a few seconds (the card will then say ‚ÄúStarting‚Äù), and should be ready for usage (‚ÄúRunning‚Äù) in another couple of seconds:\n\n\n\n\n\n\n\nOnce it appears, click on the blue Connect to VS Code button to open VS Code in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) ‚Äî click ‚ÄúYes‚Äù (and check the box) and ‚ÄúDon‚Äôt Show Again‚Äù, respectively:"
  },
  {
    "objectID": "02_osc.html#vs-code",
    "href": "02_osc.html#vs-code",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "5 VS Code",
    "text": "5 VS Code\n\n5.1 What is VS Code?\nVS Code is basically a fancy text editor. Its full name is Visual Studio Code, but at OSC, it is also referred to as ‚ÄúCode Server‚Äù.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as ‚ÄúIDEs‚Äù: Integrated Development Environments. The RStudio program is another good example of an IDE. For our purposes:\n\nVS code will be our IDE for Unix shell code\nRStudio will be our IDE for R\n\n\n\n\n5.2 The VS Code User Interface\n\n\n\n\n\n\nSide bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA  (‚Äúhamburger menu‚Äù), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle (wide) Side Bar options ‚Äî but we‚Äôll only use the default selection, the Explorer (file browser)\n\n\n\nEditor pane and Welcome document\nThe main part of the VS Code is the editor pane. Here, we can open files like scripts and other types of text files, and images. (Whenever you open VS Code, an editor tab with a Welcome document is automatically opened. This provides some help and some shortcuts like to recently opened files and folders.)\n\n\nTerminal (with a Unix shell)\n Open a terminal by clicking ¬†  ¬† =&gt; Terminal =&gt; New Terminal.\n\n\n\n Exercise: Try a few color themes\n\nAccess the ‚ÄúColor Themes‚Äù option by clicking  =&gt; Color Theme.\nTry out a few themes and see pick one you like!\n\n\n\n\n\n5.3 A folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\n(If you need to switch folders, click ¬†  ¬† =&gt; ¬† File ¬† =&gt; ¬† Open Folder.)\n\n\n\n\n\n\n\nSome VS Code tips and tricks\n\n\n\n\n\n\nResizing panes\nYou can resize panes (the terminal, editor, and side bar) by hovering your cursor over the borders and then dragging.\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called ‚ÄúCommand Palette‚Äù can be handy, especially if you know what you are looking for. To access the Command Palette, click ¬†  ¬† and then Command Palette (or press F1 or Ctrl/‚åò+Shift+P).\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system: ¬†  ¬† =&gt; ¬† Help ¬† =&gt; ¬† Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.) A couple of useful keyboard shortcuts are highlighted below.\n\n\n\n\n\n\n\n\n\n\nSpecific useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nWorking with keyboard shortcuts for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, in some case, you‚Äôll have to replace Ctrl with ‚åò):\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nToggle the (wide) Side Bar: Ctrl+B\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+‚¨Ü/‚¨á will move lines up or down."
  },
  {
    "objectID": "02_osc.html#further-reading",
    "href": "02_osc.html#further-reading",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Further reading",
    "text": "Further reading\n\nOSC‚Äôs learning resources\n\nAn extended version of this introduction\nOSC‚Äôs online asynchronous courses\nOSC‚Äôs new User Resource Guide5"
  },
  {
    "objectID": "02_osc.html#footnotes",
    "href": "02_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr Python‚Ü©Ô∏é\nBut we don‚Äôt have to pay anything for educational projects like this one. Otherwise, for OSC‚Äôs rates for academic research, see this page.‚Ü©Ô∏é\nGPUs are e.g.¬†used for Nanopore basecalling‚Ü©Ô∏é\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available ‚Äî we‚Äôll talk about those later on.‚Ü©Ô∏é\n Attribution: This page uses material from an OSC Introduction written by Mike Sovic and from OSC‚Äôs Kate Cahill Software Carpentry introduction to OSC.‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for an Ohio State University workshop on Amplicon Metabarcoding, held from March 13-15, 2024 in Wooster, Ohio.\nThis workshop is co-organized by:\n\nDr.¬†Soledad Benitez-Ponce, Dept. of Plant Pathology (Personal website)\nDr.¬†Timothy Frey, Dept. of Plant Pathology\nFiama Guevara, Dept. of Plant Pathology\nMelanie Medina Lopez, Dept. of Plant Pathology\nDr.¬†Jelmer Poelstra, Molecular and Cellular Imaging Center (MCIC)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "homework/shell.html#why-the-unix-shell",
    "href": "homework/shell.html#why-the-unix-shell",
    "title": "Homework: Intro to the Unix Shell",
    "section": "1 Why the Unix shell?",
    "text": "1 Why the Unix shell?\nMany of the things you typically do by pointing and clicking can alternatively be done by typing commands. The Unix shell allows you to interact with computers via commands.\nHere are some reasons why you may want to use this seemingly archaic technique:\n\nWorking efficiently with large files\nMaking it easier to repeat (& automate) similar tasks across files, samples, and projects\nAchieving better reproducibility in research\nAt least in bioinformatics, being able to use access the largest and most recent set of approaches and all their options ‚Äî many graphical user interface programs lag behind in functionality and may cost money as well.\nWorking effectively with remote high-performance computing like at the Ohio Supercomputer Center (OSC)\n\n\n\n\n\n\n\nSide note: Some Unix shell terminology (Click to expand)\n\n\n\n\n\nHere are a few interrelated terms you‚Äôre likely to run across:\n\nCommand Line ‚Äî the most general term, an interface1 where you type commands\nTerminal ‚Äî the program/app/window that can run a Unix shell\nShell ‚Äî a command line interface to your computer\nUnix Shell ‚Äî the types of shells on Unix family (Linux + Mac) computers\nBash ‚Äî the specific Unix shell language that is most common on Unix computers\n\nWhile you‚Äôve seen that these are not all synonyms, in day-to-day computing/bioinformatics, they are often used interchangeably."
  },
  {
    "objectID": "homework/shell.html#how-to-go-through-this-page",
    "href": "homework/shell.html#how-to-go-through-this-page",
    "title": "Homework: Intro to the Unix Shell",
    "section": "2 How to go through this page",
    "text": "2 How to go through this page\nYou will be using a Unix shell at the Ohio Supercomputer Center (OSC) ‚Äî see the instructions below to open one.\nPlease follow along actively by typing and executing all commands shown below (unless it explicitly says you shouldn‚Äôt run something), not just the section that are labeled as ‚Äúexercises‚Äù. If you skip certain commands, later ones will in many cases not work.\n\nOpening a Unix shell at OSC\n\nLog in to OSC‚Äôs OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, click on the ‚ÄúClusters‚Äù dropdown menu and then click Pitzer Shell Access.\nA Unix shell will open in a new browser tab (see screenshot below). You‚Äôre ready to go!\n\n\n\n\n\n\n\n\n\nUsing this shell\n\n\n\nYou can‚Äôt right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail ‚Äî if the latter, click on the clipboard icon in your browser‚Äôs address bar (see red circle in screenshot below):\n\n\nYou may also want to change the shell‚Äôs color scheme by selecting an option other than ‚ÄúDefault‚Äù in the ‚ÄúThemes:‚Äù dropdown menu in the top-right."
  },
  {
    "objectID": "homework/shell.html#the-basics",
    "href": "homework/shell.html#the-basics",
    "title": "Homework: Intro to the Unix Shell",
    "section": "3 The basics",
    "text": "3 The basics\n\n3.1 The prompt\nInside your terminal, the ‚Äúprompt‚Äù indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-directory&gt;]$\nFor example (note that ~ means your Home directory/folder):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we‚Äôll get our prompt back and can type a new command.\n\n\n\n\n\n\n‚ÄúDirectory‚Äù (or ‚Äúdir‚Äù) for short is Unix-speak for a computer ‚Äúfolder‚Äù\n\n\n\n\n\n\n\n\n\n3.2 A few simple commands: date, whoami, pwd\nThe Unix shell comes with hundreds of ‚Äúcommands‚Äù: small programs that perform specific actions. If you‚Äôre familiar with R or Python, a Unix command is like an R/Python function.\nLet‚Äôs start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nTue Mar 5 09:11:51 EST 2024\nThe whoami (who-am-i) command prints your username:\nwhoami\njelmer\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n# [Yours will be different! You are in your Home directory.]\n\nAll 3 of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n\n\n\nWorking directory and paths (we‚Äôll talk more about paths later)\n\n\n\n\nWhen working in a Unix shell, you are always ‚Äúin‚Äù a specific directory: your working directory (‚Äúworking dir‚Äù for short).\nIn a path (= location of a file or directory) such as that output by pwd, directories are separated by forward slashes /.\n\n\n\n\n\n\n\n\n\nCase and spaces\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names.\nAvoid spaces in file and directory names!2 Use e.g.¬†underscores to distinguish words (my_long_filename).\n\n\n\n\n\n\n3.3 cd and command actions & arguments\nIn the above three command line expressions:\n\nWe merely typed a command and nothing else\nThe command provided some information, which was printed to screen\n\nBut many commands perform an action other than providing information. For example, you can use the command cd to Change Directory (i.e.¬†change your working dir). And like many commands that perform an action, cd normally has no output at all.\nLet‚Äôs use cd to move to another directory by specifying the path to that directory after the cd command:\ncd /fs/ess/PAS2714\npwd\n/fs/ess/PAS2714\nIn more abstract terms, what we did above was to provide cd with an argument, namely the path of the dir to move to. Arguments generally tell commands what file(s) or directory/ies to operate on.\nAs we‚Äôve seen, then, cd gives no output when it successfully changed the working directory. But let‚Äôs also see what happens when it does not succeed ‚Äî it gives an error:\ncd /fs/ess/PAs2714\nbash: cd: /fs/ess/PAs2714: No such file or directory\n\n\nWhat was the problem with the path we specified? (Click to see the answer)\n\nWe used a lowercase ‚Äús‚Äù in /PAs2714/ ‚Äî this should have been /PAS2714/.\nAs pointed out above, everything, including paths, is case-sensitive in the Unix shell!\n\n\n\n\n3.4 ls and command options\n\nThe default behavior of ls\nThe ls command, short for ‚Äúlist‚Äù, will list files and directories:\nls\nsandbox   share   users\n(You should still be in /fs/ess/PAS2714. If not, cd there first.)\n\n\n\n\n\n\nSide note: ls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell ‚Äî the most common ones are:\n\nEntries in blue are directories (like data and metadata above)\nEntries in black are regular files (like README.md above)\nEntries in red are compressed files (we‚Äôll see an example soon).\n\n\n\n\nBy default, ls will list files and dirs in your current working dir, and in the way shown above. For which dir ls lists files and dirs can be changed with arguments, and how ls shows the output can be changed with options.\n\n\n\nOptions\nIn general, whereas arguments tell a command what to operate on, options will modify its behavior. For example, we can call ls with the option -l (a dash followed by a lowercase L):\nls -l \ntotal 2\ndrwxr-xr-x+ 2 jelmer PAS0471 4096 Mar  1 16:23 sandbox\ndrwxr-xr-x+ 4 jelmer PAS0471 4096 Mar  1 16:13 share\ndrwxrwxrwx+ 3 jelmer PAS0471 4096 Mar  1 16:19 users\nNotice that it lists the same items as above, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet‚Äôs add another option, -h:\nls -l -h\ntotal 1.5K\ndrwxr-xr-x+ 2 jelmer PAS0471 4.0K Mar  1 16:23 sandbox\ndrwxr-xr-x+ 4 jelmer PAS0471 4.0K Mar  1 16:13 share\ndrwxrwxrwx+ 3 jelmer PAS0471 4.0K Mar  1 16:19 users\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have ‚ÄúHuman-readable filesizes‚Äù (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\nConveniently, options can be ‚Äúpasted together‚Äù as follows:\nls -lh\n# (Output not shown, same as above)\n\n\n\nArguments\nArguments to ls should be dirs or files to operate on. For example, if we wanted to see what‚Äôs inside the share dir, instead of inside our working dir, we could type3:\nls share\ndata  README.md  results\n\n\n\nIntermezzo: file viewing and a quick intro to the dataset\nTo find out what data is contained in this dir, let‚Äôs take a look at the README.md file, which provides some information about the data set we will work with during the workshop.\nThere are several commands to view the contents of files ‚Äî the simplest is cat, which will print the entire contents of a file to screen:\ncat share/README.md\nThis 16S amplicon metabarcoding data set compares soil bacterial populations\nunder two different rotational schemes (corn-soy) vs (corn-soy-wheat) at\ntwo research farms in Ohio (Northwest Agricultural Research Station(NW) and Western Agricultural Research Station (W)).\nThere are 32 plots (Ex: 102A) in four blocks (100-400).\nPlots were split into A and BC plots to include a cover crop treatment.\nThe head command will only print the first 10 lines of a file. Let‚Äôs use that to examine this dataset‚Äôs metadata file:\nhead share/data/meta/meta.tsv\nSampleID        Location        Rotation        Plot    Block\nNW102AB NWARS   CS      102AB   100\nNW102C  NWARS   CS      102C    100\nNW103AB NWARS   CSW     103AB   100\nNW103C  NWARS   CSW     103C    100\nNW201AB NWARS   CSW     201AB   200\nNW201C  NWARS   CSW     201C    200\nNW203A  NWARS   CS      203A    200\nNW203BC NWARS   CS      203BC   200\nNW304A  NWARS   CSW     304A    300\n\n\nLet‚Äôs dig a little deeper and check the share/data dir:\nls share/data\nfastq  meta  ref\nThe data dir appears to contain three (sub)dirs with different kinds of data. We‚Äôll talk in detail about that later, but for now let‚Äôs look inside the fastq dir:\nls share/data/fastq\nNW102AB_R1.fastq.gz  NW201C_R1.fastq.gz   NW305AB_R1.fastq.gz  NW404BC_R1.fastq.gz  W204A_R1.fastq.gz   W303C_R1.fastq.gz   W404A_R1.fastq.gz\nNW102AB_R2.fastq.gz  NW201C_R2.fastq.gz   NW305AB_R2.fastq.gz  NW404BC_R2.fastq.gz  W204A_R2.fastq.gz   W303C_R2.fastq.gz   W404A_R2.fastq.gz\nNW102C_R1.fastq.gz   NW203A_R1.fastq.gz   NW305C_R1.fastq.gz   W101AB_R1.fastq.gz   W204BC_R1.fastq.gz  W304AB_R1.fastq.gz  W404BC_R1.fastq.gz\nNW102C_R2.fastq.gz   NW203A_R2.fastq.gz   NW305C_R2.fastq.gz   W101AB_R2.fastq.gz   W204BC_R2.fastq.gz  W304AB_R2.fastq.gz  W404BC_R2.fastq.gz\nNW103AB_R1.fastq.gz  NW203BC_R1.fastq.gz  NW403A_R1.fastq.gz   W101C_R1.fastq.gz    W205A_R1.fastq.gz   W304C_R1.fastq.gz\nNW103AB_R2.fastq.gz  NW203BC_R2.fastq.gz  NW403A_R2.fastq.gz   W101C_R2.fastq.gz    W205A_R2.fastq.gz   W304C_R2.fastq.gz\nNW103C_R1.fastq.gz   NW304A_R1.fastq.gz   NW403BC_R1.fastq.gz  W103AB_R1.fastq.gz   W205BC_R1.fastq.gz  W403AB_R1.fastq.gz\nNW103C_R2.fastq.gz   NW304A_R2.fastq.gz   NW403BC_R2.fastq.gz  W103AB_R2.fastq.gz   W205BC_R2.fastq.gz  W403AB_R2.fastq.gz\nNW201AB_R1.fastq.gz  NW304BC_R1.fastq.gz  NW404A_R1.fastq.gz   W103C_R1.fastq.gz    W303AB_R1.fastq.gz  W403C_R1.fastq.gz\nNW201AB_R2.fastq.gz  NW304BC_R2.fastq.gz  NW404A_R2.fastq.gz   W103C_R2.fastq.gz    W303AB_R2.fastq.gz  W403C_R2.fastq.gz\nAh, FASTQ files! These contain our sequence data (the reads from the Illumina sequencer), and we‚Äôll go and explore them in a bit.\n\n\nCombining options and arguments\nWe‚Äôll combine options and arguments to take a closer look at our dir with FASTQ files ‚Äî now the -h option is especially useful and allows us to see that the FASTQ files are around 2-3 Mb in size:\nls -lh share/data/fastq\ntotal 150M\n-rw-r-----+ 1 jelmer PAS0471 2.0M Mar  1 11:24 NW102AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 11:24 NW102AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 11:24 NW102C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.0M Mar  1 11:24 NW102C_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 1.9M Mar  1 11:24 NW103AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 11:24 NW103AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 11:24 NW103C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.1M Mar  1 11:24 NW103C_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 1.9M Mar  1 11:24 NW201AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.5M Mar  1 11:24 NW201AB_R2.fastq.gz\n# [...output truncated...]\n\n\n\n\n\n\nWhy so small?\n\n\n\nThe FASTQ files are so small because we‚Äôve ‚Äúsubsampled‚Äù them: these only contain 10% of the reads of the original files. This will allow us to do the demonstrational analyses in the workshops more rapidly.\n\n\n\n\n\n Exercise: Listing files\nList the files in the share/data/ref dir:\n\nWhat is the file size?\nDo you know what kind of file this is?\n\n\n\nClick for the solution\n\nls -lh share/data/ref\ntotal 131M\n-rwxr--r-- 1 jelmer PAS2714 131M Feb 27 11:53 silva_nr99_v138.1_train_set.fa.gz\n\nThe file is 131 Mb large.\nThis is a FASTA file with nucleotide sequences (hence the extension .fa), which has been compressed (hence the extension .gz).\n\n\n\n\n\n\n3.5 Miscellaneous tips\n\nCommand history: If you hit the ‚áß (up arrow) once, you‚Äôll retrieve your most recent command, and if you keep hitting it, you‚Äôll go further back. The ‚á© (down arrow) will go the other way: towards the present.\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nTab completion: file paths can Tab-complete! Try to type a partial path and test it. If you‚Äôre not getting it to work, it might be worth Googling this feature and watching a demo video.\nAny text that comes after a # is considered a comment instead of code!\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/fs/ess/PAS2714\n\n\n\nIf your prompt is ‚Äúmissing‚Äù, the shell is still busy executing your command, or you typed an incomplete command. To abort in either of these scenarios, press Ctrl+C and you will get your prompt back.\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n("
  },
  {
    "objectID": "homework/shell.html#paths-and-environment-variables",
    "href": "homework/shell.html#paths-and-environment-variables",
    "title": "Homework: Intro to the Unix Shell",
    "section": "4 Paths and environment variables",
    "text": "4 Paths and environment variables\n\n4.1 Paths\n\nAbsolute (full) paths versus relative paths\n\nAbsolute (full) paths (e.g.¬†/fs/ess/PAS2714)\nPaths that begin with a / always start from the computer‚Äôs root directory, and are called ‚Äúabsolute paths‚Äù.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nRelative paths (e.g.¬†data/fastq)\nPaths that instead start from your current working directory are called ‚Äúrelative paths‚Äù.\n(These work like directions along the lines of ‚Äútake the second left:‚Äù they depend on your current location.)\n\n# Move into the 'PAS2714' dir with an absolute path:\ncd /fs/ess/PAS2714\n\n# Then, move into the 'share/data' dir with a relative path:\ncd share/data                   # Absolute path is /fs/ess/PAS2714/share/data\n\n\nPath shortcuts\n\n~ (a tilde) ‚Äî represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) ‚Äî represents the current working directory.\n.. (two periods) ‚Äî Represents the directory ‚Äúone level up‚Äù, i.e.¬†towards the computer‚Äôs root dir.\n\n# (You should be in /fs/ess/PAS2714/share/data)\nls ..              # One level up, listing /fs/ess/PAS2714/share\ndata  README.md  results\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\nls ../..            # Two levels up, listing /fs/ess/PAS2714\nsandbox  share  users\n\n\n\n\n\n\nThese shortcuts work with all commands\n\n\n\nAll of the above shortcuts (., .., ~) are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise: Path shortcuts\n\nA) Use relative paths to move up to /fs/ess/PAS2714 and back to share/data once again.\n\n\n\n(Click for the solution)\n\ncd ../..\ncd share/data\n\n\nB) List the files in your Home dir without moving there.\n\n\n\n(Click for the solution)\n\nls ~\n# (Output not shown, will vary from person to person)\n\n\n\n\n\n4.2 Environment variables\nYou are likely familiar with the concept of variables in either the Unix shell, R, or another language.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell4.\nYou need the echo command, a general command to print text, to print the value of $x (cf.¬†in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to the Unix shell\"\nWelcome to the Unix shell\n\n\n\nEnvironment variables are pre-existing variables that have been assigned values automatically. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nB) Print ‚ÄúHello there, &lt;your username&gt;‚Äù (e.g.¬†‚ÄúHello there, marcus‚Äù) to the screen:\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello there $USER\"\nHello there jelmer"
  },
  {
    "objectID": "homework/shell.html#managing-files-and-dirs",
    "href": "homework/shell.html#managing-files-and-dirs",
    "title": "Homework: Intro to the Unix Shell",
    "section": "5 Managing files and dirs",
    "text": "5 Managing files and dirs\n\n5.1 Create dirs with mkdir\nThe mkdir command creates new directories. For example, to create your own dir within /fs/ess/PAS2714:\ncd /fs/ess/PAS2714/users\n\nmkdir $USER\nLet‚Äôs move into our newly created dir and create two directories at once:\ncd $USER\n\nmkdir scripts sandbox\nLet‚Äôs check what we did:\nls\nsandbox  scripts\n\n\n\n\n\n\n\nConfused by $USER?\n\n\n\nInstead of $USER, you can also type your literal username. If you do that, make sure that you get your username exactly right, including any capitalization. For example, I (username jelmer) could have run the following commands instead of the ones above with $USER:\nmkdir jelmer\ncd jelmer\n\n\n\n\n\n\n\n\nSide note: Recursive mkdir with -p (Click to expand)\n\n\n\n\n\nBy default, mkdir does not work recursively: that is, it will refuse to make a dir inside a dir that does not yet exist. And if you try to do so, the resulting error might confuse you:\nmkdir sandbox/2024/02/07\nmkdir: cannot create directory ‚Äòsandbox/2024/02/07‚Äô: No such file or directory\n\nWhy won‚Äôt you do your job, mkdir!? üò°\n\nInstead, we need to use the -p option to mkdir:\nmkdir -p sandbox/2024/02/07\nThe -p option also changes mkdir‚Äôs behavior when you try to create a dir that already exists. Without -p that will result in an error, and with -p it doesn‚Äôt complain about that (and it won‚Äôt recreate/overwrite the dir either).\n\n\n\n\n\n\n5.2 Copy files and dirs with cp\nAbove, you created your own directory ‚Äî now, let‚Äôs get you a copy of the data we saw in the data dir.\nThe cp command copies files and/or directories from one location to another. It has two required arguments: what you want to copy (the source), and where you want to copy it to (the destination). We can summarize its basic syntax as cp &lt;source&gt; &lt;destination&gt;.\nLet‚Äôs start by copying a single file twice:\n# You should be in /fs/ess/PAS2714/users/$USER/\n\n# Only provide a dir as the destination =&gt; Don't change the file name:\ncp /fs/ess/PAS2714/sandbox/testfile.txt sandbox/\n\n# Provide a file name as the destination =&gt; Give the copy a new name:\ncp /fs/ess/PAS2714/sandbox/testfile.txt sandbox/testfile_mycopy.txt\n\n# Check the files we created:\nls sandbox\ntestfile_mycopy.txt  testfile.txt\n\ncp is not recursive by default, so if you want to copy a directory and all of its contents, you need to use its -r option. We‚Äôll use that option to copy the dir with FASTQ files:\ncp -rv /fs/ess/PAS2714/share/data /fs/ess/PAS2714/users/$USER/\n‚Äò/fs/ess/PAS2714/share/data‚Äô -&gt; ‚Äò./data‚Äô\n‚Äò/fs/ess/PAS2714/share/data/meta‚Äô -&gt; ‚Äò./data/meta‚Äô\n‚Äò/fs/ess/PAS2714/share/data/meta/meta.tsv‚Äô -&gt; ‚Äò./data/meta/meta.tsv‚Äô\n‚Äò/fs/ess/PAS2714/share/data/ref‚Äô -&gt; ‚Äò./data/ref‚Äô\n‚Äò/fs/ess/PAS2714/share/data/ref/silva_nr99_v138.1_train_set.fa.gz‚Äô -&gt; ‚Äò./data/ref/silva_nr99_v138.1_train_set.fa.gz‚Äô\n‚Äò/fs/ess/PAS2714/share/data/fastq‚Äô -&gt; ‚Äò./data/fastq‚Äô\n‚Äò/fs/ess/PAS2714/share/data/fastq/W404A_R2.fastq.gz‚Äô -&gt; ‚Äò./data/fastq/W404A_R2.fastq.gz‚Äô\n‚Äò/fs/ess/PAS2714/share/data/fastq/NW203A_R2.fastq.gz‚Äô -&gt; ‚Äò./data/fastq/NW203A_R2.fastq.gz‚Äô\n‚Äò/fs/ess/PAS2714/share/data/fastq/W205BC_R2.fastq.gz‚Äô -&gt; ‚Äò./data/fastq/W205BC_R2.fastq.gz‚Äô\n# [...output truncated...]\n\n\n\n\n\n\nAbove we also used the -v option, short for verbose, to make cp tell us what it did\n\n\n\n\n\n\nWe can also get a nice recursive overview of all our files with tree:\ntree -C                 # '-C' for colors, not visible on this site though\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ fastq\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ NW102AB_R1.fastq.gz\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ NW102AB_R2.fastq.gz\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ NW102C_R1.fastq.gz\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ NW102C_R2.fastq.gz\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ NW103AB_R1.fastq.gz\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ NW103AB_R2.fastq.gz\n        ‚îú‚îÄ‚îÄ [...Other FASTQ files not shown...]\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ meta\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ meta.tsv\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ref\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ silva_nr99_v138.1_train_set.fa.gz\n‚îú‚îÄ‚îÄ sandbox\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ testfile_mycopy.txt\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ testfile.txt\n‚îî‚îÄ‚îÄ scripts\n\n\n\n5.3 Move with mv, and cp/mv tips\nThe mv command is nearly identical to the cp command, except that it:\n\nMoves rather than copies files and/or dirs\nWorks recursively by default\n\nThere is no separate renaming command, as both cp and mv allow you to provide a different name for the target.\nLet‚Äôs start by moving the testfile.txt into our current working dir:\nmv sandbox/testfile.txt .\nAnd we can move and rename at the same time as well ‚Äî let‚Äôs do that to move testfile.txt back and give it a new name at once:\nmv testfile.txt sandbox/testfile_v2.txt\n\n\n\n\n\n\nOverwriting\n\n\n\nBy default, both mv and cp will overwrite files without warning! Use the -i (forinteractive) option to make it let you confirm before overwriting anything.\n\n\n\n\n\n\n\n\nRenaming rules for both cp and mv ‚Äî if the destination is:\n\n\n\n\nAn existing dir, the file(s) will keep their original names.\nNot an existing dir, the path specifies the new name of the file or dir, depending on what the source is.\n\n\n\n\nExercise: Practice with mv\nIn which directory (in terms of a relative path from your working dir) would the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files\nmv data/fastq fastq\nmv data/fastq .\n\nWhat if you wanted to move the FASTQ files directly into your current working directory (from data/fastq)?\n\n\n\nSolutions (click here)\n\nIn which directory (in terms of relative path from your working dir) will the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files ‚Äî in the dir fastq_files (we‚Äôve really just renamed the dir fastq to fastq_files)\nmv data/fastq fastq ‚Äî in fastq (because our source is a dir, so is the destination)\nmv data/fastq . ‚Äî in fastq also! (we‚Äôd need the syntax shown below to move the individual files directly into our current dir)\n\nWhat if you wanted to move the FASTQ files directly into your current working directory?\nFor one file:\nmv data/fastq/ASPC1_A178V_R1.fastq.gz .\nFor all files:\nmv data/fastq/* .\n\n\n\n\n\n5.4 Remove files with rm\nThe rm command removes (deletes) files and directories.\nOne important thing to note upfront is that rm will permanently and irreversibly delete files without the typical ‚Äúintermediate step‚Äù of placing them in a trash bin, like you are used to with your personal computer. With a healthy dosis of fear installed, let‚Äôs dive in.\nTo remove one or more files, you can simply pass the file names as arguments to rm as with previous commands. We will also use the -v (verbose) option to have it tell us what it did:\nrm -v sandbox/testfile_v2.txt\nremoved sandbox/testfile_v2.txt\n\n\nRecursive rm\nAs a safety measure, rm will by default only delete files and not directories or their contents ‚Äî i.e., like mkdir and cp, it refuses to act recursively by default. To remove dirs and their contents, use the -r option:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‚Äòd1‚Äô: Is a directory\n# But it does work with the '-r' option:\nrm -rv d1\nremoved directory: ‚Äòd1/d2/d3‚Äô\nremoved directory: ‚Äòd1/d2‚Äô\nremoved directory: ‚Äòd1‚Äô\nYou should obviously be quite careful with rm -r!\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous ‚Äî for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that ‚Äî that way, if the dir isn‚Äôt empty after all, you‚Äôll get an error."
  },
  {
    "objectID": "homework/shell.html#globbing-and-loops",
    "href": "homework/shell.html#globbing-and-loops",
    "title": "Homework: Intro to the Unix Shell",
    "section": "6 Globbing and loops",
    "text": "6 Globbing and loops\n\n6.1 Globbing with shell wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing. Wildcards are symbols that have a special meaning.\nIn globbing, the * wildcard matches any number of any character, including nothing.\nThe example below will match any files that contain the string ‚Äú_R1‚Äù:\n# (You should still be in /fs/ess/PAS2714/users/$USER)\nls data/fastq/*_R1*\ndata/fastq/NW102AB_R1.fastq.gz  data/fastq/NW201C_R1.fastq.gz   data/fastq/NW305AB_R1.fastq.gz  data/fastq/NW404BC_R1.fastq.gz  data/fastq/W204A_R1.fastq.gz   data/fastq/W303C_R1.fastq.gz   data/fastq/W404A_R1.fastq.gz\ndata/fastq/NW102C_R1.fastq.gz   data/fastq/NW203A_R1.fastq.gz   data/fastq/NW305C_R1.fastq.gz   data/fastq/W101AB_R1.fastq.gz   data/fastq/W204BC_R1.fastq.gz  data/fastq/W304AB_R1.fastq.gz  data/fastq/W404BC_R1.fastq.gz\ndata/fastq/NW103AB_R1.fastq.gz  data/fastq/NW203BC_R1.fastq.gz  data/fastq/NW403A_R1.fastq.gz   data/fastq/W101C_R1.fastq.gz    data/fastq/W205A_R1.fastq.gz   data/fastq/W304C_R1.fastq.gz\ndata/fastq/NW103C_R1.fastq.gz   data/fastq/NW304A_R1.fastq.gz   data/fastq/NW403BC_R1.fastq.gz  data/fastq/W103AB_R1.fastq.gz   data/fastq/W205BC_R1.fastq.gz  data/fastq/W403AB_R1.fastq.gz\ndata/fastq/NW201AB_R1.fastq.gz  data/fastq/NW304BC_R1.fastq.gz  data/fastq/NW404A_R1.fastq.gz   data/fastq/W103C_R1.fastq.gz    data/fastq/W303AB_R1.fastq.gz  data/fastq/W403C_R1.fastq.gz\nSome more file matching examples with * ‚Äî if you would be in your data/fastq dir, then:\n\n\n\nPattern\nMatches files whose names‚Ä¶\n\n\n\n\n*\nContain anything (matches all files)\n\n\n*fastq.gz\nEnd in ‚Äú.fastq.gz‚Äù\n\n\nNW1*\nStart with ‚ÄúNW1‚Äù\n\n\n*_R1*\nContain ‚Äú_R1‚Äù\n\n\n\n\n\n Exercise: Practice with *\nWhat pattern would you use if you wanted to select FASTQ files for the samples whose IDs end in AB (e.g.¬†NW102AB)?\n\n\nClick here for the solutions\n\nWe‚Äôll need a * on either side of our pattern, because the file names neither start not end with the pattern:\nls data/fastq/*AB_*\ndata/fastq/NW102AB_R1.fastq.gz  data/fastq/NW103AB_R2.fastq.gz  data/fastq/NW305AB_R1.fastq.gz  data/fastq/W101AB_R2.fastq.gz  data/fastq/W303AB_R1.fastq.gz  data/fastq/W304AB_R2.fastq.gz\ndata/fastq/NW102AB_R2.fastq.gz  data/fastq/NW201AB_R1.fastq.gz  data/fastq/NW305AB_R2.fastq.gz  data/fastq/W103AB_R1.fastq.gz  data/fastq/W303AB_R2.fastq.gz  data/fastq/W403AB_R1.fastq.gz\ndata/fastq/NW103AB_R1.fastq.gz  data/fastq/NW201AB_R2.fastq.gz  data/fastq/W101AB_R1.fastq.gz   data/fastq/W103AB_R2.fastq.gz  data/fastq/W304AB_R1.fastq.gz  data/fastq/W403AB_R2.fastq.gz\n\n\n\n\n\n6.2 For loops\nLoops are a universal element of programming languages, and are used to repeat operations. Here, we‚Äôll only cover the most common type of loop: the for loop.\nA for loop iterates over a collection, such as a list of files, and allows you to perform one or more actions for each element in the collection. In the example below, our ‚Äúcollection‚Äù is just a short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\n\n\n\n\n\n\nWhat was actually run under the hood is the following:\n\n\n\n# (Don't run this)\na_number=1\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=2\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=3\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\n\nHere are two key things to understand about for loops:\n\nIn each iteration of the loop, one element in the collection is being assigned to the variable specified after for. In the example above, we used a_number as the variable name, so that variable contained 1 when the loop ran for the first time, 2 when it ran for the second time, and 3 when it ran for the third and last time.\nThe loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\n\n\n\n\n\n\n\nA further explanation of for loop syntax (Click to expand)\n\n\n\n\n\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\nCombining loops and globbing\nA very useful strategy is to loop over files with globbing, for example:\nfor fastq_file in data/fastq/*fastq.gz; do\n    echo \"Running an analysis for file $fastq_file\"...\n    # Additional commands to process the FASTQ file\ndone\nRunning an analysis for file data/fastq/NW102AB_R1.fastq.gz...\nRunning an analysis for file data/fastq/NW102AB_R2.fastq.gz...\nRunning an analysis for file data/fastq/NW102C_R1.fastq.gz...\nRunning an analysis for file data/fastq/NW102C_R2.fastq.gz...\nRunning an analysis for file data/fastq/NW103AB_R1.fastq.gz...\nRunning an analysis for file data/fastq/NW103AB_R2.fastq.gz...\nRunning an analysis for file data/fastq/NW103C_R1.fastq.gz...\n#[...output truncated...]\n\n\n\n Exercise: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\nClick for the solution\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom"
  },
  {
    "objectID": "homework/shell.html#footnotes",
    "href": "homework/shell.html#footnotes",
    "title": "Homework: Intro to the Unix Shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)‚Ü©Ô∏é\nIt‚Äôs certainly possible to have spaces in file names, but it‚Äôs a bad idea, and will get you into trouble sooner or later.‚Ü©Ô∏é\nBeginners will often cd into a dir just to list its contents, but the method shown below is much quicker.‚Ü©Ô∏é\nAnytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.‚Ü©Ô∏é"
  }
]